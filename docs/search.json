[
  {
    "objectID": "CODE_OF_CONDUCT.html",
    "href": "CODE_OF_CONDUCT.html",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.\nWe pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.\n\n\n\nExamples of behavior that contributes to a positive environment for our community include:\n\nDemonstrating empathy and kindness toward other people\nBeing respectful of differing opinions, viewpoints, and experiences\nGiving and gracefully accepting constructive feedback\nAccepting responsibility and apologizing to those affected by our mistakes, and learning from the experience\nFocusing on what is best not just for us as individuals, but for the overall community\n\nExamples of unacceptable behavior include:\n\nThe use of sexualized language or imagery, and sexual attention or advances of any kind\nTrolling, insulting or derogatory comments, and personal or political attacks\nPublic or private harassment\nPublishing others’ private information, such as a physical or email address, without their explicit permission\nOther conduct which could reasonably be considered inappropriate in a professional setting\n\n\n\n\nCommunity leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.\nCommunity leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.\n\n\n\nThis Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.\n\n\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be reported to Alec (robit.alec@gmail.com), or Isabella. All complaints will be reviewed and investigated promptly and fairly.\nAll community leaders are obligated to respect the privacy and security of the reporter of any incident.\n\n\n\nCommunity leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:\n\n\nCommunity Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.\nConsequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.\n\n\n\nCommunity Impact: A violation through a single incident or series of actions.\nConsequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.\n\n\n\nCommunity Impact: A serious violation of community standards, including sustained inappropriate behavior.\nConsequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.\n\n\n\nCommunity Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals.\nConsequence: A permanent ban from any sort of public interaction within the community.\n\n\n\n\nThis Code of Conduct is adapted from the Contributor Covenant, version 2.0, available at https://www.contributor-covenant.org/version/2/0/code_of_conduct.html.\nCommunity Impact Guidelines were inspired by Mozilla’s code of conduct enforcement ladder.\nFor answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations."
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#our-pledge",
    "href": "CODE_OF_CONDUCT.html#our-pledge",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.\nWe pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community."
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#our-standards",
    "href": "CODE_OF_CONDUCT.html#our-standards",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "Examples of behavior that contributes to a positive environment for our community include:\n\nDemonstrating empathy and kindness toward other people\nBeing respectful of differing opinions, viewpoints, and experiences\nGiving and gracefully accepting constructive feedback\nAccepting responsibility and apologizing to those affected by our mistakes, and learning from the experience\nFocusing on what is best not just for us as individuals, but for the overall community\n\nExamples of unacceptable behavior include:\n\nThe use of sexualized language or imagery, and sexual attention or advances of any kind\nTrolling, insulting or derogatory comments, and personal or political attacks\nPublic or private harassment\nPublishing others’ private information, such as a physical or email address, without their explicit permission\nOther conduct which could reasonably be considered inappropriate in a professional setting"
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#enforcement-responsibilities",
    "href": "CODE_OF_CONDUCT.html#enforcement-responsibilities",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.\nCommunity leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate."
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#scope",
    "href": "CODE_OF_CONDUCT.html#scope",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event."
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#enforcement",
    "href": "CODE_OF_CONDUCT.html#enforcement",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to Alec (robit.alec@gmail.com), or Isabella. All complaints will be reviewed and investigated promptly and fairly.\nAll community leaders are obligated to respect the privacy and security of the reporter of any incident."
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#enforcement-guidelines",
    "href": "CODE_OF_CONDUCT.html#enforcement-guidelines",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:\n\n\nCommunity Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.\nConsequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.\n\n\n\nCommunity Impact: A violation through a single incident or series of actions.\nConsequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.\n\n\n\nCommunity Impact: A serious violation of community standards, including sustained inappropriate behavior.\nConsequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.\n\n\n\nCommunity Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals.\nConsequence: A permanent ban from any sort of public interaction within the community."
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#attribution",
    "href": "CODE_OF_CONDUCT.html#attribution",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "This Code of Conduct is adapted from the Contributor Covenant, version 2.0, available at https://www.contributor-covenant.org/version/2/0/code_of_conduct.html.\nCommunity Impact Guidelines were inspired by Mozilla’s code of conduct enforcement ladder.\nFor answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistical Rethinking Colearning 2023",
    "section": "",
    "text": "NOTE: website format taken from Alec Robitaille\n\nSchedule\n\nLectures\nHomework\n\nParticipant notes and homework solutions\nResources\nInstallation\nCode of Conduct\n\n\n\nThird round of Statistical Rethinking colearning, this time with 2024 lectures and homework.\nThe first round of Statistical Rethinking colearning (2022) is available here.\nThe second round of Statistical Rethinking colearning (2023) is available here.\n\n\n\n\n\n\n\nMeeting date\nReading\nLectures\n\n\n\n\n25 January\nChapters 1, 2 and 3\n[1] &lt;Science Before Statistics&gt; &lt;Slides&gt;  [2] &lt;Garden of Forking Data&gt; &lt;Slides&gt;\n\n\n08 February\nChapter 4\n[3] &lt;Geocentric Models&gt; &lt;Slides&gt;  [4] &lt;Categories and Curves&gt; &lt;Slides&gt;\n\n\n22 February\nChapters 5 and 6\n[5] &lt;Elemental Confounds&gt; &lt;Slides&gt;  [6] &lt;Good and Bad Controls&gt; &lt;Slides&gt;\n\n\n07 March\nChapters 7 and 8\n[7] &lt;Overfitting&gt; &lt;Slides&gt;  [8] &lt;MCMC&gt; &lt;Slides&gt;\n\n\n21 March\nChapters 9, 10 and 11\n[9] &lt;Modeling Events&gt; &lt;Slides&gt;  [10] &lt;Counts and Confounds&gt; &lt;Slides&gt;\n\n\n04 April\nChapters 11 and 12\n[11] &lt;Ordered Categories&gt; &lt;Slides&gt;  [12] &lt;Multilevel Models&gt; &lt;Slides&gt;\n\n\n18 April\nChapter 13\n[13] &lt;Multilevel Adventures&gt; &lt;Slides&gt;  [14] &lt;Correlated Features&gt; &lt;Slides&gt;\n\n\n2 May\nChapter 14\n[15] &lt;Social Networks&gt; &lt;Slides&gt;  [16] &lt;Gaussian Processes&gt; &lt;Slides&gt;\n\n\n16 May\nChapter 15\n[17] Measurement Error  [18] Missing Data\n\n\n30 May\nChapters 16 and 17\n[19] Beyond GLMs: State-space Models, ODEs  [20] Horoscopes\n\n\n\n\n\n\n\n\n\nMeeting date\nHomework\nSolutions\n\n\n\n\n1 February\nHomework 1\nSolutions\n\n\n15 February\nHomework 2\nSolutions\n\n\n29 February\nHomework 3\nSolutions\n\n\n14 March\nHomework 4\nSolutions\n\n\n28 March\nHomework 5\nSolutions\n\n\n11 April\nHomework 6\nSolutions\n\n\n25 April\nHomework 7\nSolutions\n\n\n9 May\nHomework 8\nSolutions\n\n\n23 May\nHomework 9\nSolutions\n\n\n30 May\nHomework 10\nSolutions\n\n\n\n\n\n\n\n\nAlec\nBella (this repo)\n\n\n\n\n\nAdditional material using other packages or languages\n\nOriginal R: https://github.com/rmcelreath/rethinking/\nR + Tidyverse + ggplot2 + brms: https://bookdown.org/content/4857/\nPython and PyMC3: Python/PyMC3\nJulia and Turing: https://github.com/StatisticalRethinkingJulia and https://github.com/StatisticalRethinkingJulia/TuringModels.jl\n\nSee Richard’s comments about these here: https://github.com/rmcelreath/stat_rethinking_2023#coding\n2022 colearning:\n\nLectures: https://github.com/rmcelreath/stat_rethinking_2022#calendar--topical-outline\nHomework: https://github.com/rmcelreath/stat_rethinking_2022/tree/main/homework\n\nAlso, Alec’s notes and solutions of the 2019 material: https://github.com/robitalec/statistical-rethinking and https://www.statistical-rethinking.robitalec.ca/\n\n\n\nPackage specific install directions. We’ll update these as we go!\nRethinking\n\nrethinking\n\nStan\n\ncmdstanr\nRStan\nbrms\n\nTargets\n\ntargets\nstantargets\n\nV8, needed for the dagitty package\n\nV8\n\n\n\n\nPlease note that this project is released with a Code of Conduct. By participating in this project you agree to abide by its terms."
  },
  {
    "objectID": "index.html#schedule",
    "href": "index.html#schedule",
    "title": "Statistical Rethinking Colearning 2023",
    "section": "",
    "text": "Meeting date\nReading\nLectures\n\n\n\n\n25 January\nChapters 1, 2 and 3\n[1] &lt;Science Before Statistics&gt; &lt;Slides&gt;  [2] &lt;Garden of Forking Data&gt; &lt;Slides&gt;\n\n\n08 February\nChapter 4\n[3] &lt;Geocentric Models&gt; &lt;Slides&gt;  [4] &lt;Categories and Curves&gt; &lt;Slides&gt;\n\n\n22 February\nChapters 5 and 6\n[5] &lt;Elemental Confounds&gt; &lt;Slides&gt;  [6] &lt;Good and Bad Controls&gt; &lt;Slides&gt;\n\n\n07 March\nChapters 7 and 8\n[7] &lt;Overfitting&gt; &lt;Slides&gt;  [8] &lt;MCMC&gt; &lt;Slides&gt;\n\n\n21 March\nChapters 9, 10 and 11\n[9] &lt;Modeling Events&gt; &lt;Slides&gt;  [10] &lt;Counts and Confounds&gt; &lt;Slides&gt;\n\n\n04 April\nChapters 11 and 12\n[11] &lt;Ordered Categories&gt; &lt;Slides&gt;  [12] &lt;Multilevel Models&gt; &lt;Slides&gt;\n\n\n18 April\nChapter 13\n[13] &lt;Multilevel Adventures&gt; &lt;Slides&gt;  [14] &lt;Correlated Features&gt; &lt;Slides&gt;\n\n\n2 May\nChapter 14\n[15] &lt;Social Networks&gt; &lt;Slides&gt;  [16] &lt;Gaussian Processes&gt; &lt;Slides&gt;\n\n\n16 May\nChapter 15\n[17] Measurement Error  [18] Missing Data\n\n\n30 May\nChapters 16 and 17\n[19] Beyond GLMs: State-space Models, ODEs  [20] Horoscopes\n\n\n\n\n\n\n\n\n\nMeeting date\nHomework\nSolutions\n\n\n\n\n1 February\nHomework 1\nSolutions\n\n\n15 February\nHomework 2\nSolutions\n\n\n29 February\nHomework 3\nSolutions\n\n\n14 March\nHomework 4\nSolutions\n\n\n28 March\nHomework 5\nSolutions\n\n\n11 April\nHomework 6\nSolutions\n\n\n25 April\nHomework 7\nSolutions\n\n\n9 May\nHomework 8\nSolutions\n\n\n23 May\nHomework 9\nSolutions\n\n\n30 May\nHomework 10\nSolutions"
  },
  {
    "objectID": "index.html#participant-notes-and-homework-solutions",
    "href": "index.html#participant-notes-and-homework-solutions",
    "title": "Statistical Rethinking Colearning 2023",
    "section": "",
    "text": "Alec\nBella (this repo)"
  },
  {
    "objectID": "index.html#resources",
    "href": "index.html#resources",
    "title": "Statistical Rethinking Colearning 2023",
    "section": "",
    "text": "Additional material using other packages or languages\n\nOriginal R: https://github.com/rmcelreath/rethinking/\nR + Tidyverse + ggplot2 + brms: https://bookdown.org/content/4857/\nPython and PyMC3: Python/PyMC3\nJulia and Turing: https://github.com/StatisticalRethinkingJulia and https://github.com/StatisticalRethinkingJulia/TuringModels.jl\n\nSee Richard’s comments about these here: https://github.com/rmcelreath/stat_rethinking_2023#coding\n2022 colearning:\n\nLectures: https://github.com/rmcelreath/stat_rethinking_2022#calendar--topical-outline\nHomework: https://github.com/rmcelreath/stat_rethinking_2022/tree/main/homework\n\nAlso, Alec’s notes and solutions of the 2019 material: https://github.com/robitalec/statistical-rethinking and https://www.statistical-rethinking.robitalec.ca/"
  },
  {
    "objectID": "index.html#installation",
    "href": "index.html#installation",
    "title": "Statistical Rethinking Colearning 2023",
    "section": "",
    "text": "Package specific install directions. We’ll update these as we go!\nRethinking\n\nrethinking\n\nStan\n\ncmdstanr\nRStan\nbrms\n\nTargets\n\ntargets\nstantargets\n\nV8, needed for the dagitty package\n\nV8"
  },
  {
    "objectID": "index.html#code-of-conduct",
    "href": "index.html#code-of-conduct",
    "title": "Statistical Rethinking Colearning 2023",
    "section": "",
    "text": "Please note that this project is released with a Code of Conduct. By participating in this project you agree to abide by its terms."
  },
  {
    "objectID": "homework/homework-01.html",
    "href": "homework/homework-01.html",
    "title": "Homework - Week 01",
    "section": "",
    "text": "Suppose the globe tossing data had turned out to be 3 water and 11 land. Construct the posterior distribution.\n\nDAG:\n\n\n\n\n\nPosterior:\n\n# create the sample manually\nN_W &lt;- 3\nN_L &lt;- 11\nsample &lt;- c(rep(\"W\", N_W), rep(\"L\", N_L))\n\n# use compute_posterior with the sample function to compute the posterior \ncompute_posterior &lt;- function(the_sample, poss = seq(0, 1, length.out = 1000)){\n  W &lt;- sum(the_sample==\"W\")\n  L &lt;- sum(the_sample==\"L\")\n  ways &lt;- sapply(poss, function(q) (q*4)^W * ((1-q)*4)^L)\n  post &lt;- ways/sum(ways)\n  bars &lt;- sapply(post, function(q) make_bar(q))\n  data.frame(poss, ways, post = round(post, 3), bars)\n}\n\npost &lt;- compute_posterior(sample)\n\nggplot(post) + \n  geom_smooth(aes(x = poss, y = post), se = F) + \n  labs(y = \"Posterior Probability\", x = \"p\") + \n  theme_classic()"
  },
  {
    "objectID": "homework/homework-01.html#q1",
    "href": "homework/homework-01.html#q1",
    "title": "Homework - Week 01",
    "section": "",
    "text": "Suppose the globe tossing data had turned out to be 3 water and 11 land. Construct the posterior distribution.\n\nDAG:\n\n\n\n\n\nPosterior:\n\n# create the sample manually\nN_W &lt;- 3\nN_L &lt;- 11\nsample &lt;- c(rep(\"W\", N_W), rep(\"L\", N_L))\n\n# use compute_posterior with the sample function to compute the posterior \ncompute_posterior &lt;- function(the_sample, poss = seq(0, 1, length.out = 1000)){\n  W &lt;- sum(the_sample==\"W\")\n  L &lt;- sum(the_sample==\"L\")\n  ways &lt;- sapply(poss, function(q) (q*4)^W * ((1-q)*4)^L)\n  post &lt;- ways/sum(ways)\n  bars &lt;- sapply(post, function(q) make_bar(q))\n  data.frame(poss, ways, post = round(post, 3), bars)\n}\n\npost &lt;- compute_posterior(sample)\n\nggplot(post) + \n  geom_smooth(aes(x = poss, y = post), se = F) + \n  labs(y = \"Posterior Probability\", x = \"p\") + \n  theme_classic()"
  },
  {
    "objectID": "homework/homework-01.html#q2",
    "href": "homework/homework-01.html#q2",
    "title": "Homework - Week 01",
    "section": "Q2",
    "text": "Q2\n\nUsing the posterior distribution from Q1, compute the posterior predictive distribution for the next 5 tosses of the same globe. I recommend you use the sampling method.\n\nSampling:\n\n# first sample your posterior \nn &lt;- 1e4\nsamples &lt;- sample(post$poss, prob = post$post, size = n, replace = T)\n\n# now simulate 5 tosses using the probability of each possible value \nN_toss &lt;- 5\nposterior_predict &lt;- data.frame(probpredict = rbinom(n, size = N_toss, prob = samples))\n\n# plot probability for each number of water samples you will get in 5 tosses\nggplot(posterior_predict) + \n  geom_bar(aes(x = probpredict)) + \n  labs(x = \"Number of W Tosses\") + \n  theme_classic()"
  },
  {
    "objectID": "homework/homework-01.html#q3-optional",
    "href": "homework/homework-01.html#q3-optional",
    "title": "Homework - Week 01",
    "section": "Q3 (optional):",
    "text": "Q3 (optional):\n\nSuppose you observe W = 7 water points, but you forgot to write down how many times the globe was tossed, so you don’t know the number of land points, L. Assume that p = 0.7 and compute the posterior distribution of the number of tosses N. Hint: Use the binomial distribution.\n\n\nN_tosses &lt;- seq(7, 20)\n\nprob_tosses &lt;- data.frame(N_tosses = N_tosses, prob = dbinom(x = 7, size = N_tosses, prob = 0.7))\n\nggplot(prob_tosses) + \n  geom_col(aes(x = N_tosses, y = prob)) + \n  labs(x = \"Number of Tosses to get W = 7 when p = 0.7\", y = \"Probability\") + \n  scale_x_continuous(n.breaks = 13) + \n  theme_classic()"
  },
  {
    "objectID": "homework/homework-listing.html",
    "href": "homework/homework-listing.html",
    "title": "Homework",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\nDate\n\n\nAuthor\n\n\n\n\n\n\nHomework - Week 01\n\n\nundefined\n\n\nIsabella C. Richmond\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "notes/notes-04.html",
    "href": "notes/notes-04.html",
    "title": "Lecture 04 - Categories & Curves",
    "section": "",
    "text": "Rose: “Unobserved causes are ignorable unless they are shared”\nThorn:"
  },
  {
    "objectID": "notes/notes-04.html#rose-thorn",
    "href": "notes/notes-04.html#rose-thorn",
    "title": "Lecture 04 - Categories & Curves",
    "section": "",
    "text": "Rose: “Unobserved causes are ignorable unless they are shared”\nThorn:"
  },
  {
    "objectID": "notes/notes-04.html#drawing-inferences",
    "href": "notes/notes-04.html#drawing-inferences",
    "title": "Lecture 04 - Categories & Curves",
    "section": "Drawing Inferences",
    "text": "Drawing Inferences\n\nlinear model can accomodate anything, thus we need to think carefully about our scientific model\ngenerative model + multiple estimands = multiple estimators\nquite often the estimate we want is not in a summary table because it depends on multiple unknowns in the posterior distribution or making assumptions about the population, so we often need to do post-processing"
  },
  {
    "objectID": "notes/notes-04.html#categories",
    "href": "notes/notes-04.html#categories",
    "title": "Lecture 04 - Categories & Curves",
    "section": "Categories",
    "text": "Categories\n\ncategories are discrete and non-linear\ndiscrete, unordered types\nwe want to stratify by category, to fit a separate line for each"
  },
  {
    "objectID": "notes/notes-04.html#howell-data",
    "href": "notes/notes-04.html#howell-data",
    "title": "Lecture 04 - Categories & Curves",
    "section": "Howell Data",
    "text": "Howell Data\n\n\nLoading required package: cmdstanr\n\n\nThis is cmdstanr version 0.6.1\n\n\n- CmdStanR documentation and vignettes: mc-stan.org/cmdstanr\n\n\n- CmdStan path: /home/icrichmond/.cmdstan/cmdstan-2.33.1\n\n\n- CmdStan version: 2.33.1\n\n\n\nA newer version of CmdStan is available. See ?install_cmdstan() to install it.\nTo disable this check set option or environment variable CMDSTANR_NO_VER_CHECK=TRUE.\n\n\nLoading required package: posterior\n\n\nThis is posterior version 1.4.1\n\n\n\nAttaching package: 'posterior'\n\n\nThe following objects are masked from 'package:stats':\n\n    mad, sd, var\n\n\nThe following objects are masked from 'package:base':\n\n    %in%, match\n\n\nLoading required package: parallel\n\n\nrethinking (Version 2.40)\n\n\n\nAttaching package: 'rethinking'\n\n\nThe following object is masked from 'package:stats':\n\n    rstudent\n\n\n\nhow are height, weight, and sex causally related?\nhow are height, weight, and sex statistically related?\n\n\nd &lt;- dagitty(\"dag {\n                H -&gt; W\n                S -&gt; W\n                S -&gt; H\n             }\")\n\ndrawdag(d)\n\n\n\n\n\nheight influences weight\nsex influences weight and height\nweight is influenced by height and sex\ninfluence of sex is both direct and indirect on weight\n\\(H = f_{H}(S)\\)\n\\(W = f_{W}(H,S)\\)\nUnobserved causes are ignorable unless they are shared between variables (common cause) = confound\n\n\nsim_HW &lt;- function(S, b, a){\n  N &lt;- length(S)\n  H &lt;- ifelse(S==1, 150, 160) + rnorm(N, 0, 5)\n  W &lt;- a[S] + b[S]*H + rnorm(N, 0, 5)\n  data.frame(S, H, W)\n}\n\n# S = 1 female : S = 2 male\nS &lt;- rbern(100)+1\ndat &lt;- sim_HW(S, b=c(0.5, 0.6), a=c(0,0))\nhead(dat)\n\n  S        H        W\n1 2 159.3816 98.60949\n2 2 159.0705 99.91792\n3 2 160.5331 91.31716\n4 1 140.8750 67.57970\n5 2 159.6841 93.36216\n6 1 153.8269 80.43557\n\n\n\nscientific questions:\n\ncausal effect of H on W?\ncausal effect of S on W?\ndirect causal effect of S on W?\n\nwe need to stratify by S to answer qs 2 and 3\ncoding categorical variables\n\nindicator variables (0/1)\nindex variables (1,2,3,4)\nindex variables are generally preferable"
  },
  {
    "objectID": "notes/notes-04.html#index-variables",
    "href": "notes/notes-04.html#index-variables",
    "title": "Lecture 04 - Categories & Curves",
    "section": "Index Variables",
    "text": "Index Variables\n\noften we want to give each index the same prior\nInfluence of colour coded by: \\(\\alpha = [\\alpha_1 , \\alpha_2, \\alpha_3, \\alpha_4 ]\\)\n\\(y_i \\sim Normal(\\mu_i, \\sigma)\\)\n\\(\\mu_i = \\alpha_{S[i]}\\)\nPriors\n\n\\(\\alpha_j \\sim Normal(60, 10)\\)\n\\(\\sigma \\sim Uniform(0, 10)\\)\nIf you use indicator variables, one becomes the default and the other is the adjustment so you must set separate priors for both"
  },
  {
    "objectID": "notes/notes-04.html#total-causal-effect-of-sex-on-weight",
    "href": "notes/notes-04.html#total-causal-effect-of-sex-on-weight",
    "title": "Lecture 04 - Categories & Curves",
    "section": "Total Causal Effect of Sex on Weight",
    "text": "Total Causal Effect of Sex on Weight\n\nS &lt;- rep(1, 100)\nsimF &lt;- sim_HW(S, b=c(0.5, 0.6), a=c(0,0))\n\nS &lt;- rep(2, 100)\nsimM &lt;- sim_HW(S, b=c(0.5, 0.6), a=c(0,0))\n\n# effect of sex (male-female)\nmean(simM$W - simF$W)\n\n[1] 20.60525\n\n\n\nestimating model and synthetic example\n\n\nS &lt;- rbern(100)+1\ndat &lt;- sim_HW(S, b = c(0.5,0.6), a = c(0,0))\n\nm_SW &lt;- quap(alist(\n  W ~ dnorm(mu, sigma),\n  mu &lt;- a[S],\n  a[S] ~ dnorm(60, 10),\n  sigma ~ dunif(0, 10)\n), data = dat)\n\nprecis(m_SW, depth = 2)\n\n           mean        sd      5.5%     94.5%\na[1]  74.146647 0.8146886 72.844617 75.448676\na[2]  95.390673 0.8152587 94.087733 96.693614\nsigma  5.779165 0.4090514  5.125422  6.432908\n\n\n\nanalyze the real sample\n\n\nd &lt;- Howell1\nd &lt;- d[ d$age &gt;= 18,]\n\ndat &lt;- list(\n  W = d$weight,\n  S = d$male + 1\n)\n\nm_SW &lt;- quap(alist(\n  W ~ dnorm(mu, sigma), \n  mu &lt;- a[S],\n  a[S] ~ dnorm(60, 10),\n  sigma ~ dunif(0, 10)\n), data = dat)\n\n\nposterior means and predictions\na1 and a2 are often not what we are after\n\ndifference in mean weight is not in the posterior - must calculate a contrast\nbut the mean weight of each category does exist in the posterior\n\nfirst plot is just mean weight for each sex\ndistributions of weights is the posterior predicted, second figure\ncontrast is usually what we are after, third figure\n\n\n# posterior mean W\npost &lt;- extract.samples(m_SW)\ndensity_f &lt;- density(post$a[, 1])\ndensity_m &lt;- density(post$a[, 2])\nplot(density_f, col=2, xlim = c(min(density_f$x,density_m$x), max(density_f$x,density_m$x)), main = \"Posterior Mean Weight (kg)\") + \nlines(density_m, col=4)\n\n\n\n\ninteger(0)\n\n# posterior W predictions\nW1 &lt;- rnorm(1000, post$a[,1], post$sigma)\nW2 &lt;- rnorm(1000, post$a[,2], post$sigma)\npredict_f &lt;- density(W1)\npredict_m &lt;- density(W2)\nplot(predict_f, col=2, xlim = c(min(predict_f$x,predict_m$x), max(predict_f$x,predict_m$x)), main = \"Posterior Predicted Weight (kg)\") + \nlines(predict_m, col=4)\n\n\n\n\ninteger(0)\n\n\n\ncontrasting\n\nneed to compute the difference between the categories\nit is not legitimate to compare overlap in distributions\nwe must compute contrast distribution\noverlap in distributions does not indicate that they are the same (or different)\n\naverage difference between men and women in this sample:\n\n\nmu_contrast &lt;- post$a[,2] - post$a[,1]\ndens(mu_contrast)\n\n\n\n\n\nweight contrasting:\n\n\n# contrast\nW_contrast &lt;- W2 - W1\ndens(W_contrast)\n\n\n\n# proportion above zero\nsum(W_contrast &gt; 0)/1000\n\n[1] 0.803\n\nsum(W_contrast &lt; 0)/1000\n\n[1] 0.197"
  },
  {
    "objectID": "notes/notes-04.html#direct-causal-effect-of-sex-on-weight",
    "href": "notes/notes-04.html#direct-causal-effect-of-sex-on-weight",
    "title": "Lecture 04 - Categories & Curves",
    "section": "Direct Causal Effect of Sex on Weight",
    "text": "Direct Causal Effect of Sex on Weight\n\n“controlling” for the indirect effect of sex through height\nwant to “block” association through H\n\n\nS &lt;- rbern(100)+1\n# slopes are the same so there is no effect of height on weight through slope but men are on average 10 kg heavier (intercept 10)\nset.seed(12)\ndat &lt;- sim_HW(S, b = c(0.5, 0.5), a = c(0, 10))\n\n\n\\(W_{i} \\sim Normal(\\mu _{i}, \\sigma)\\)\n\\(\\mu _{i} = \\alpha _{S[i]} + \\beta _{S[i]}(H_{i} - \\bar H)\\)\n\nThis equation centers the height \\((H_{i} - \\bar H)\\)\nCentering H means that alpha represents the average weight of a person with average height\nCentering also makes priors easier for alpha\n\\(\\alpha = [\\alpha _{1}, \\alpha _{2}]\\) , \\(\\beta = [\\beta _{1}, \\beta _{2}]\\)\n\nanalyze the sample\n\n\nd &lt;- Howell1\nd &lt;- d[d$age &gt;= 18, ]\ndat &lt;- list(W = d$weight, H = d$height, Hbar = mean(d$height), S = d$male + 1)\n\nm_SHW &lt;- quap(alist(\n  W ~ dnorm(mu, sigma),\n  mu &lt;- a[S] + b[S]*(H-Hbar),\n  a[S] ~ dnorm(60, 10),\n  b[S] ~ dunif(0, 1),\n  sigma ~ dunif(0, 10)\n), data = dat)\n\n\nwe need to compute the difference of expected weight at each height to get the actual estimate that we are looking for (for the direct effect of sex on weight)\ncompute posterior predictive for each group, calculate contrast, plot\n\n\nxseq &lt;- seq(from=130, to=190, len=50)\n\nmuF &lt;- link(m_SHW, data = list(S=rep(1,50), H=xseq, Hbar=mean(d$height)))\n\nmuM &lt;- link(m_SHW, data = list(S=rep(2,50), H=xseq, Hbar = mean(d$height)))\n\nmu_contrast &lt;- muF - muM\n\nplot(NULL, xlim=range(xseq), ylim = range(-6, 8), xlab = \"height (cm)\", ylab = \"weight contrast (F-M)\") +\nlines(xseq, apply(muF, 2, mean)) + \nlines(xseq, apply(muM, 2, mean)) + \nfor (p in c(0.5, 0.6, 0.7, 0.8, 0.9, 0.99)) {\n  shade(apply(mu_contrast, 2, PI, prob = p), xseq)\n  abline(h=0)\n  }\n\n\n\n\ninteger(0)\n\n\n\nwomen tend to be heavier at high heights and vice versa for men\nbut most of the effect is centered around zero, therefore:\nnearly all of the causal effects of S acts through H\n\nwhen we block H, we see very little effect of sex on weight"
  },
  {
    "objectID": "notes/notes-04.html#categorical-variables",
    "href": "notes/notes-04.html#categorical-variables",
    "title": "Lecture 04 - Categories & Curves",
    "section": "Categorical Variables",
    "text": "Categorical Variables\n\ncommon, easy to use with index coding\nuse samples to compute relevant contrasts\nalways summarize (mean, interval) as the last step\nwe want mean difference and not difference of means"
  },
  {
    "objectID": "notes/notes-04.html#curves-from-lines",
    "href": "notes/notes-04.html#curves-from-lines",
    "title": "Lecture 04 - Categories & Curves",
    "section": "Curves from Lines",
    "text": "Curves from Lines\n\nmany non linear relationships, which can be fit my linear models easily (but not mechanistic, like a geocentric model)\nlinear models can easily fit curves: 2 strategies\n\npolynomials (bad)\nsplines and GAMs (less bad + useful)"
  },
  {
    "objectID": "notes/notes-04.html#polynomial-models",
    "href": "notes/notes-04.html#polynomial-models",
    "title": "Lecture 04 - Categories & Curves",
    "section": "Polynomial Models",
    "text": "Polynomial Models\n\nstill linear because its an additive function of the parameters\ncreate strange symmetries and explosive uncertainty\nno local smoothing, only global smoothing\nany data point at any part of the x-axis can significantly change the curve even at arbitrarily far distances\ncan’t predict anything outside of the data with any certainty\ndo not use"
  },
  {
    "objectID": "notes/notes-04.html#splines-gams",
    "href": "notes/notes-04.html#splines-gams",
    "title": "Lecture 04 - Categories & Curves",
    "section": "Splines + GAMs",
    "text": "Splines + GAMs\n\ngreat for locally inferred function\nadd together a bunch of locally trained terms\ncan add as many locally trained terms as you want\neach term has a weight and slope and only affects its own region"
  },
  {
    "objectID": "notes/notes-04.html#full-luxury-bayes",
    "href": "notes/notes-04.html#full-luxury-bayes",
    "title": "Lecture 04 - Categories & Curves",
    "section": "Full Luxury Bayes",
    "text": "Full Luxury Bayes\n\ninstead of two models for two estimands, use one model for full causal model\ncan simulate interventions with this approach\nrequires more simulations\n\n\nm_SHW_full &lt;- quap(alist(\n  \n  # weight\n  W ~ dnorm(mu, sigma),\n  mu &lt;- a[S] + b[S]*(H-Hbar),\n  a[S] ~ dnorm(60, 10),\n  b[S] ~ dunif(0, 1),\n  sigma ~ dunif(0, 10),\n  \n  # height\n  H ~ dnorm(nu, tau),\n  nu &lt;- h[S],\n  h[S] ~ dnorm(160, 10),\n  tau ~ dunif(0, 10)\n  \n), data = dat)\n\n\npost &lt;- extract.samples(m_SHW_full)\nHbar &lt;- dat$Hbar\nn &lt;- 1e4\n\nwith(post, {\n  \n  H_S1 &lt;- rnorm(n, h[,1], tau)\n  W_S1 &lt;- rnorm(n, a[,2] + b[,1]*(H_S2-Hbar), sigma)\n  \n  W_do_S &lt;&lt;- W_S2 - W_S1\n  \n})\n\n# automate\nHWsim &lt;- sim(m_SHW_full, data = list(S=c(1,2)), vars = c(\"H\", \"W\"))\nW_do_S_auto &lt;- HWsim$W[,2] - HWsim$W[,1]\n\n\nyou can either do one statistical model for each estimand OR one simulation for each estimand (full luxury Bayes)"
  },
  {
    "objectID": "notes/notes-04.html#todo",
    "href": "notes/notes-04.html#todo",
    "title": "Lecture 04 - Categories & Curves",
    "section": "TODO:",
    "text": "TODO:"
  },
  {
    "objectID": "notes/notes-02.html",
    "href": "notes/notes-02.html",
    "title": "Lecture 02 - The Garden of Forking Data",
    "section": "",
    "text": "Rose: understanding the predictive posterior this time\nThorn: what if you don’t know your misclassification rate"
  },
  {
    "objectID": "notes/notes-02.html#rose-thorn",
    "href": "notes/notes-02.html#rose-thorn",
    "title": "Lecture 02 - The Garden of Forking Data",
    "section": "",
    "text": "Rose: understanding the predictive posterior this time\nThorn: what if you don’t know your misclassification rate"
  },
  {
    "objectID": "notes/notes-02.html#globe-example",
    "href": "notes/notes-02.html#globe-example",
    "title": "Lecture 02 - The Garden of Forking Data",
    "section": "Globe Example",
    "text": "Globe Example\nestimand: proportion of the globe covered in water - do people know what an estimand is?\n\na research/scientific question\n\nestimator: statistical way of producing an estimate\n\ngenerative model tests your estimator"
  },
  {
    "objectID": "notes/notes-02.html#generative-model-globe",
    "href": "notes/notes-02.html#generative-model-globe",
    "title": "Lecture 02 - The Garden of Forking Data",
    "section": "Generative Model (globe)",
    "text": "Generative Model (globe)\n\np = proportion of water\nW = water observations\nN = number of tosses\nL = land observations\nN influences W and L because higher tosses means higher numbers\nstart with how the variables influence each other, i.e., causal model\n\nstart conceptually, with science\nintervention on N is also an intervention on W and L\n\n\n\nlibrary(dagitty)\nlibrary(rethinking)\n\nLoading required package: cmdstanr\n\n\nThis is cmdstanr version 0.6.1\n\n\n- CmdStanR documentation and vignettes: mc-stan.org/cmdstanr\n\n\n- CmdStan path: /home/icrichmond/.cmdstan/cmdstan-2.33.1\n\n\n- CmdStan version: 2.33.1\n\n\n\nA newer version of CmdStan is available. See ?install_cmdstan() to install it.\nTo disable this check set option or environment variable CMDSTANR_NO_VER_CHECK=TRUE.\n\n\nLoading required package: posterior\n\n\nThis is posterior version 1.4.1\n\n\n\nAttaching package: 'posterior'\n\n\nThe following objects are masked from 'package:stats':\n\n    mad, sd, var\n\n\nThe following objects are masked from 'package:base':\n\n    %in%, match\n\n\nLoading required package: parallel\n\n\nrethinking (Version 2.40)\n\n\n\nAttaching package: 'rethinking'\n\n\nThe following object is masked from 'package:stats':\n\n    rstudent\n\nd &lt;- dagitty(\"dag {\n                p -&gt; W\n                p -&gt; L\n                N -&gt; W \n                N -&gt; L }\")\n\ndrawdag(d)\n\n\n\n\n\\[\nW,L = f(p,N)\n\\]\n\nW and L are functions of p and N\nDAGs are not generative, but they make causal models which allow us to make generative models\n\nthey represent functional relationships"
  },
  {
    "objectID": "notes/notes-02.html#bayesian-data-analysis",
    "href": "notes/notes-02.html#bayesian-data-analysis",
    "title": "Lecture 02 - The Garden of Forking Data",
    "section": "Bayesian Data Analysis",
    "text": "Bayesian Data Analysis\nFor each possible explanation of the sample,\n\ncount all the ways the sample could happen (given a particular explanation)\nexplanations with more ways to produce the sample are more plausible"
  },
  {
    "objectID": "notes/notes-02.html#garden-of-forking-data",
    "href": "notes/notes-02.html#garden-of-forking-data",
    "title": "Lecture 02 - The Garden of Forking Data",
    "section": "Garden of Forking Data",
    "text": "Garden of Forking Data\n\nrelies on samples being independent\nrelative differences between probabilities are dependent on sample size (differences will be smaller with smaller sample sizes because there is less evidence)\nnormalizing to probability allows for interpretability and easier math\ncollection of probabilities is a posterior distribution\n\n\nsample &lt;- c(\"W\", \"L\", \"W\", \"W\", \"W\", \"L\", \"W\", \"L\", \"W\")\n\nW &lt;- sum(sample==\"W\")\nL &lt;- sum(sample==\"L\")\np &lt;- c(0, 0.25, 0.5, 0.75, 1)\nways &lt;- sapply(p, function(q) (q*4)^W * ((1-q)*4)^L)\nprob &lt;- ways/sum(ways)\ncbind(p, ways, prob)\n\n        p ways       prob\n[1,] 0.00    0 0.00000000\n[2,] 0.25   27 0.02129338\n[3,] 0.50  512 0.40378549\n[4,] 0.75  729 0.57492114\n[5,] 1.00    0 0.00000000\n\nsim_globe &lt;- function(p = 0.7, N = 9){\n  sample(c(\"W\", \"L\"), size = N, prob = c(p, 1-p), replace = T)\n}\n\nsim_globe()\n\n[1] \"W\" \"W\" \"W\" \"W\" \"W\" \"W\" \"L\" \"W\" \"W\""
  },
  {
    "objectID": "notes/notes-02.html#testing",
    "href": "notes/notes-02.html#testing",
    "title": "Lecture 02 - The Garden of Forking Data",
    "section": "Testing",
    "text": "Testing\n\nhave to test\ntest using extreme values where you intuitively know the right answer\nexplore different sampling design\n\n\nsim_globe &lt;- function(p = 0.7, N = 9){\n  \n  sample(c(\"W\", \"L\"), size = N, prob=c(p, 1-p), replace = T)\n}\n\nsim_globe()\n\n[1] \"W\" \"W\" \"L\" \"W\" \"W\" \"W\" \"W\" \"W\" \"W\"\n\nsim_globe(p=1, N=11) \n\n [1] \"W\" \"W\" \"W\" \"W\" \"W\" \"W\" \"W\" \"W\" \"W\" \"W\" \"W\"\n\nsum(sim_globe(p=0.5, N=1e4) == \"W\")/1e4\n\n[1] 0.5011\n\n\nFunction:\n\nlibrary(crayon)\n\nmake_bar &lt;- function(q,size=20) {\n    n &lt;- round(q*size)\n    s1 &lt;- concat( rep(\"#\",n) )\n    s2 &lt;- concat( rep(\" \",size-n) )\n    concat(s1,s2)\n}\n\ncompute_posterior &lt;- function(the_sample, poss = c(0,0.25,0.5,0.75,1)){\n  W &lt;- sum(the_sample==\"W\")\n  L &lt;- sum(the_sample==\"L\")\n  ways &lt;- sapply(poss, function(q) (q*4)^W * ((1-q)*4)^L)\n  post &lt;- ways/sum(ways)\n  bars &lt;- sapply(post, function(q) make_bar(q))\n  data.frame(poss, ways, post = round(post,3), bars)\n}\n\ncompute_posterior(sim_globe())\n\n  poss ways  post                 bars\n1 0.00    0 0.000                     \n2 0.25    9 0.003                     \n3 0.50  512 0.189 ####                \n4 0.75 2187 0.808 ################    \n5 1.00    0 0.000"
  },
  {
    "objectID": "notes/notes-02.html#real-number-sampling",
    "href": "notes/notes-02.html#real-number-sampling",
    "title": "Lecture 02 - The Garden of Forking Data",
    "section": "Real Number Sampling",
    "text": "Real Number Sampling\n\nmore possibilities = less probability in each option/outcome\n\nprobability is spread out across many options\n\nnormalizing to probability allows our equation to calculate infinite number of “sides”/outcomes\n\n\\[\np^W(1-p)^L\n\\]\np = probability\ndensity = probability when we are assessing infinite number of possibilities\n\nshape of the posterior embodies sample size\n\nno min sample size -&gt; just more uncertain posterior\nposterior distribution embodies sample size\n\nno point estimates! estimate is entire posterior distribution\n\ncan use summary points from post dist for communication purposes\n\nintervals are merely indicators of the shape of the posterior distribution\n\nno “true interval” i.e., 95% CI doesn’t exist\ninterval is just distribution lower/upper bounds"
  },
  {
    "objectID": "notes/notes-02.html#analyze-sample-summarize",
    "href": "notes/notes-02.html#analyze-sample-summarize",
    "title": "Lecture 02 - The Garden of Forking Data",
    "section": "Analyze Sample + Summarize",
    "text": "Analyze Sample + Summarize\n\npost_samples &lt;- rbeta(1e3, 6+1, 3+1)\n\ndens(post_samples, lwd = 4, col = 2, xlab = \"prop water\", adj = 0.1)\n\ncurve(dbeta(x, 6+1, 3+1), add = T, lty = 2, lwd = 3)\n\n\n\n\n\nposterior prediction = “what would we bet?”\n\nhow many W’s do we expect to see in the next 10 tosses\n\nfor each sample of post dist, we can create a predictive distribution, then posterior predictive\n\nincorporates uncertainty from posterior distribution\n\n\n\npost_samples &lt;- rbeta(1e4, 6+1, 3+1)\n\npred_post &lt;- sapply(post_samples, function(p) \nsum(sim_globe(p, 10)==\"W\"))\n\ntab_post &lt;- table(pred_post)\n\n#for (i in 0:10) lines(c(i,i),c(0,tab_post[i+1]), lwd = 4, col = 4)"
  },
  {
    "objectID": "notes/notes-02.html#misclassification-bonus-round",
    "href": "notes/notes-02.html#misclassification-bonus-round",
    "title": "Lecture 02 - The Garden of Forking Data",
    "section": "Misclassification (Bonus Round)",
    "text": "Misclassification (Bonus Round)\n\nW* is misclassified due to sampling error and measurement process\n\ntrue W is unknown\n\n\n\nlibrary(dagitty)\nlibrary(rethinking)\n\nd &lt;- dagitty(\"dag {\n                p -&gt; W\n                N -&gt; W\n                W -&gt; Wm\n                M -&gt; Wm\n             }\")\n\ndrawdag(d)\n\n\n\n\n\nincorporate measurement error with x (error rate of 10%)\n\n\nsim_globe2 &lt;- function(p = 0.7, N = 9, x = 0.1){\n  \n  true_sample &lt;- sample(c(\"W\", \"L\"), size = N, prob = c(p, 1-p), replace = T)\n  \n  obs_sample &lt;- ifelse(runif(N) &lt; x,\n                       ifelse(true_sample == \"W\", \"L\", \"W\"),\n                       true_sample)\n  \n  return(obs_sample)\n  \n}\n\n\nhow do you know the error rate?\ndon’t understand mechanism behind incorporating x but understand why x needs to be incorporated + consequences of not"
  },
  {
    "objectID": "notes/notes-02.html#todo",
    "href": "notes/notes-02.html#todo",
    "title": "Lecture 02 - The Garden of Forking Data",
    "section": "TODO",
    "text": "TODO"
  },
  {
    "objectID": "notes/notes-10.html",
    "href": "notes/notes-10.html",
    "title": "Lecture 10 - Counts & Hidden Confounds",
    "section": "",
    "text": "Rose:\nThorn:"
  },
  {
    "objectID": "notes/notes-10.html#rose-thorn",
    "href": "notes/notes-10.html#rose-thorn",
    "title": "Lecture 10 - Counts & Hidden Confounds",
    "section": "",
    "text": "Rose:\nThorn:"
  },
  {
    "objectID": "notes/notes-10.html#generalized-linear-models",
    "href": "notes/notes-10.html#generalized-linear-models",
    "title": "Lecture 10 - Counts & Hidden Confounds",
    "section": "Generalized Linear Models",
    "text": "Generalized Linear Models\n\nExpected value is some function of an additive combination of parameters\nuniform changes in predictor not uniform changes in prediction\nall predictor variables interact and moderate one another\ninlfuences predictions and uncertainty of predictions"
  },
  {
    "objectID": "notes/notes-10.html#confounded-admissions",
    "href": "notes/notes-10.html#confounded-admissions",
    "title": "Lecture 10 - Counts & Hidden Confounds",
    "section": "Confounded Admissions",
    "text": "Confounded Admissions\n\nability is a confound for admissions\n\n\n\n\n\ninclude confound in the model:\n\n\n# continuing from UCBadmit example\n# what happens when there is a confound?\n\nset.seed(17)\nN &lt;- 2000 # number of applicants\n# even gender distribution\nG &lt;- sample( 1:2 , size=N , replace=TRUE )\n# sample ability, high (1) to average (0)\nu &lt;- rbern(N,0.1)\n# gender 1 tends to apply to department 1, 2 to 2\n# and G=1 with greater ability tend to apply to 2 as well\nD &lt;- rbern( N , ifelse( G==1 , u*0.5 , 0.8 ) ) + 1\n# matrix of acceptance rates [dept,gender]\naccept_rate_u0 &lt;- matrix( c(0.1,0.1,0.1,0.3) , nrow=2 )\naccept_rate_u1 &lt;- matrix( c(0.2,0.3,0.2,0.5) , nrow=2 )\n# simulate acceptance\np &lt;- sapply( 1:N , function(i) \n    ifelse( u[i]==0 , accept_rate_u0[D[i],G[i]] , accept_rate_u1[D[i],G[i]] ) )\nA &lt;- rbern( N , p )\n\ntable(G,D)\ntable(G,A)\n\ndat_sim &lt;- list( A=A , D=D , G=G )\n\n# total effect gender\nm1 &lt;- ulam(\n    alist(\n        A ~ bernoulli(p),\n        logit(p) &lt;- a[G],\n        a[G] ~ normal(0,1)\n    ), data=dat_sim , chains=4 , cores=4 )\n\npost1 &lt;- extract.samples(m1)\npost1$fm_contrast &lt;- post1$a[,1] - post1$a[,2]\nprecis(post1)\n\n# direct effects - now confounded!\nm2 &lt;- ulam(\n    alist(\n        A ~ bernoulli(p),\n        logit(p) &lt;- a[G,D],\n        matrix[G,D]:a ~ normal(0,1)\n    ), data=dat_sim , chains=4 , cores=4 )\n\nprecis(m2,3)\n\n# contrast\npost2 &lt;- extract.samples(m2)\npost2$fm_contrast_D1 &lt;- post2$a[,1,1] - post2$a[,2,1]\npost2$fm_contrast_D2 &lt;- post2$a[,1,2] - post2$a[,2,2]\nprecis(post2)\n\ndens( post2$fm_contrast_D1 , lwd=4 , col=4 , xlab=\"F-M contrast in each department\" )\ndens( post2$fm_contrast_D2 , lwd=4 , col=2 , add=TRUE )\nabline(v=0,lty=3)\n\ndens( post2$a[,1,1] , lwd=4 , col=2 , xlim=c(-3,1) )\ndens( post2$a[,2,1] , lwd=4 , col=4 , add=TRUE )\ndens( post2$fm_contrast_D1 , lwd=4 , add=TRUE )\n\ndens( post2$a[,1,2] , lwd=4 , col=2 , add=TRUE , lty=4 )\ndens( post2$a[,2,2] , lwd=4 , col=4 , add=TRUE , lty=4)\ndens( post2$fm_contrast_D2 , lwd=4 , add=TRUE , lty=4)\n\n\nsorting can mask a lot of things through collider bias"
  },
  {
    "objectID": "notes/notes-10.html#citation-networks",
    "href": "notes/notes-10.html#citation-networks",
    "title": "Lecture 10 - Counts & Hidden Confounds",
    "section": "Citation Networks",
    "text": "Citation Networks\n\n\n\n\n\n\nin the absence of strong causal assumptions, we can’t conclude anything\nproxies for quality are often poor proxies (e.g., citations)\nif you want causal inference, you must make causal assumptions"
  },
  {
    "objectID": "notes/notes-10.html#sensitivity-analysis",
    "href": "notes/notes-10.html#sensitivity-analysis",
    "title": "Lecture 10 - Counts & Hidden Confounds",
    "section": "Sensitivity Analysis",
    "text": "Sensitivity Analysis\n\nwhat are the implications of what we don’t know?\nassume confound exists, model its consequences for different strengths/kinds of influence\nhow strong must the confound be to change conclusions?\ninclude quality as an unobserved variable\n\nchange parameter size/strength to test the effect of unobserved confound\n\n\n\n# sensitivity\n\ndat_sim$D2 &lt;- ifelse( D==2 , 1 , 0 )\ndat_sim$b &lt;- c(1,1)\ndat_sim$g &lt;- c(1,0)\ndat_sim$N &lt;- length(dat_sim$D2)\n\nm3s &lt;- ulam(\n    alist( \n        # A model\n        A ~ bernoulli(p),\n        logit(p) &lt;- a[G,D] + b[G]*u[i],\n        matrix[G,D]:a ~ normal(0,1),\n\n        # D model\n        D2 ~ bernoulli(q),\n        logit(q) &lt;- delta[G] + g[G]*u[i],\n        delta[G] ~ normal(0,1),\n\n        # declare unobserved u\n        vector[N]:u ~ normal(0,1)\n    ), data=dat_sim , chains=4 , cores=4 )\n\nprecis(m3s,3,pars=c(\"a\",\"delta\"))\n\npost3s &lt;- extract.samples(m3s)\npost3s$fm_contrast_D1 &lt;- post3s$a[,1,1] - post3s$a[,2,1]\npost3s$fm_contrast_D2 &lt;- post3s$a[,1,2] - post3s$a[,2,2]\n\ndens( post2$fm_contrast_D1 , lwd=1 , col=4 , xlab=\"F-M contrast in each department\" , xlim=c(-2,1) )\ndens( post2$fm_contrast_D2 , lwd=1 , col=2 , add=TRUE )\nabline(v=0,lty=3)\ndens( post3s$fm_contrast_D1 , lwd=4 , col=4 , add=TRUE )\ndens( post3s$fm_contrast_D2 , lwd=4 , col=2 , add=TRUE )\n\nplot( jitter(u) , apply(post3s$u,2,mean) , col=ifelse(G==1,2,4) , lwd=3 )\n\n\nyou can say the strength of the confound needed to undo the results you found\n\nimportant thing to report - don’t pretend confounds don’t exist\ncan’t eliminate possibility of confounding\n\na lot of the most important science cannot be done experimentally so we need to be able to do these things"
  },
  {
    "objectID": "notes/notes-10.html#poisson-counts",
    "href": "notes/notes-10.html#poisson-counts",
    "title": "Lecture 10 - Counts & Hidden Confounds",
    "section": "Poisson Counts",
    "text": "Poisson Counts\n\n\n\n\n\n\ntotal count is not binomial: no maximum\n\nPoisson distribution: very high maximum and very low probability of each success\n\nPoisson distribution uses the log link - must be positive\n\nexponential scaling can be surprising\nlarge priors makes extremely long tails with very large values\nwant higher mean, lower variance\n\n\n\n# model\n\nlibrary(rethinking)\ndata(Kline)\nd &lt;- Kline\nd$P &lt;- scale( log(d$population) )\nd$contact_id &lt;- ifelse( d$contact==\"high\" , 2 , 1 )\n\ndat &lt;- list(\n    T = d$total_tools ,\n    P = d$P ,\n    C = d$contact_id )\n\n# intercept only\nm11.9 &lt;- ulam(\n    alist(\n        T ~ dpois( lambda ),\n        log(lambda) &lt;- a,\n        a ~ dnorm( 3 , 0.5 )\n    ), data=dat , chains=4 , log_lik=TRUE )\n\n# interaction model\nm11.10 &lt;- ulam(\n    alist(\n        T ~ dpois( lambda ),\n        log(lambda) &lt;- a[C] + b[C]*P,\n        a[C] ~ dnorm( 3 , 0.5 ),\n        b[C] ~ dnorm( 0 , 0.2 )\n    ), data=dat , chains=4 , log_lik=TRUE )\n\ncompare( m11.9 , m11.10 , func=PSIS )\n\nk &lt;- PSIS( m11.10 , pointwise=TRUE )$k\nplot( dat$P , dat$T , xlab=\"log population (std)\" , ylab=\"total tools\" ,\n    col=ifelse( dat$C==1 , 4 , 2 ) , lwd=4+4*normalize(k) ,\n    ylim=c(0,75) , cex=1+normalize(k) )\n# set up the horizontal axis values to compute predictions at\nP_seq &lt;- seq( from=-1.4 , to=3 , len=100 )\n\n# predictions for C=1 (low contact)\nlambda &lt;- link( m11.10 , data=data.frame( P=P_seq , C=1 ) )\nlmu &lt;- apply( lambda , 2 , mean )\nlci &lt;- apply( lambda , 2 , PI )\nlines( P_seq , lmu , lty=2 , lwd=1.5 )\nshade( lci , P_seq , xpd=TRUE , col=col.alpha(4,0.3) )\n\n# predictions for C=2 (high contact)\nlambda &lt;- link( m11.10 , data=data.frame( P=P_seq , C=2 ) )\nlmu &lt;- apply( lambda , 2 , mean )\nlci &lt;- apply( lambda , 2 , PI )\nlines( P_seq , lmu , lty=1 , lwd=1.5 )\nshade( lci , P_seq , xpd=TRUE , col=col.alpha(2,0.3))\n\nidentify( dat$P , dat$T , d$culture )\n\n# natural scale now\n\nplot( d$population , d$total_tools , xlab=\"population\" , ylab=\"total tools\" ,\n    col=ifelse( dat$C==1 , 4 , 2 ) , lwd=4+4*normalize(k) ,\n    ylim=c(0,75) , cex=1+normalize(k) )\nP_seq &lt;- seq( from=-5 , to=3 , length.out=100 )\n# 1.53 is sd of log(population)\n# 9 is mean of log(population)\npop_seq &lt;- exp( P_seq*1.53 + 9 )\nlambda &lt;- link( m11.10 , data=data.frame( P=P_seq , C=1 ) )\nlmu &lt;- apply( lambda , 2 , mean )\nlci &lt;- apply( lambda , 2 , PI )\nlines( pop_seq , lmu , lty=2 , lwd=1.5 )\nshade( lci , pop_seq , xpd=TRUE , col=col.alpha(4,0.3))\n\nlambda &lt;- link( m11.10 , data=data.frame( P=P_seq , C=2 ) )\nlmu &lt;- apply( lambda , 2 , mean )\nlci &lt;- apply( lambda , 2 , PI )\nlines( pop_seq , lmu , lty=1 , lwd=1.5 )\nshade( lci , pop_seq , xpd=TRUE , col=col.alpha(2,0.3) )\n\nidentify( d$population , d$total_tools , d$culture )\n\n\nnumber of effective parameters penalty shows how well the model performs after you drop individual data points\n\ntherefore models with more parameters often have lower effective parameters\n\ngamma-Poisson is the appropriate analog to a student t-test - wider tails\n\n\n# innovation/loss model\n\ndat2 &lt;- list( T=d$total_tools, P=d$population, C=d$contact_id )\nm11.11 &lt;- ulam(\n    alist(\n        T ~ dpois( lambda ),\n        lambda &lt;- exp(a[C])*P^b[C]/g,\n        a[C] ~ dnorm(1,1),\n        b[C] ~ dexp(1),\n        g ~ dexp(1)\n    ), data=dat2 , chains=4 , cores=4 , log_lik=TRUE )\n\nprecis(m11.11,2)\n\nplot( d$population , d$total_tools , xlab=\"population\" , ylab=\"total tools\" ,\n    col=ifelse( dat$C==1 , 4 , 2 ) , lwd=4+4*normalize(k) ,\n    ylim=c(0,75) , cex=1+normalize(k) )\nP_seq &lt;- seq( from=-5 , to=3 , length.out=100 )"
  },
  {
    "objectID": "notes/notes-10.html#count-glms",
    "href": "notes/notes-10.html#count-glms",
    "title": "Lecture 10 - Counts & Hidden Confounds",
    "section": "Count GLMs",
    "text": "Count GLMs\n\ndistributions from constraints\nmaximum entropy priors: binomial, Poisson, and extensions\nrobust regressions: beta-binomial, gamma-Poisson"
  },
  {
    "objectID": "notes/notes-13.html",
    "href": "notes/notes-13.html",
    "title": "Lecture 13 - Multilevel Adventures",
    "section": "",
    "text": "Rose:\nThorn:"
  },
  {
    "objectID": "notes/notes-13.html#rose-thorn",
    "href": "notes/notes-13.html#rose-thorn",
    "title": "Lecture 13 - Multilevel Adventures",
    "section": "",
    "text": "Rose:\nThorn:"
  },
  {
    "objectID": "notes/notes-13.html#multilevel-adventures",
    "href": "notes/notes-13.html#multilevel-adventures",
    "title": "Lecture 13 - Multilevel Adventures",
    "section": "Multilevel Adventures",
    "text": "Multilevel Adventures\n\ncluster: kinds of groups in the data\nfeatures: aspets of the model (parameters) that vary by cluster\ncluster (tanks) -&gt; features (survival)\nadd clusters = more index (categorical) variables, more population priors\nadd features = more parameters, more dimensions in each population prior"
  },
  {
    "objectID": "notes/notes-13.html#varying-effects",
    "href": "notes/notes-13.html#varying-effects",
    "title": "Lecture 13 - Multilevel Adventures",
    "section": "Varying Effects",
    "text": "Varying Effects\n\na way for us to try and estimate unmeasured confounds\nvarying effect strategy: unmeasured features of clusters leave an imprint on the data that can be measured by 1) repeat observations of each cluster and 2) partial pooling among clusters\npredictive perspective: important source of cluster-level variation, regularize\ncausal perspective: competing causes or unobserved confounds\ninterested in varying effects from a predictive and a causal perspective\nfixed effects: varying effects with variance fixed at infinity, no pooling"
  },
  {
    "objectID": "notes/notes-13.html#practical-difficulties",
    "href": "notes/notes-13.html#practical-difficulties",
    "title": "Lecture 13 - Multilevel Adventures",
    "section": "Practical Difficulties",
    "text": "Practical Difficulties\n\nvarying effects are a good default but come with difficulties\n\nhow to use more than one cluster type at the same time?\nhow to calculate predictions\nhow to sample chains efficiently\ngroup-level confounding"
  },
  {
    "objectID": "notes/notes-13.html#varying-districts",
    "href": "notes/notes-13.html#varying-districts",
    "title": "Lecture 13 - Multilevel Adventures",
    "section": "Varying Districts",
    "text": "Varying Districts\n\ncluster by district\nestimand: C in each district, partially pooled\nvarying intercept on each district\n\\(C_i \\sim Bernoulli(p_i)\\)\n\\(logit(p_i) = \\alpha_{D[i]}\\)\n\\(\\alpha_j \\sim Normal(\\bar{\\alpha}, \\sigma)\\)\n\\(\\bar{\\alpha} \\sim Normal(0,1)\\)\n\\(\\sigma \\sim Exponential(1)\\)\n\n\n# simple varying intercepts model\nlibrary(rethinking)\ndata(bangladesh)\nd &lt;- bangladesh\n\ndat &lt;- list(\n    C = d$use.contraception,\n    D = as.integer(d$district) )\n\nmCD &lt;- ulam(\n    alist(\n        C ~ bernoulli(p),\n        logit(p) &lt;- a[D],\n        vector[61]:a ~ normal(abar,sigma),\n        abar ~ normal(0,1),\n        sigma ~ exponential(1)\n    ) , data=dat , chains=4 , cores=4 )\n\n\n# plot estimates\np &lt;- link( mCD , data=list(D=1:61) )\n# blank2(w=2)\nplot( NULL , xlab=\"district\" , lwd=3 , col=2 , xlim=c(1,61), ylim=c(0,1) , ylab=\"prob use contraception\" )\n\npoints( 1:61 , apply(p,2,mean) , xlab=\"district\" , lwd=3 , col=2 , ylim=c(0,1) , ylab=\"prob use contraception\" )\n\n for ( i in 1:61 ) lines( c(i,i) , PI(p[,i]) , lwd=8 , col=col.alpha(2,0.5) )\n\n# show raw proportions - have to skip 54\nn &lt;- table(dat$D)\nCn &lt;- xtabs(dat$C ~ dat$D)\npC &lt;- as.numeric( Cn/n )\npC &lt;- c( pC[1:53] , NA , pC[54:60] )\npoints( pC , lwd=2 )\n\n# only some labels via locator\nn &lt;- table(dat$D)\nn &lt;- as.numeric(n)\nn &lt;- c( n[1:53] , 0 , n[54:60] )\nidentify( 1:61 , pC , labels=n , cex=1 )\n\n\npartial pooling shrinks districts with low sampling towards mean\n\nbetter predictions\n\nwhat is the effect of urban living? District features are potential group-level confounds\neach district\n\\(C_i \\sim Bernoulli(p_i)\\)\n\\(logit(p_i) = \\alpha_{D[i]} + \\beta_{D[i]}U_i\\)\n\\(\\alpha_j \\sim Normal(\\bar{\\alpha}, \\sigma)\\) = regularizing prior for rural\n\\(\\beta_j \\sim Normal(\\bar{\\beta}, \\tau)\\) = regularizing prior for urban effect\n\\(\\bar{\\alpha}, \\bar{\\beta} \\sim Normal(0,1)\\) = averages\n\\(\\sigma, \\tau \\sim Exponential(1)\\) = standard deviations\n\n\ndat &lt;- list(\n    C = d$use.contraception,\n    D = as.integer(d$district),\n    U = ifelse(d$urban==1,1,0) )\n\n# total U\nmCDU &lt;- ulam(\n    alist(\n        C ~ bernoulli(p),\n        logit(p) &lt;- a[D] + b[D]*U,\n        vector[61]:a ~ normal(abar,sigma),\n        vector[61]:b ~ normal(bbar,tau),\n        c(abar,bbar) ~ normal(0,1),\n        c(sigma,tau) ~ exponential(1)\n    ) , data=dat , chains=4 , cores=4 )\n\ntraceplot(mCDU,pars=\"tau\",lwd=2,n_cols=1)\ntrankplot(mCDU,pars=\"tau\",lwd=3,n_cols=1)\n\n# non-centered version\nmCDUnc &lt;- ulam(\n    alist(\n        C ~ bernoulli(p),\n        logit(p) &lt;- a[D] + b[D]*U,\n        # define effects using other parameters\n        save&gt; vector[61]:a &lt;&lt;- abar + za*sigma,\n        save&gt; vector[61]:b &lt;&lt;- bbar + zb*tau,\n        # z-scored effects\n        vector[61]:za ~ normal(0,1),\n        vector[61]:zb ~ normal(0,1),\n        # ye olde hyper-priors\n        c(abar,bbar) ~ normal(0,1),\n        c(sigma,tau) ~ exponential(1)\n    ) , data=dat , chains=4 , cores=4 )\n\n# plot estimates\n\nUval &lt;- 0\nxcol &lt;- ifelse(Uval==0,2,4)\np &lt;- link( mCDUnc , data=list(D=1:61,U=rep(Uval,61)) )\n# blank2(w=2,h=0.8)\nplot( NULL , xlab=\"district\" , lwd=3 , col=2 , xlim=c(1,61), ylim=c(0,1) , ylab=\"prob use contraception\" )\nabline(h=0.5,lty=2,lwd=0.5)\n\npoints( 1:61 , apply(p,2,mean) , xlab=\"district\" , lwd=3 , col=xcol , ylim=c(0,1) , ylab=\"prob use contraception\" )\n\n for ( i in 1:61 ) lines( c(i,i) , PI(p[,i]) , lwd=8 , col=col.alpha(xcol,0.5) )\n\n# show raw proportions - have to skip 54\nn &lt;- table(dat$D,dat$U)\nCn &lt;- xtabs(dat$C ~ dat$D + dat$U)\npC &lt;- as.numeric( Cn[,Uval+1]/n[,Uval+1] )\npC &lt;- c( pC[1:53] , NA , pC[54:60] )\npoints( pC , lwd=2 )\n\n# only some labels via locator\nnn &lt;- as.numeric(n[,Uval+1])\nnn &lt;- c( nn[1:53] , 0 , nn[54:60] )\nidentify( 1:61 , pC , labels=nn , cex=1 )\n\n# show standard deviations\npost &lt;- extract.samples(mCDUnc)\ndens(post$sigma,xlab=\"posterior standard deviation\",lwd=3,col=2,xlim=c(0,1.2))\ndens(post$tau,lwd=3,col=4,add=TRUE,adj=0.2)\ncurve(dexp(x,1),from=0,to=1.3,add=TRUE,lwd=2,lty=2)\n\n\npriors inside of priors is good for models but can create ill-fitting models\nthe more you cut up the data because of different varying effects, the sample sizes will inevitably get smaller -&gt; partial pooling helps"
  },
  {
    "objectID": "notes/notes-01.html",
    "href": "notes/notes-01.html",
    "title": "Lecture 01 - The Golem of Prague",
    "section": "",
    "text": "Rose: I LOVE DAGS. Really interesting to think about the non-uniqueness of null models in ecology.\nThorn: the difference between regression/intervention – have i always just done regression?"
  },
  {
    "objectID": "notes/notes-01.html#rose-thorn",
    "href": "notes/notes-01.html#rose-thorn",
    "title": "Lecture 01 - The Golem of Prague",
    "section": "",
    "text": "Rose: I LOVE DAGS. Really interesting to think about the non-uniqueness of null models in ecology.\nThorn: the difference between regression/intervention – have i always just done regression?"
  },
  {
    "objectID": "notes/notes-01.html#third-edition",
    "href": "notes/notes-01.html#third-edition",
    "title": "Lecture 01 - The Golem of Prague",
    "section": "Third Edition",
    "text": "Third Edition\n\npeach boxes instead of blue boxes"
  },
  {
    "objectID": "notes/notes-01.html#causal-inference",
    "href": "notes/notes-01.html#causal-inference",
    "title": "Lecture 01 - The Golem of Prague",
    "section": "Causal Inference",
    "text": "Causal Inference\n\nstatistical models require scientific (causal) models\ncorrelation is a very limited measure of association\n\nassociation can occur without correlation\n\ncausation requires intervention - it is not just the behaviour without intervention\ncausal prediction = prediction of the consequences of an intervention (implications of changing one variable on another variable)\n\nknowing the cause of an action allows you to create predictions\nwhat happens if I do this?\n\ncausal imputation = knowing the cause of an action allows you to reconstruct possible outcomes (i.e., what if I had done something else?)\nEven for description, causal models are required"
  },
  {
    "objectID": "notes/notes-01.html#dags",
    "href": "notes/notes-01.html#dags",
    "title": "Lecture 01 - The Golem of Prague",
    "section": "DAGs",
    "text": "DAGs\n\nabstract causal models: includes names of variables and their causal relationships\ntells you the consequences of an intervention\ntells you what you can decide/ask without additional assumptions\nfacilitates you asking scientific questions\neach causal query requires a different model"
  },
  {
    "objectID": "notes/notes-01.html#golems",
    "href": "notes/notes-01.html#golems",
    "title": "Lecture 01 - The Golem of Prague",
    "section": "Golems",
    "text": "Golems\n\nstatistical models = golems\noften not possible to design and outline a null hypothesis that is meaningful to reject in observational science\n\nwhat is a null ecological community?\n\nthink of good example/explanation for no null ecology/previous two slides\n\ntakeaway is that null hypothesis does not give you cause/process behind outcome\n\nwhat is your null? is it unique?"
  },
  {
    "objectID": "notes/notes-01.html#todo",
    "href": "notes/notes-01.html#todo",
    "title": "Lecture 01 - The Golem of Prague",
    "section": "TODO",
    "text": "TODO"
  },
  {
    "objectID": "notes/notes-18.html",
    "href": "notes/notes-18.html",
    "title": "Lecture 18 - Missing Data",
    "section": "",
    "text": "Rose:\nThorn:"
  },
  {
    "objectID": "notes/notes-18.html#rose-thorn",
    "href": "notes/notes-18.html#rose-thorn",
    "title": "Lecture 18 - Missing Data",
    "section": "",
    "text": "Rose:\nThorn:"
  },
  {
    "objectID": "notes/notes-18.html#missing-data",
    "href": "notes/notes-18.html#missing-data",
    "title": "Lecture 18 - Missing Data",
    "section": "Missing Data",
    "text": "Missing Data\n\nobserved data is a special case - we trick ourselves into believing there is no error\nmissing data = some cases unobserved\nnot totally missing - they have constraints and relationships to other variables"
  },
  {
    "objectID": "notes/notes-18.html#workflow",
    "href": "notes/notes-18.html#workflow",
    "title": "Lecture 18 - Missing Data",
    "section": "Workflow",
    "text": "Workflow\n\ndropping cases with missing values is sometimes justifiable\nright thing to do depends upon causal assumptions\nimputation is often beneficial/necessary\nBayesian imputation: compute posterior probability distribution of missing values\nMarginalizing unknowns: averaging over distribution of missing values"
  },
  {
    "objectID": "notes/notes-18.html#bayesian-imputation",
    "href": "notes/notes-18.html#bayesian-imputation",
    "title": "Lecture 18 - Missing Data",
    "section": "Bayesian Imputation",
    "text": "Bayesian Imputation\n\ncausal model of all variables implies strategy for imputation\nsometimes imputation is unnecessary, e.g., discrete parameters\nsometimes imputation is easier, e.g., censored observations"
  },
  {
    "objectID": "notes/notes-18.html#imputing-primates",
    "href": "notes/notes-18.html#imputing-primates",
    "title": "Lecture 18 - Missing Data",
    "section": "Imputing Primates",
    "text": "Imputing Primates\n\nmissing values already have probability distributions\nexpress causal model for each partially-observed variable\nreplace each missing value with a parameter\nnot the same as non-Bayesian imputation\n\nthat generates datasets and runs the model multiple times\nthis estimates probability distributions using other parameters and relationships\n\nimputation without relationships among predictors is risky"
  },
  {
    "objectID": "notes/notes-08.html",
    "href": "notes/notes-08.html",
    "title": "Lecture 08 - Markov chain Monte Carlo",
    "section": "",
    "text": "Rose: skateboarding metaphor\nThorn: not understanding parameters in model specification"
  },
  {
    "objectID": "notes/notes-08.html#rose-thorn",
    "href": "notes/notes-08.html#rose-thorn",
    "title": "Lecture 08 - Markov chain Monte Carlo",
    "section": "",
    "text": "Rose: skateboarding metaphor\nThorn: not understanding parameters in model specification"
  },
  {
    "objectID": "notes/notes-08.html#modelling-approaches",
    "href": "notes/notes-08.html#modelling-approaches",
    "title": "Lecture 08 - Markov chain Monte Carlo",
    "section": "Modelling Approaches",
    "text": "Modelling Approaches\n\nquadratic approximation makes strong assumptions about what the model looks like - approximately Gaussian\nMCMC is intensive but with less assumptions and more flexible"
  },
  {
    "objectID": "notes/notes-08.html#markov-chain-monte-carlo",
    "href": "notes/notes-08.html#markov-chain-monte-carlo",
    "title": "Lecture 08 - Markov chain Monte Carlo",
    "section": "Markov chain Monte Carlo",
    "text": "Markov chain Monte Carlo\n\nchain: sequence of draws from distribution\nMarkov chain: history doesn’t matter, just where you are now\nMonte Carlo: random simulation"
  },
  {
    "objectID": "notes/notes-08.html#hamiltonian-monte-carlo",
    "href": "notes/notes-08.html#hamiltonian-monte-carlo",
    "title": "Lecture 08 - Markov chain Monte Carlo",
    "section": "Hamiltonian Monte Carlo",
    "text": "Hamiltonian Monte Carlo\n\nuses random pathways that follow the distribution of the variables\n\n\ndag &lt;- dagify(\n  S ~ Q + J + X,\n  Q ~ X,\n  J ~ Z,\n  outcome = 'S',\n  latent = 'Q',\n    coords = list(x = c(Q = 0, X = 0, S = 1, J = 1, Z = 2),\n                y = c(X = 0, J = 0, Z = 0, Q = 1, S = 1))\n)\n\nggdag(dag) + theme_dag()\n\n\n\n\n\nEstimand: association between wine quality and wine origin. Stratify by judge for efficiency.\n\n\ndata(Wines2012)\nd &lt;- Wines2012\n\ndat &lt;- list(\n  S = standardize(d$score),\n  J = as.numeric(d$judge), \n  W = as.numeric(d$wine),\n  X = ifelse(d$wine.amer == 1,1,2),\n  Z = ifelse(d$judge.amer == 1,1,2)\n)\n\nmQ &lt;- ulam(alist(\n  S ~ dnorm(mu, sigma), \n  mu &lt;- Q[W],\n  Q[W] ~ dnorm(0, 1), \n  sigma ~ dexp(1)),\n  data = dat, chains = 4, cores = 4)\n\nprecis(mQ, 2)\n\n\ntrace plots: visualization of the Markov chain\nwant more than one chain in order to check convergence\n\nconvergence: each chain explores the right distribution and every chain explores the same distribution\n\nR-hat is chain convergence diagnostic\n\nvariance ratio\nif chains do converge, beginning of the chain and end of chain should be exploring the same place and therefore the chain is stationary\nas total variance (among chains) approaches average variance within chains, R-hat approaches 1\ndoes not guarantee convergence but gives an idea that the chains are working when ~ 1\n\nn_eff = effective samples\n\napproximation of how long the chain would be if each sample was completely independent of the one before it\nwhen samples are autocorrelated, you have fewer effective samples\ntypically n_eff is smaller than number of samples you actually took\n\n\n\nmQO &lt;- ulam(alist(\n  S ~ dnorm(mu, sigma), \n  mu &lt;- Q[W] + O[X],\n  Q[W] ~ dnorm(0, 1),\n  O[X] ~ dnorm(0, 1),\n  sigma ~ dexp(1)),\n  data = dat, chains = 4, cores = 4)\n\nplot(precis(mQO, 2))\n\nmQOJ &lt;- ulam(alist(\n  S ~ dnorm(mu, sigma), \n  mu &lt;- (Q[W] + O[X] - H[J])*D[J],\n  Q[W] ~ dnorm(0, 1),\n  O[X] ~ dnorm(0, 1),\n  H[J] ~ dnorm(0, 1),\n  D[J] ~ dexp(1),\n  sigma ~ dexp(1)),\n  data = dat, chains = 4, cores = 4)\n\nplot(precis(mQOJ, 2))\n\n\noften if there is an issue, it is with your model\n\nloop back to basic scientific questions and assumptions\ndivergent transitions are one of the signs of this - “rejected proposal”"
  },
  {
    "objectID": "notes/notes-listing.html",
    "href": "notes/notes-listing.html",
    "title": "Notes",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\nDate\n\n\nAuthor\n\n\n\n\n\n\nLecture 01 - The Golem of Prague\n\n\nInvalid Date\n\n\nIsabella C. Richmond\n\n\n\n\nLecture 02 - The Garden of Forking Data\n\n\nJan 23, 2023\n\n\nIsabella C. Richmond\n\n\n\n\nLecture 03 - Geocentric Models\n\n\nFeb 8, 2023\n\n\nIsabella C. Richmond\n\n\n\n\nLecture 04 - Categories & Curves\n\n\nFeb 8, 2023\n\n\nIsabella C. Richmond\n\n\n\n\nLecture 05 - Elemental Confounds\n\n\nFeb 22, 2023\n\n\nIsabella C. Richmond\n\n\n\n\nLecture 06 - Good & Bad Controls\n\n\nFeb 22, 2023\n\n\nIsabella C. Richmond\n\n\n\n\nLecture 07 - Fitting Over & Under\n\n\nMar 8, 2023\n\n\nIsabella C. Richmond\n\n\n\n\nLecture 08 - Markov chain Monte Carlo\n\n\nMar 8, 2023\n\n\nIsabella C. Richmond\n\n\n\n\nLecture 09 - Modeling Events\n\n\nMar 22, 2023\n\n\nIsabella C. Richmond\n\n\n\n\nLecture 10 - Counts & Hidden Confounds\n\n\nMar 22, 2023\n\n\nIsabella C. Richmond\n\n\n\n\nLecture 11 - Ordered Categories\n\n\nApr 5, 2023\n\n\nIsabella C. Richmond\n\n\n\n\nLecture 12 - Multilevel Models\n\n\nApr 5, 2023\n\n\nIsabella C. Richmond\n\n\n\n\nLecture 13 - Multilevel Adventures\n\n\nApr 18, 2023\n\n\nIsabella C. Richmond\n\n\n\n\nLecture 14 - Correlated Features\n\n\nApr 19, 2023\n\n\nIsabella C. Richmond\n\n\n\n\nLecture 17 - Measurement & Misclassification\n\n\nMay 5, 2023\n\n\nIsabella C. Richmond\n\n\n\n\nLecture 18 - Missing Data\n\n\nMay 5, 2023\n\n\nIsabella C. Richmond\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "notes/notes-03.html",
    "href": "notes/notes-03.html",
    "title": "Lecture 03 - Geocentric Models",
    "section": "",
    "text": "Rose: Non-independent variables in a summary table\nThorn: Stupid resolution to my plot coding issue"
  },
  {
    "objectID": "notes/notes-03.html#rose-thorn",
    "href": "notes/notes-03.html#rose-thorn",
    "title": "Lecture 03 - Geocentric Models",
    "section": "",
    "text": "Rose: Non-independent variables in a summary table\nThorn: Stupid resolution to my plot coding issue"
  },
  {
    "objectID": "notes/notes-03.html#linear-regressions",
    "href": "notes/notes-03.html#linear-regressions",
    "title": "Lecture 03 - Geocentric Models",
    "section": "Linear Regressions",
    "text": "Linear Regressions\n\nessentially a geocentric model - overly simplified\nseparate from a causal model\nassociations are from the causal model, not the statistical model"
  },
  {
    "objectID": "notes/notes-03.html#gaussian-distributions",
    "href": "notes/notes-03.html#gaussian-distributions",
    "title": "Lecture 03 - Geocentric Models",
    "section": "Gaussian Distributions",
    "text": "Gaussian Distributions\n\nthere are many more ways to end up in the center than to end up on the periphery\nwhy the Gaussian distribution spontaneously occurs in natural systems\ngenerative: if we add fluctuations, we tend towards normal distribution (lots of summed fluctuations in nature)\ninferential: estimating mean and variance, normal distribution is best one to use because it is least informative (no other information present)\nnormal distribution is just a tool for estimating mean/variance because it has widest distribution\n\ndata doesn’t have to be normal to be able to leverage the tool to estimate mean/variance"
  },
  {
    "objectID": "notes/notes-03.html#workflow",
    "href": "notes/notes-03.html#workflow",
    "title": "Lecture 03 - Geocentric Models",
    "section": "Workflow",
    "text": "Workflow\n\nState a clear question\nSketch causal assumptions\nDefine generative model from sketch\nUse generative model to build estimator\nProfit\n\n\nlibrary(rethinking)\nlibrary(dagitty)\ndata(Howell1)"
  },
  {
    "objectID": "notes/notes-03.html#describing-models",
    "href": "notes/notes-03.html#describing-models",
    "title": "Lecture 03 - Geocentric Models",
    "section": "Describing Models",
    "text": "Describing Models\n= is deterministic\n\n~ is distributional"
  },
  {
    "objectID": "notes/notes-03.html#howell-example",
    "href": "notes/notes-03.html#howell-example",
    "title": "Lecture 03 - Geocentric Models",
    "section": "Howell Example",
    "text": "Howell Example\n\nQuestion/Estimand: Describe association between adult weight and height\n\n\nd2 &lt;- Howell1[Howell1$age&gt;=18,]\n\n\nScientific model: weight is some function of height and unobserved influences\n\n\nd &lt;- dagitty(\"dag {\n                H -&gt; W\n                U -&gt; W\n             }\")\n\ndrawdag(d)\n\n\n\n\n\\[\nW = f(H, U)\n\\]\n\nGenerative/Statistical model\n\n\nTwo options: dynamic (complex and ongoing) and static\n\nStatic model allows us to imagine changes at specific times and still use a Gaussian distribution\n\nFor adults, weight is a proportion of height plus the influence of unobserved causes\n\n\\[\nW = \\beta H+U\n\\]\n\nsim_weight &lt;- function(H, b, sd){\n  U &lt;- rnorm(length(H), 0, sd)\n  W &lt;- b*H + U\n  return(W)\n}\n\nH &lt;- runif(200, min = 130, max = 170)\nW &lt;- sim_weight(H, b = 0.5, sd = 5)\nplot(W ~ H, col = 2, lwd = 3)\n\n\n\n\n\ncan adjust to produce biologically unrealistic relationships by adjusting b to be large or small\n\n\\[ W_i = \\beta H_i + U_i \\\\ U_i \\sim Normal(0, \\sigma) \\\\ H_i \\sim Uniform(130, 170) \\]\n\n~ indicates distribution\n= is deterministic\nwe want to estimate how the average weight changes with height\n\\[\nE(W_{i}|H_{i}) = \\alpha + \\beta H_{i}\n\\]\n\\(E(W_{i}|H_{i})\\) = average weight conditional on height\n\\(\\alpha\\) = intercept (when height is 0, what is weight? Scientifically should be zero but putting it in model to make sure our model is scientifically sound)\n\\(\\beta\\) = slope\nPosterior distribution:\n\n\\[\nPr(\\alpha,\\beta,\\sigma| H_{i}, W_{i}) = \\frac{Pr(W_{i}|H_{i}, \\alpha,\\beta,\\sigma)Pr(\\alpha,\\beta,\\sigma)}{Z}\n\\]\nalpha, beta, sigma are our three unknowns: alpha and beta define the line and sigma defines the error around it\nunknowns are conditional (|) on the data Hi and Wi\nalpha, sigma, beta are unknown so we need a posterior distribution for them (and they are dependent on the data)\n\\(Pr(\\alpha,\\beta,\\sigma| H_{i}, W_{i})\\) = posterior probability of specific line\n\\(Pr(W_{i}|H_{i}, \\alpha,\\beta,\\sigma)\\) = probability of each weight dependent on height value, alpha, beta, sigma (garden of forking data)\n\\(Pr(\\alpha,\\beta,\\sigma)\\) = prior\nZ = normalizing constant\n\\(W_{i} \\sim Normal(\\mu _{i}), \\sigma)\\): linear model\n\\(\\mu _{i} = \\alpha + \\beta H_{i}\\)\n\nQuadratic approximation (continuous version of grid approximation)\n\napproximate posterior distribution as a multivariate Gaussian distrubution\nposteriors are often Gaussian\n\\(W_{i} \\sim Normal(\\mu _{i}), \\sigma)\\)\n\\(\\mu _{i} = \\alpha + \\beta H_{i}\\)\n\\(\\alpha \\sim Normal(0,10)\\)\n\\(\\beta \\sim Uniform(0,1)\\)\n\\(\\sigma \\sim Uniform(0, 10)\\)\n\n\n\nH &lt;- runif(10, 130, 170)\nW &lt;- sim_weight(H, b = 0.5, sd = 5)\n\nm3.1 &lt;- quap(alist(\n  W ~ dnorm(mu, sigma),\n  mu &lt;- a + b*H,\n  a ~ dnorm(0, 10),\n  b ~ dunif(0, 1),\n  sigma ~ dunif(0, 10)\n), data = list(W=W, H=H))\n\n\nPriors\n\nwe want to constrain to scientifically plausible values\njustify with information outside the data - like the rest of model\npriors have no essential differences from posteriors except for data – we can use them as generative models and ensure that our assumptions are reasonable\n\n\n\nn &lt;- 1e3\na &lt;- rnorm(n, 0, 10)\nb &lt;- runif(n, 0, 1)\nplot(NULL, xlim = c(130, 170), ylim = c(50, 90)) + \nfor (j in 1:50) abline(a=a[j], b=b[j], col = 'red') \n\n\n\n\ninteger(0)\n\n\n\nValidate model\n\n\ntest statistical model with simulated observations from scientific model - need to test if your model is working\nuse many values and make sure your model responds appropriately\n\n\n# model summary\nprecis(m3.1)\n\n           mean         sd        5.5%      94.5%\na     1.2508639 9.47623652 -13.8939923 16.3957201\nb     0.5060533 0.06383068   0.4040396  0.6080671\nsigma 6.8526424 1.53444846   4.4002974  9.3049874\n\n\n\nAnalyze data\n\n\ndat &lt;- list(W = d2$weight, H = d2$height)\nm3.2 &lt;- quap(alist(\n  W ~ dnorm(mu, sigma),\n  mu &lt;- a + b*H,\n  a ~ dnorm(0, 10), \n  b ~ dunif(0, 1),\n  sigma ~ dunif(0, 10)\n), data = dat)\n\nprecis(m3.2)\n\n             mean         sd        5.5%       94.5%\na     -43.3729992 4.16867670 -50.0353498 -36.7106487\nb       0.5717157 0.02693731   0.5286647   0.6147667\nsigma   4.2500254 0.16148270   3.9919448   4.5081059\n\n\n\nparameters are not independent of one another, they cannot be independently interpreted\n\nuse posterior predictions and describe/interpret those by sampling the posterior distribution\n\nPosterior Predictive Distribution\n\n\npost &lt;- extract.samples(m3.2)\nplot(d2$height, d2$weight) + \nfor (j in 1:20) abline(a=post$a[j], b=post$b[j])\n\n\n\n\ninteger(0)\n\nheight_seq &lt;- seq(130, 190, len = 20)\nW_postpred &lt;- sim(m3.2, data = list(H=height_seq))\nW_PI &lt;- apply(W_postpred, 2, PI)\n\nplot(d2$height, d2$weight) + \nfor (j in 1:20) abline(a=post$a[j], b=post$b[j]) +\nlines(height_seq, W_PI[1,], lty = 2) +\nlines(height_seq, W_PI[2,], lty = 2)\n\n\n\n\ninteger(0)"
  },
  {
    "objectID": "notes/notes-03.html#todo",
    "href": "notes/notes-03.html#todo",
    "title": "Lecture 03 - Geocentric Models",
    "section": "TODO:",
    "text": "TODO:"
  },
  {
    "objectID": "notes/notes-06.html",
    "href": "notes/notes-06.html",
    "title": "Lecture 06 - Good & Bad Controls",
    "section": "",
    "text": "Rose: identifying and working through bad controls\nThorn:"
  },
  {
    "objectID": "notes/notes-06.html#rose-thorn",
    "href": "notes/notes-06.html#rose-thorn",
    "title": "Lecture 06 - Good & Bad Controls",
    "section": "",
    "text": "Rose: identifying and working through bad controls\nThorn:"
  },
  {
    "objectID": "notes/notes-06.html#randomization",
    "href": "notes/notes-06.html#randomization",
    "title": "Lecture 06 - Good & Bad Controls",
    "section": "Randomization",
    "text": "Randomization\n\nrandomizing the treatment can remove the confound (only available for experiments)"
  },
  {
    "objectID": "notes/notes-06.html#causal-thinking",
    "href": "notes/notes-06.html#causal-thinking",
    "title": "Lecture 06 - Good & Bad Controls",
    "section": "Causal Thinking",
    "text": "Causal Thinking\n\nin an experiment, we cut causes of the treatment -&gt; we randomize\nsimulating intervention mimics randomization\ndo(X) means intervene on X\nexample: simple confound\n\n\ndag &lt;- dagify(\n\n    X ~ U,\n\n    Y ~ U + X\n\n)\n\nggdag(dag) +\n\n    theme_dag()\n\n\n\n\n\nstratifying by U removes causal relationship and allows to test effect of X -&gt; Y\nmarginalize or average over control variables\n\nthe coefficient is not usually satisfactory, need to marginalize\n\n\n\ndag &lt;- dagify(\n\n    Baboons ~ Cheetahs,\n\n    Gazelle ~ Baboons + Cheetahs\n\n)\n\nggdag(dag) +\n\n    theme_dag()\n\n\n\n\n\npopulations of each of these species influences the other\n\nwhen cheetahs are present, baboons are scared and do not influence gazelle population\nwhen cheetahs are absent, baboons eat and regulate gazelle population\nto assess causal effect of baboons, need to average over cheetah population"
  },
  {
    "objectID": "notes/notes-06.html#do-calculus",
    "href": "notes/notes-06.html#do-calculus",
    "title": "Lecture 06 - Good & Bad Controls",
    "section": "Do-Calculus",
    "text": "Do-Calculus\n\nallows us to determine if it is possible to answer our question using a DAG\nbackdoor criterion\n\nshortcut to apply do-calculus to use your eyes\nrule to find a set of variables to stratify by to yield estimate of our estimand\n\n\nidentify all paths connecting treatment (X) to outcome (Y)\npaths with arrows entering X are backdoor paths (non-causal paths)\nfind adjustment set that closes/blocks all backdoor paths\n\n\n\n\n\n# simulate confounded Y\nN &lt;- 200\nb_XY &lt;- 0\nb_UY &lt;- -1\nb_UZ &lt;- -1\nb_ZX &lt;- 1\n\nset.seed(10)\nU &lt;- rbern(N)\nZ &lt;- rnorm(N, b_UZ*U)\nX &lt;- rnorm(N, b_ZX*Z)\nY &lt;- rnorm(N, b_XY*X + b_UY*U)\nd &lt;- list(Y=Y, X=X, Z=Z)\n\n# ignore U,Z \nm_YX &lt;- quap(alist(\n  Y ~ dnorm(mu, sigma),\n  mu &lt;- a + b_XY*X,\n  a ~ dnorm(0,1),\n  b_XY ~ dnorm(0, 1),\n  sigma ~ dexp(1)\n), data = d)\n\n# stratify by Z \nm_YXZ &lt;- quap(alist(\n    Y ~ dnorm(mu, sigma),\n  mu &lt;- a + b_XY*X + b_Z*Z,\n  a ~ dnorm(0,1),\n  c(b_XY, b_Z) ~ dnorm(0, 1),\n  sigma ~ dexp(1)\n), data = d)\n\npost &lt;- extract.samples(m_YX)\npost2 &lt;- extract.samples(m_YXZ)\ndens(post$b_XY)\n\n\n\n#dens(post2$b_XY, add = TRUE)\n\n\nany variable you add to a model as part of the adjustment set (ie to control for), its coefficients are usually not interpretable\nminimum adjustment set is not always the best set"
  },
  {
    "objectID": "notes/notes-06.html#good-bad-controls",
    "href": "notes/notes-06.html#good-bad-controls",
    "title": "Lecture 06 - Good & Bad Controls",
    "section": "Good & Bad Controls",
    "text": "Good & Bad Controls\n\ncontrol variable: variable introduced to an analysis so that a causal estimate is possible\n\ngood and bad controls\n\nvariables not being collinear is not a good reason for including/excluding variables\n\ncollinearity can arise from many statistical processes\n\npost-treatment variables are often risky controls\nif there is no backdoor path to variable of interest, you don’t need to control for it\n\n\n# sim confounding by post-treatment variable\n\nf &lt;- function(n=100,bXZ=1,bZY=1) {\n    X &lt;- rnorm(n)\n    u &lt;- rnorm(n)\n    Z &lt;- rnorm(n, bXZ*X + u)\n    Y &lt;- rnorm(n, bZY*Z + u )\n    bX &lt;- coef( lm(Y ~ X) )['X']\n    bXZ &lt;- coef( lm(Y ~ X + Z) )['X']\n    return( c(bX,bXZ) )\n}\n\nsim &lt;- mcreplicate( 1e4 , f(bZY=0), mc.cores = 1)\n\n[ 1000 / 10000 ]\n[ 2000 / 10000 ]\n[ 3000 / 10000 ]\n[ 4000 / 10000 ]\n[ 5000 / 10000 ]\n[ 6000 / 10000 ]\n[ 7000 / 10000 ]\n[ 8000 / 10000 ]\n[ 9000 / 10000 ]\n[ 10000 / 10000 ]\n\ndens( sim[1,] , lwd=3 , xlab=\"posterior mean\" , xlim=c(-1,0.8) , ylim=c(0,2.7)  )\n\n\n\n#dens( sim[2,] , lwd=3 , col=2 , add=TRUE )\n\n\ncase control bias (selection on outcome)\n\nvery bad to add descendents of your outcome to your model\nweakly stratifying by the outcome (e.g., stratifying by Z)\n\n\n\ndag &lt;- dagify(\n    Y ~ X,\n    Z ~ Y\n)\n\nggdag(dag) +\n    theme_dag()\n\n\n\n\n\nf &lt;- function(n=100,bXY=1,bYZ=1) {\n    X &lt;- rnorm(n)\n    Y &lt;- rnorm(n, bXY*X )\n    Z &lt;- rnorm(n, bYZ*Y )\n    bX &lt;- coef( lm(Y ~ X) )['X']\n    bXZ &lt;- coef( lm(Y ~ X + Z) )['X']\n    return( c(bX,bXZ) )\n}\n\nsim &lt;- mcreplicate( 1e4 , f(), mc.cores = 1 )\n\n[ 1000 / 10000 ]\n[ 2000 / 10000 ]\n[ 3000 / 10000 ]\n[ 4000 / 10000 ]\n[ 5000 / 10000 ]\n[ 6000 / 10000 ]\n[ 7000 / 10000 ]\n[ 8000 / 10000 ]\n[ 9000 / 10000 ]\n[ 10000 / 10000 ]\n\ndens( sim[1,] , lwd=3 , xlab=\"posterior mean\" , xlim=c(0,1.5) , ylim=c(0,5)  )\n\n\n\n#dens( sim[2,] , lwd=3 , col=2 , add=TRUE )\n\n\nprecision parasite\n\nno backdoors because Z is not connected to Y except through X\nnot good to stratify Z because you are explaining part of the effect of X with Z\n\n\n\ndag &lt;- dagify(\n    Y ~ X,\n    X ~ Z\n)\n\nggdag(dag) +\n    theme_dag()\n\n\n\n\n\nf &lt;- function(n=100,bZX=1,bXY=1) {\n    Z &lt;- rnorm(n)\n    X &lt;- rnorm(n, bZX*Z )\n    Y &lt;- rnorm(n, bXY*X )\n    bX &lt;- coef( lm(Y ~ X) )['X']\n    bXZ &lt;- coef( lm(Y ~ X + Z) )['X']\n    return( c(bX,bXZ) )\n}\n\nsim &lt;- mcreplicate( 1e4 , f(n=50), mc.cores = 1 )\n\n[ 1000 / 10000 ]\n[ 2000 / 10000 ]\n[ 3000 / 10000 ]\n[ 4000 / 10000 ]\n[ 5000 / 10000 ]\n[ 6000 / 10000 ]\n[ 7000 / 10000 ]\n[ 8000 / 10000 ]\n[ 9000 / 10000 ]\n[ 10000 / 10000 ]\n\ndens( sim[1,] , lwd=3 , xlab=\"posterior mean\" , xlim=c(0.5,1.5) , ylim=c(0,4.5)  )\ndens( sim[2,] , lwd=3 , col=2 , add=TRUE )\n\n\n\n\n\nbias amplification\n\nX and Y confounded by u\nadding Z biases your answer because it “double” activates the confound\n\n\n\ndag &lt;- dagify(\n    Y ~ X + u,\n    X ~ Z + u\n)\n\nggdag(dag) +\n    theme_dag()\n\n\n\n\n\nf &lt;- function(n=100,bZX=1,bXY=1) {\n    Z &lt;- rnorm(n)\n    u &lt;- rnorm(n)\n    X &lt;- rnorm(n, bZX*Z + u )\n    Y &lt;- rnorm(n, bXY*X + u )\n    bX &lt;- coef( lm(Y ~ X) )['X']\n    bXZ &lt;- coef( lm(Y ~ X + Z) )['X']\n    return( c(bX,bXZ) )\n}\n\nsim &lt;- mcreplicate( 1e4 , f(bXY=0), mc.cores = 1)\n\n[ 1000 / 10000 ]\n[ 2000 / 10000 ]\n[ 3000 / 10000 ]\n[ 4000 / 10000 ]\n[ 5000 / 10000 ]\n[ 6000 / 10000 ]\n[ 7000 / 10000 ]\n[ 8000 / 10000 ]\n[ 9000 / 10000 ]\n[ 10000 / 10000 ]\n\ndens( sim[1,] , lwd=3 , xlab=\"posterior mean\" , xlim=c(-0.5,1) , ylim=c(0,5.5)  )\n\n\n\n#dens( sim[2,] , lwd=3 , col=2 , add=TRUE )\n\nabline_w &lt;- function(...,col=1,lwd=1,dlwd=2) {\n    abline(...,col=\"white\",lwd=lwd+dlwd)\n    abline(...,col=col,lwd=lwd)\n}\n\nn &lt;- 1000\nZ &lt;- rbern(n)\nu &lt;- rnorm(n)\nX &lt;- rnorm(n, 7*Z + u )\nY &lt;- rnorm(n, 0*X + u )\n\ncols &lt;- c( col.alpha(2,0.5) , col.alpha(4,0.5) )\nplot( X , Y  , col=cols[Z+1] , lwd=2 )\n\n\n\n#abline_w( lm(Y~X) , lwd=3 )\n#\n#abline_w( lm(Y[Z==1]~X[Z==1]) , lwd=3 , col=4 )\n#\n#abline_w( lm(Y[Z==0]~X[Z==0]) , lwd=3 , col=2 )"
  },
  {
    "objectID": "notes/notes-06.html#summary",
    "href": "notes/notes-06.html#summary",
    "title": "Lecture 06 - Good & Bad Controls",
    "section": "Summary",
    "text": "Summary\n\nadding control variables can be worse than omitting\nthere are good controls - backdoor criterion\nmake assumptions explicit"
  },
  {
    "objectID": "notes/notes-06.html#bonus---table-2-fallacy",
    "href": "notes/notes-06.html#bonus---table-2-fallacy",
    "title": "Lecture 06 - Good & Bad Controls",
    "section": "Bonus - Table 2 Fallacy",
    "text": "Bonus - Table 2 Fallacy\n\nnot all coefficients are causal effects\nstatistical model designed to identify X -&gt; Y will not also identify effects of control variables\n\\(Y_i \\sim Normal(\\mu_i, \\sigma)\\)\n\\(\\mu_i = \\alpha + \\beta_xX_i + \\beta_SS_i + \\beta_AA_i\\)\nthink through DAG for each control variable to see what the coefficient actually means\nno interpretation without causal representation"
  },
  {
    "objectID": "notes/notes-06.html#todo",
    "href": "notes/notes-06.html#todo",
    "title": "Lecture 06 - Good & Bad Controls",
    "section": "TODO",
    "text": "TODO\n\nread Table 2 Fallacy paper"
  },
  {
    "objectID": "notes/notes-07.html",
    "href": "notes/notes-07.html",
    "title": "Lecture 07 - Fitting Over & Under",
    "section": "",
    "text": "Rose: not trying to be perfect, just better\nThorn: thinking through prediction vs causality - when do I want prediction?"
  },
  {
    "objectID": "notes/notes-07.html#rose-thorn",
    "href": "notes/notes-07.html#rose-thorn",
    "title": "Lecture 07 - Fitting Over & Under",
    "section": "",
    "text": "Rose: not trying to be perfect, just better\nThorn: thinking through prediction vs causality - when do I want prediction?"
  },
  {
    "objectID": "notes/notes-07.html#problems-of-prediction",
    "href": "notes/notes-07.html#problems-of-prediction",
    "title": "Lecture 07 - Fitting Over & Under",
    "section": "Problems of Prediction",
    "text": "Problems of Prediction\n\nwhat function describes the data (fitting, compression)\nwhat functions explains these points (causal inference)\nwhat would happen if we changed the data (intervention)\nwhat is the next observation from the same process (prediction)\n\nprediction is the absence of intervention\nprediction does not require causal inference\n\nLeave-one-out cross-validation\n\n\n\ndrop one point\nfit line to remaining\npredict dropped point\nrepeat (1) with next point\nscore is error on dropped\n\n\ntask you use to assess the expected predictive accuracy of a statistical procedure\nscore in: fit to the sample / score out: fit to prediction\nLPPD (log posterior probability of observation) used for cross-validation because it includes the entire posterior\nmore flexible patterns generally perform better in sample and worse out of sample"
  },
  {
    "objectID": "notes/notes-07.html#cross-validation",
    "href": "notes/notes-07.html#cross-validation",
    "title": "Lecture 07 - Fitting Over & Under",
    "section": "Cross-Validation",
    "text": "Cross-Validation\n\nfor simple models, more parameters improves fit to sample BUT may reduce accuracy of predictions out of sample\naccurate models trade off flexibility with overfitting"
  },
  {
    "objectID": "notes/notes-07.html#regularization",
    "href": "notes/notes-07.html#regularization",
    "title": "Lecture 07 - Fitting Over & Under",
    "section": "Regularization",
    "text": "Regularization\n\noverfitting depends upon the priors\ndon’t be too excited about every point in the sample, because not every point in the sample is regular (not all points are representative)\nskeptical priors regularize models - have tighter variance that reduces flexibility\n\ndownweights improbable values\n\nskeptical priors improve model prediction - regularize so that models learn regular features and ignore irregular features\n\nthere is such a thing as too tight priors\n\nRegularizing priors -&gt; for pure prediction uses, you can tune the prior using cross-validation\n\ncausal inference uses science to choose priors"
  },
  {
    "objectID": "notes/notes-07.html#prediction-penalty",
    "href": "notes/notes-07.html#prediction-penalty",
    "title": "Lecture 07 - Fitting Over & Under",
    "section": "Prediction Penalty",
    "text": "Prediction Penalty\n\nFor N points, cross-validation requires fitting N models\n\nfeasible for few data points but for many data points gets unwieldy\n\nImportance sampling (PSIS) and information criteria (WAIC) allow you to assess prediction penalty from one model posterior distribution (for predictive models)\nWAIC, PSIS, cross-validation (CV) measure overfitting\n\nregularization manages overfiting\n\nCausal inference is not addressed by measuring or addressing overfitting\n\nthese tools are addressing the performance of a predictive model, not a causal model\nshould not select causal models based on these values because they are not associated with causality"
  },
  {
    "objectID": "notes/notes-07.html#model-mis-selection",
    "href": "notes/notes-07.html#model-mis-selection",
    "title": "Lecture 07 - Fitting Over & Under",
    "section": "Model Mis-selection",
    "text": "Model Mis-selection\n\nDo not use predictive criteria (WAIC, PSIS, CV) to choose a causal estimate\nPredictive criteria prefer confounds and colliders\n\nimprove predictive accuracy"
  },
  {
    "objectID": "notes/notes-07.html#outliers-robust-regression",
    "href": "notes/notes-07.html#outliers-robust-regression",
    "title": "Lecture 07 - Fitting Over & Under",
    "section": "Outliers & Robust Regression",
    "text": "Outliers & Robust Regression\n\nsome points are more influential than others - ‘outliers’\noutliers are information - don’t necessarily want to remove them\n\nbut they often have high leverage/weight because they are “surprising”\ndropping outliers ignores the problem - predictions will still be bad\nmodel is wrong, not the data\n\ncan quantify the influence of each point on the posterior distribution using cross-validation\ncan also use a mixture model/robust regression to address outliers\ndivorce rate example\n\nMaine and Idaho are outliers in divorce/age relationship\nquantify influence of outliers using PSIS k statistic or WAIC penalty term\nunmodelled sources of variation cause outliers -&gt; error distributions are not constant across the sample\n\nassuming that the dataset has multiple error distributions, with the same mean but different variations indicates that you are using a student t-test\nGaussian distribution has extremely thin tails - very skeptical\nstudent t distribution is much less skeptical, wider tails, much less influenced by outliers + more robust\n\n\n\n\ndata(WaffleDivorce)\nd &lt;- WaffleDivorce\n\n# model\ndat &lt;- list(\n    D = standardize(d$Divorce),\n    M = standardize(d$Marriage),\n    A = standardize(d$MedianAgeMarriage)\n)\n\nm5.3 &lt;- quap(alist(\n  D ~ dnorm(mu, sigma), \n  mu &lt;- a + bM*M + bA*A,\n  a ~ dnorm(0, 0.2),\n  bM ~ dnorm(0, 0.5), \n  bA ~ dnorm(0, 0.5),\n  sigma ~ dexp(1)\n), data = dat)\n\nm5.3t &lt;- quap(alist(\n  D ~ dstudent(2, mu, sigma), \n  mu &lt;- a + bM*M + bA*A,\n  a ~ dnorm(0, 0.2),\n  bM ~ dnorm(0, 0.5), \n  bA ~ dnorm(0, 0.5),\n  sigma ~ dexp(1)\n), data = dat)"
  },
  {
    "objectID": "notes/notes-07.html#robust-regressions",
    "href": "notes/notes-07.html#robust-regressions",
    "title": "Lecture 07 - Fitting Over & Under",
    "section": "Robust Regressions",
    "text": "Robust Regressions\n\nunobserved heterogeneity in sample -&gt; mixture of Gaussian errors\n\nthicker tails means model is less surprised/more robust\n\nstudent-t regression can be a good default for undertheorized domains\n\nbecause Gaussian distribution is so skeptical"
  },
  {
    "objectID": "notes/notes-07.html#prediction",
    "href": "notes/notes-07.html#prediction",
    "title": "Lecture 07 - Fitting Over & Under",
    "section": "Prediction",
    "text": "Prediction\n\npossible to make very good predictions without knowing causes\noptimizing prediction does not reliably reveal causes"
  },
  {
    "objectID": "notes/notes-09.html",
    "href": "notes/notes-09.html",
    "title": "Lecture 09 - Modeling Events",
    "section": "",
    "text": "Rose: starting to understand what a generalized linear model actually is\nThorn: algebra and links"
  },
  {
    "objectID": "notes/notes-09.html#rose-thorn",
    "href": "notes/notes-09.html#rose-thorn",
    "title": "Lecture 09 - Modeling Events",
    "section": "",
    "text": "Rose: starting to understand what a generalized linear model actually is\nThorn: algebra and links"
  },
  {
    "objectID": "notes/notes-09.html#modeling-events",
    "href": "notes/notes-09.html#modeling-events",
    "title": "Lecture 09 - Modeling Events",
    "section": "Modeling Events",
    "text": "Modeling Events\nNOTE: read causal foundations of bias, fairness, and …\n\nobservations are counts\nunknowns are possibilities, odds\neverything is interacting\n\nWhat is the effect of gender on university admissions?\n\n\n\n\n\n\ntotal effect of discrimination is what people experience\nquite often, the thing we are able to estimate is what we want (total effects are easier to estimate but subpaths are interesting and important to understanding equity)\n\n\nN &lt;- 1000 # number of applicants\nG &lt;- sample(1:2, size = N, replace = T) # gender distribution\nD &lt;- rbern(N, ifelse(G==1, 0.3, 0.8)) + 1 #gener 1 tends to apply to appartment 1, 2 to 2\naccept_rate &lt;- matrix(c(0.1, 0.3, 0.1, 0.3), nrow = 2) # matrix of acceptance rates [dept, gender]\nA &lt;- rbern(N, accept_rate[D,G]) # simulate acceptance\n\n\nwe observe: count of events\nwe estimate: probability (or log-odds) of events\n\nlike the globe tossing model, but need “proportion of water” stratified by other variables"
  },
  {
    "objectID": "notes/notes-09.html#generalized-linear-models",
    "href": "notes/notes-09.html#generalized-linear-models",
    "title": "Lecture 09 - Modeling Events",
    "section": "Generalized Linear Models",
    "text": "Generalized Linear Models\n\nlinear models: expected value is additive “linear” combination of parameters\ngeneralized linear models: expected value is some function of an additive combination of parameters\nf is the link function - links parameters of distribution to linear model\ndistributions: relative number of ways to observe data, given assumptions about rates, probabilities, slopes, etc.\ndistributions are matched to constraints on observed variables\nlink functions are matched to distributions\nexponential distribution - the time to an event that has a constant rate. Values you sample from exponential distribution are latencies/rates (continuous data above one)\n\nprocess that produces events and then you count those events = binomial distribution\ndon’t know maximum number of counts = poisson (special case of binomial)\nsum exponential processes = gamma\nlarge means of gamma distribution = normal distribution\n\ndistribution you want is governed by the constraints you assume about your variable\n\nyou cannot test if your data are normal\nneed to think about constraints and match to distributions"
  },
  {
    "objectID": "notes/notes-09.html#logit-link",
    "href": "notes/notes-09.html#logit-link",
    "title": "Lecture 09 - Modeling Events",
    "section": "Logit Link",
    "text": "Logit Link\n\nlog-odds = logit link\nBernoulli/binomial models most often use the logit link\nlinear model output is on the “log-odds scale”"
  },
  {
    "objectID": "notes/notes-09.html#logistic-priors",
    "href": "notes/notes-09.html#logistic-priors",
    "title": "Lecture 09 - Modeling Events",
    "section": "Logistic Priors",
    "text": "Logistic Priors\n\nlogit link compresses parameter distributions\non log-odds scale, above +4 = almost always, below -4 = almost never\n\ntherefore, tighter priors are better\n\n\n\n# generative model, basic mediator scenario\n\nN &lt;- 1000 # number of applicants\n# even gender distribution\nG &lt;- sample( 1:2 , size=N , replace=TRUE )\n# gender 1 tends to apply to department 1, 2 to 2\nD &lt;- rbern( N , ifelse( G==1 , 0.3 , 0.8 ) ) + 1\n# matrix of acceptance rates [dept,gender]\naccept_rate &lt;- matrix( c(0.1,0.3,0.1,0.3) , nrow=2 )\naccept_rate &lt;- matrix( c(0.05,0.2,0.1,0.3) , nrow=2 )\n# simulate acceptance\np &lt;- sapply( 1:N , function(i) accept_rate[D[i],G[i]] )\nA &lt;- rbern( N , p )\n\n# total effect gender\ndat_sim &lt;- list( A=A , D=D , G=G )\n\nm1 &lt;- ulam(\n    alist(\n        A ~ bernoulli(p),\n        logit(p) &lt;- a[G],\n        a[G] ~ normal(0,1)\n    ), data=dat_sim , chains=4 , cores=4 )\n\nprecis(m1,depth=2)\n\n# direct effects\nm2 &lt;- ulam(\n    alist(\n        A ~ bernoulli(p),\n        logit(p) &lt;- a[G,D],\n        matrix[G,D]:a ~ normal(0,1)\n    ), data=dat_sim , chains=4 , cores=4 )\n\nprecis(m2,depth=3)\n\n# aggregate the dat_sim for binomial instead of Bernoulli\n\nx &lt;- as.data.frame(cbind( A=dat_sim$A , G=dat_sim$G , D=dat_sim$D  ))\nhead(x,20)\n\ndat_sim2 &lt;- aggregate( A ~ G + D , dat_sim , sum )\ndat_sim2$N &lt;- aggregate( A ~ G + D , dat_sim , length )$A\n\nm2_bin &lt;- ulam(\n    alist(\n        A ~ binomial(N,p),\n        logit(p) &lt;- a[G,D],\n        matrix[G,D]:a ~ normal(0,1)\n    ), data=dat_sim2 , chains=4 , cores=4 )\n\nprecis(m2_bin,3)\n\nModel real data:\n\ndata(UCBadmit)\nd &lt;- UCBadmit\n\ndat &lt;- list( \n    A = d$admit,\n    N = d$applications,\n    G = ifelse(d$applicant.gender==\"female\",1,2),\n    D = as.integer(d$dept)\n)\n\n# total effect gender\nmG &lt;- ulam(\n    alist(\n        A ~ binomial(N,p),\n        logit(p) &lt;- a[G],\n        a[G] ~ normal(0,1)\n    ), data=dat , chains=4 , cores=4 )\n\nprecis(mG,2)\n\n# direct effects\nmGD &lt;- ulam(\n    alist(\n        A ~ binomial(N,p),\n        logit(p) &lt;- a[G,D],\n        matrix[G,D]:a ~ normal(0,1)\n    ), data=dat , chains=4 , cores=4 )\n\nprecis(mGD,3)\n\n# check chains\n\ntraceplot(mGD)\ntrankplot(mGD)\n\n# contrasts\n# on probability scale\n\npost1 &lt;- extract.samples(mG)\nPrA_G1 &lt;- inv_logit( post1$a[,1] )\nPrA_G2 &lt;- inv_logit( post1$a[,2] )\ndiff_prob &lt;- PrA_G1 - PrA_G2\ndens(diff_prob,lwd=4,col=2,xlab=\"Gender contrast (probability)\")\n\npost2 &lt;- extract.samples(mGD)\nPrA &lt;- inv_logit( post2$a ) \ndiff_prob_D_ &lt;- sapply( 1:6 , function(i) PrA[,1,i] - PrA[,2,i] )\nplot(NULL,xlim=c(-0.2,0.3),ylim=c(0,25),xlab=\"Gender contrast (probability)\",ylab=\"Density\")\nfor ( i in 1:6 ) dens( diff_prob_D_[,i] , lwd=4 , col=1+i , add=TRUE )\nabline(v=0,lty=3)\n\n# marginal effect of gender perception (direct effect)\n\n# compute department weights via simulation\n# we can just compute predictions as if all applications had been perceived as men\n# and then again as if all had been perceived as women\n# difference is marginal effect of perception, beause does not change department assignments (G -&gt; A only, no G -&gt; D)\n\n# OLD WRONG CODE!\n#p_G1 &lt;- link( mGD , data=list(N=dat$N,D=dat$D,G=rep(1,12)) )\n#p_G2 &lt;- link( mGD , data=list(N=dat$N,D=dat$D,G=rep(2,12)) )\n\n# NEW CORRECT CODE\n\n# number of applicatons to simulate\ntotal_apps &lt;- sum(dat$N)\n\n# number of applications per department\napps_per_dept &lt;- sapply( 1:6 , function(i) sum(dat$N[dat$D==i]) )\n\n# simulate as if all apps from women\np_G1 &lt;- link(mGD,data=list(\n    D=rep(1:6,times=apps_per_dept),\n    N=rep(1,total_apps),\n    G=rep(1,total_apps)))\n\n# simulate as if all apps from men\np_G2 &lt;- link(mGD,data=list(\n    D=rep(1:6,times=apps_per_dept),\n    N=rep(1,total_apps),\n    G=rep(2,total_apps)))\n\n# summarize\ndens( p_G1 - p_G2 , lwd=4 , col=2 , xlab=\"effect of gender perception\" )\nabline(v=0,lty=3)\n\n# show each dept density with weight as in population\nw &lt;- xtabs( dat$N ~ dat$D ) / sum(dat$N)\nw &lt;- w/max(w)\nplot(NULL,xlim=c(-0.2,0.3),ylim=c(0,25),xlab=\"Gender contrast (probability)\",ylab=\"Density\")\nfor ( i in 1:6 ) dens( diff_prob_D_[,i] , lwd=2+8*w[i]^3 , col=1+i , add=TRUE )\nabline(v=0,lty=3)"
  },
  {
    "objectID": "notes/notes-09.html#post-stratification",
    "href": "notes/notes-09.html#post-stratification",
    "title": "Lecture 09 - Modeling Events",
    "section": "Post-Stratification",
    "text": "Post-Stratification\n\ndescription, prediction, and causal inference often require post-stratification\nway to predict what the intervention will do to a specific population\npost-stratification = re-weighting estimates for target population"
  },
  {
    "objectID": "notes/notes-09.html#survival-analysis",
    "href": "notes/notes-09.html#survival-analysis",
    "title": "Lecture 09 - Modeling Events",
    "section": "Survival Analysis",
    "text": "Survival Analysis\n\nanother way of modelling events but we care about the time it took for an event to happen instead of number of times it happened\ncannot ignore censored cases (left-censored = don’t know when time started, right-censored = observation ended before event)\nexponential or gamma distribution"
  },
  {
    "objectID": "notes/notes-17.html",
    "href": "notes/notes-17.html",
    "title": "Lecture 17 - Measurement & Misclassification",
    "section": "",
    "text": "Rose: feels extremely relevant\nThorn: weird log functions"
  },
  {
    "objectID": "notes/notes-17.html#rose-thorn",
    "href": "notes/notes-17.html#rose-thorn",
    "title": "Lecture 17 - Measurement & Misclassification",
    "section": "",
    "text": "Rose: feels extremely relevant\nThorn: weird log functions"
  },
  {
    "objectID": "notes/notes-17.html#measurement-error",
    "href": "notes/notes-17.html#measurement-error",
    "title": "Lecture 17 - Measurement & Misclassification",
    "section": "Measurement Error",
    "text": "Measurement Error\n\nmany variables are proxies of the cause of interest\ndon’t consider how things are measured\nmeasurement error can have many effects on estimates"
  },
  {
    "objectID": "notes/notes-17.html#modeling-measurement",
    "href": "notes/notes-17.html#modeling-measurement",
    "title": "Lecture 17 - Measurement & Misclassification",
    "section": "Modeling Measurement",
    "text": "Modeling Measurement\n\ndivorce, marriage, and age statistics are measured with error and the amount of error varies by state\n\nimbalance in evidence quality\npotential confounding through measurement error\n\nstates with larger populations have less uncertainty/higher quality data\nconfounding because measurement is influenced by population size but then effects such as divorce rate can also be influenced by population size"
  },
  {
    "objectID": "notes/notes-17.html#misclassification",
    "href": "notes/notes-17.html#misclassification",
    "title": "Lecture 17 - Measurement & Misclassification",
    "section": "Misclassification",
    "text": "Misclassification\n\ncategorical version of measurement error\nrelated models: hurdle models and occupancy models"
  },
  {
    "objectID": "notes/notes-05.html",
    "href": "notes/notes-05.html",
    "title": "Lecture 05 - Elemental Confounds",
    "section": "",
    "text": "Rose: so helpful and relevant\nThorn: is all my science wrong"
  },
  {
    "objectID": "notes/notes-05.html#rose-thorn",
    "href": "notes/notes-05.html#rose-thorn",
    "title": "Lecture 05 - Elemental Confounds",
    "section": "",
    "text": "Rose: so helpful and relevant\nThorn: is all my science wrong"
  },
  {
    "objectID": "notes/notes-05.html#correlation",
    "href": "notes/notes-05.html#correlation",
    "title": "Lecture 05 - Elemental Confounds",
    "section": "Correlation",
    "text": "Correlation\n\ncorrelation is common in nature, causation is sparse\nscientific question/estimand -&gt; recipe/estimator -&gt; result/estimate"
  },
  {
    "objectID": "notes/notes-05.html#association-causation",
    "href": "notes/notes-05.html#association-causation",
    "title": "Lecture 05 - Elemental Confounds",
    "section": "Association & Causation",
    "text": "Association & Causation\n\nwe must defend against confounding in our stats\nconfounds mislead us - feature of the sample + how we use it\nfour elemental confounds\n\ncauses of confounds can be extremely diverse but they are all at their core made up of relationships between 3 variables"
  },
  {
    "objectID": "notes/notes-05.html#the-fork",
    "href": "notes/notes-05.html#the-fork",
    "title": "Lecture 05 - Elemental Confounds",
    "section": "The Fork",
    "text": "The Fork\n\nX and Y are associated \\(Y \\not\\!\\perp\\!\\!\\!\\perp X\\)\nShare a common cause Z\nOnce stratified by Z, no association \\(Y \\perp\\!\\!\\!\\perp X | Z\\)\n\n\ndag &lt;- dagify(\n    X ~ Z,\n    Y ~ Z\n)\n\nggdag(dag) +\n    theme_dag()\n\n\n\n\n\nsimulation:\n\n\nn &lt;- 1000\nZ &lt;- rbern(n, 0.5)\nX &lt;- rbern(n, (1-Z)*0.01 + Z*0.9)\nY &lt;- rbern(n, (1-Z)*0.01 + Z*0.9)\n\n\ncols &lt;- c(4,2)\nn &lt;- 300\nZ &lt;- rbern(n, 0.5)\nX &lt;- rnorm(n, (1-Z)*0.01 + Z*0.9)\nY &lt;- rnorm(n, (1-Z)*0.01 + Z*0.9)\n\nplot(X, Y, col=cols[Z+1])\n\n\n\n# abline(lm(Y[Z==1]))\n#\n#\n\n\nexample:\n\nwhy do regions of the USA with higher rates of marriage also have higher rates of divorce?\nestimand: causal effect of marriage rate on divorce rate\n\n\n\ndag &lt;- dagify(\n    D ~ A,\n    M ~ A,\n    D ~ M\n)\n\nggdag(dag) +\n    theme_dag()\n\n\n\n\n\ncauses are not in the data\nis effect of marriage rates on divorce rates just a symptom of common cause age?\nneed to break the fork to test direct effect – stratify by A\ncontinuous variable stratification means that we are adding to that variable essentially to the intercept\nstandardizing variables is almost always helpful in linear regression\n\nmake the mean zero and divide by sd\n\nto stratify by A, include as a term in the linear model:\n\n\\(D _i \\sim Normal(\\mu _i, \\sigma)\\)\n\\(\\mu _i = \\alpha + \\beta_MM_i + \\beta_AA_i\\)\n\\(\\alpha \\sim Normal(0, 0.2)\\)\n\\(\\beta_M \\sim Normal(0,0.5)\\)\n\\(\\beta_A \\sim Normal(0, 0.5)\\)\n\\(\\sigma \\sim Exponential(1)\\)\n\n\n\nlibrary(rethinking)\ndata(WaffleDivorce)\nd &lt;- WaffleDivorce\n\n# model\ndat &lt;- list(\n    D = standardize(d$Divorce),\n    M = standardize(d$Marriage),\n    A = standardize(d$MedianAgeMarriage)\n)\n\nm_DMA &lt;- quap(\n    alist(\n        D ~ dnorm(mu,sigma),\n        mu &lt;- a + bM*M + bA*A,\n        a ~ dnorm(0,0.2),\n        bM ~ dnorm(0,0.5),\n        bA ~ dnorm(0,0.5),\n        sigma ~ dexp(1)\n    ) , data=dat )\n\nplot(precis(m_DMA))\n\n\n\n\n\na causal effect is a manipulation of the generative model, an intervention\ndistribution of D when we intervene (“do”) M \\(p(D|do(M))\\)\n\nimplies deleting all arrows into M and simulating D\n\n\n\npost &lt;- extract.samples(m_DMA)\n\n# sample A from data\nn &lt;- 1e3\nAs &lt;- sample(dat$A, size = n, replace = T)\n\n# simulate D for M=0 (sample mean)\nDM0 &lt;- with(post, rnorm(n, a + bM*0 + bA*As, sigma))\n\n# simulate D for M = 1 (+1 standard deviation)\n# use the same A values \nDM1 &lt;- with(post, rnorm(n, a + bM*1 + bA*As, sigma))\n\n# contrast\nM10_contrast &lt;- DM1 - DM0\ndens(M10_contrast, lwd=4, col=2, xlab=\"effect of increase in M\")"
  },
  {
    "objectID": "notes/notes-05.html#the-pipe",
    "href": "notes/notes-05.html#the-pipe",
    "title": "Lecture 05 - Elemental Confounds",
    "section": "The Pipe",
    "text": "The Pipe\n\nX and Y are associated \\(Y \\not\\!\\perp\\!\\!\\!\\perp X\\)\nInfluence of X on Y trasmitted through Z\nOnce stratified by Z, no association \\(Y \\perp\\!\\!\\!\\perp X | Z\\)\n\n\ndag &lt;- dagify(\n    Y ~ Z,\n    Z ~ X\n)\n\nggdag(dag) +\n    theme_dag()\n\n\n\n\n\nn &lt;- 1000\nX &lt;- rbern(n, 0.5)\nZ &lt;- rbern(n, (1-X)*0.01 + X*0.9)\nY &lt;- rbern(n, (1-Z)*0.01 + Z*0.9)\n\n\neverything that Y knows about X, is already known by Z\n\nonce you learn Z, there is nothing more to learn about the association\n\n\n\ncols &lt;- c(4,2)\nn &lt;- 300\nX &lt;- rbern(n)\nZ &lt;- rbern(n, inv_logit(X))\nY &lt;- rbern(n, (2*Z-1))\n\nWarning in rbinom(n, size = 1, prob = prob): NAs produced\n\n# plot\n\n\nincluding the mediator at the wrong time can lead to incorrect inference\n\n\ndag &lt;- dagify(\n   H1 ~ H0 + Treat,\n   H1 ~ Fungus,\n   Fungus ~ Treat\n   \n)\n\nggdag(dag) +\n    theme_dag()\n\n\n\n\n\nwhat is the total causal effect of treatment?\nTreat -&gt; Fungus -&gt; H1 is a pipe, should not stratify by F\npoost-treatment bias\n\nif you stratify by a consquence of the treatment, it can induce post-treatment bias - gives you a misleading estimate of what you’re after\nconsequences of treatment should not usually be included in the estimator"
  },
  {
    "objectID": "notes/notes-05.html#the-collider",
    "href": "notes/notes-05.html#the-collider",
    "title": "Lecture 05 - Elemental Confounds",
    "section": "The Collider",
    "text": "The Collider\n\nX and Y are not associated (share no causes) \\(Y \\perp\\!\\!\\!\\perp X\\)\nX and Y both influence Z\nOnce stratified by Z, X and Y are associated \\(Y \\not\\!\\perp\\!\\!\\!\\perp X | Z\\)\n\n\ndag &lt;- dagify(\n    Z ~ X + Y\n)\n\nggdag(dag) +\n    theme_dag()\n\n\n\n\n\nn &lt;- 1000\nX &lt;- rbern(n, 0.5)\nY &lt;- rbern(n, 0.5)\nZ &lt;- rbern(n, ifelse(X+Y&gt;0, 0.9, 0.2))\n\n\nstrong correlations caused by the collider could be read as correlation but causes are not in the data\n\n\ncols &lt;- c(4,2)\nN &lt;- 300\nX &lt;- rnorm(N)\nY &lt;- rnorm(N)\nZ &lt;- rbern(N, inv_logit(2*X+2*Y-2))\n\nplot(X,Y, cols = cols[Z+1])\n\nWarning in plot.window(...): \"cols\" is not a graphical parameter\n\n\nWarning in plot.xy(xy, type, ...): \"cols\" is not a graphical parameter\n\n\nWarning in axis(side = side, at = at, labels = labels, ...): \"cols\" is not a\ngraphical parameter\n\nWarning in axis(side = side, at = at, labels = labels, ...): \"cols\" is not a\ngraphical parameter\n\n\nWarning in box(...): \"cols\" is not a graphical parameter\n\n\nWarning in title(...): \"cols\" is not a graphical parameter\n\n\n\n\n# lines\n\n\nsometimes samples come already stratified by collider\nassociations among the things you have measured post-selection is dangerous because the selection is often collider bias\nendogenous colliders\n\nif you include a collider in your estimator you can induce a spurious correlation\nhappens within your analysis\n\nexample: age and happiness\n\nestimand: influence of age on happiness\npossible confound: marital status\nsuppose age has zero influence on happiness, but that both age and happiness influence marital status\n\n\n\ndag &lt;- dagify(\n   M ~ A + H\n)\n\nggdag(dag) +\n    theme_dag()\n\n\n\n\n\nstratified by marital status, negative association between age and happiness even though they are not related -&gt; age/happiness are unrelated"
  },
  {
    "objectID": "notes/notes-05.html#the-descendant",
    "href": "notes/notes-05.html#the-descendant",
    "title": "Lecture 05 - Elemental Confounds",
    "section": "The Descendant",
    "text": "The Descendant\n\nhow it behaves depends on what it is attached to\nA is the descendant, contains information of its “parent”\nX and Y are causally associated through Z \\(Y \\not\\!\\perp\\!\\!\\!\\perp X\\)\nA holds information about Z\nOnce stratified by A, X and Y less associated (if strong enough) \\(Y \\perp\\!\\!\\!\\perp X | A\\)\n\n\ndag &lt;- dagify(\n   Z ~ X,\n   Y ~ Z,\n   A ~ Z\n)\n\nggdag(dag) +\n    theme_dag()\n\n\n\n\n\nn &lt;- 1000\nx &lt;- rbern(n, 0.5)\nz &lt;- rbern(n, (1-x)*0.1 + x*0.9)\ny &lt;- rbern(n, (1-z)*0.1 + z*0.9)\na &lt;- rbern(n, (1-z)*0.1 + z*0.9)\n\n\ndescendants are everywhere - proxies"
  },
  {
    "objectID": "notes/notes-12.html",
    "href": "notes/notes-12.html",
    "title": "Lecture 12 - Multilevel Models",
    "section": "",
    "text": "Rose: helpful but confusing (because so far from what we are taught) way to discuss random effects\nThorn: how tf does this translate to “normal” R packages"
  },
  {
    "objectID": "notes/notes-12.html#rose-thorn",
    "href": "notes/notes-12.html#rose-thorn",
    "title": "Lecture 12 - Multilevel Models",
    "section": "",
    "text": "Rose: helpful but confusing (because so far from what we are taught) way to discuss random effects\nThorn: how tf does this translate to “normal” R packages"
  },
  {
    "objectID": "notes/notes-12.html#repeat-observations",
    "href": "notes/notes-12.html#repeat-observations",
    "title": "Lecture 12 - Multilevel Models",
    "section": "Repeat Observations",
    "text": "Repeat Observations\n\nfor any given estimand, there are multiple estimators we can use (but some are better than others)\ncan include categorical responses such as individuals as seen before\n\nbut the model is not learning"
  },
  {
    "objectID": "notes/notes-12.html#multilevel-models",
    "href": "notes/notes-12.html#multilevel-models",
    "title": "Lecture 12 - Multilevel Models",
    "section": "Multilevel Models",
    "text": "Multilevel Models\n\nmodels within models\n\n\nmodel observed groups/individuals\nmodel of population of groups/individuals\n\n\nthe population model creates a kind of memory\nmultilevel models with memory learn faster, better AND models with memory resist overfitting\nmultilevel models use every observation to inform predictions about other cafes and the population of cafes"
  },
  {
    "objectID": "notes/notes-12.html#regularization",
    "href": "notes/notes-12.html#regularization",
    "title": "Lecture 12 - Multilevel Models",
    "section": "Regularization",
    "text": "Regularization\n\nmultilevel models adaptively regularize\ncomplete pooling: treat all clusters as identical -&gt; underfitting\nno pooling: treat all clusters as unrelated -&gt; overfitting\npartial pooling: adaptive compromise that achieves regularization"
  },
  {
    "objectID": "notes/notes-12.html#reedfrogs",
    "href": "notes/notes-12.html#reedfrogs",
    "title": "Lecture 12 - Multilevel Models",
    "section": "Reedfrogs",
    "text": "Reedfrogs\n\ntreatments: density, size, predation\noutcome: survival\n48 groups (“tanks”) of tadpoles\n\n\n\n\n\n\n\n\\(S_i \\sim Binomial(D_i, p_i)\\)\n\\(logit(p_i) = \\alpha_{T[i]}\\)\n\\(\\alpha_j \\sim Normal(\\overline{\\alpha}, \\sigma)\\)\n\\(\\overline{\\alpha} \\sim Normal(0, 1.5)\\)\nparameters are just unobserved variables\n\ndata is an observed parameter\n\nif we want to learn about differences in between groups, we can set the prior for the mean tank as we normally would and then leave the prior for the variation of the groups as an unobserved parameter to be learned\nfind optimal value of sigma through cross-validation\n\nalthough we are using the model to choose a prior, we are not basing it off of model fit of the sample, we are evaluating it on cross-validation (out-of-sample). So we we are just assessing whether the model is overfit - not seeing how well it fits the data/causal relationships"
  },
  {
    "objectID": "notes/notes-12.html#automatic-regularization",
    "href": "notes/notes-12.html#automatic-regularization",
    "title": "Lecture 12 - Multilevel Models",
    "section": "Automatic Regularization",
    "text": "Automatic Regularization\n\ncan use automatic regularization instead of cross-validation to remove the need to run so many models\n\n\\(S_i \\sim Binomial(D_i, p_i)\\)\n\\(logit(p_i) = \\alpha_{T[i]}\\)\n\\(\\alpha_j \\sim Normal(\\overline{\\alpha}, \\sigma)\\)\n\\(\\overline{\\alpha} \\sim Normal(0, 1.5)\\)\n\\(\\sigma \\sim Exponential(1)\\)\n\n\n\nlibrary(rethinking) \ndata(reedfrogs)\nd &lt;- reedfrogs \nd$tank &lt;- 1:nrow(d)\ndat &lt;- list(\n    S = d$surv,\n    D = d$density,\n    T = d$tank )\n\nmST &lt;- ulam( \n    alist(\n        S ~ dbinom( D , p ) ,\n        logit(p) &lt;- a[T] ,\n        a[T] ~ dnorm( a_bar , sigma ) , \n        a_bar ~ dnorm( 0 , 1.5 ) ,\n        sigma ~ dexp( 1 )\n    ), data=dat , chains=4 , log_lik=TRUE )\n\nmSTnomem &lt;- ulam( \n    alist(\n        S ~ dbinom( D , p ) ,\n        logit(p) &lt;- a[T] ,\n        a[T] ~ dnorm( a_bar , 1 ) , \n        a_bar ~ dnorm( 0 , 1.5 )\n    ), data=dat , chains=4 , log_lik=TRUE )\n\ncompare( mST , mSTnomem , func=WAIC )\n\n\nmultilevel models regularize “for free” - model mST is a multilevel model and having sigma as a prior means that it is regularized around the relevant values\nmSTnomem is not a multilevel model (sigma is not a prior - it is a fixed value)\nwhen you are working with multilevel models, when you add treatment variables, the variation among means is going to shrink because you are accounting for the variation with the different treatments\nstratify mean by predators:\n\n\\(S_i \\sim Binomial(D_i, p_i)\\)\n\\(logit(p_i) = \\alpha_{T[i]} + \\beta_PP_i\\)\n\\(\\beta_P \\sim Normal(0, 0.5)\\)\n\\(\\alpha_j \\sim Normal(\\overline{\\alpha}, \\sigma)\\)\n\\(\\overline{\\alpha_j} \\sim Normal(0, 1.5)\\)\n\\(\\sigma \\sim Exponential(1)\\)\n\n\n\n# pred model\n\ndat$P &lt;- ifelse(d$pred==\"pred\",1,0)\nmSTP &lt;- ulam( \n    alist(\n        S ~ dbinom( D , p ) ,\n        logit(p) &lt;- a[T] + bP*P ,\n        bP ~ dnorm( 0 , 0.5 ),\n        a[T] ~ dnorm( a_bar , sigma ) , \n        a_bar ~ dnorm( 0 , 1.5 ) ,\n        sigma ~ dexp( 1 )\n    ), data=dat , chains=4 , log_lik=TRUE )\n\npost &lt;- extract.samples(mSTP)\ndens( post$bP , lwd=4 , col=2 , xlab=\"bP (effect of predators)\" )\n\n\nextremely similar predictions between model with predators and model without\n\nbecause alphas can learn the behaviours of each tank without an explanation (ie predators)\n\nBUT the variation between tanks is very different between models\n\npredator model has much lower variation in sigma"
  },
  {
    "objectID": "notes/notes-12.html#multilevel-tadpoles",
    "href": "notes/notes-12.html#multilevel-tadpoles",
    "title": "Lecture 12 - Multilevel Models",
    "section": "Multilevel Tadpoles",
    "text": "Multilevel Tadpoles\n\nmodel of unobserved population helps learn about observed units\nuse data efficiently, reduce overfitting\nvarying effects: unit-specific partially pooled estimates (also called random effects depending on discipline)"
  },
  {
    "objectID": "notes/notes-12.html#varying-effects-superstitions",
    "href": "notes/notes-12.html#varying-effects-superstitions",
    "title": "Lecture 12 - Multilevel Models",
    "section": "Varying Effects Superstitions",
    "text": "Varying Effects Superstitions\n\nunits must be sampled at random (false - unrelated) - justification for partial pooling is that you learn faster\nnumber of units must be large (false - unrelated)\nassumes Gaussian variation (false) - misunderstanding of probability theory. Distributions in statistical models are not claims of frequency distributions of the variables in the real world, they are just priors. Posterior distribution does not have to be Gaussian. A Gaussian prior does not impose a Gaussian posterior distribution.\n\nprior is just a prior\nbut you can use non-random distributions in multilevel models"
  },
  {
    "objectID": "notes/notes-12.html#practical-difficulties",
    "href": "notes/notes-12.html#practical-difficulties",
    "title": "Lecture 12 - Multilevel Models",
    "section": "Practical Difficulties",
    "text": "Practical Difficulties\n\nhow to use more than one cluster at the same time?\nhow to sample efficiently?\nwhat about slopes? confounds?"
  },
  {
    "objectID": "notes/notes-12.html#bonus-random-confounds",
    "href": "notes/notes-12.html#bonus-random-confounds",
    "title": "Lecture 12 - Multilevel Models",
    "section": "Bonus: Random Confounds",
    "text": "Bonus: Random Confounds\n\nwhen unobserved group features influence individually-varying causes\n\ngroup level variables have direct and indirect influences\n\nvery confusing literature\n\n\n\n\n\n\n\nG (tank traits) have direct influences on Si (survival)\nZ (group trait) have direct influences on Si (survival)\nXi (individual trait) have direct influences on Si\nPROBLEM: G (tank traits) also indirectly influence Si through Xi\nmultilevel models that account for these confounds = Mundlak Machines\n\n\nset.seed(8672)\n\nN_groups &lt;- 30\nN_id &lt;- 200\na0 &lt;- (-2)\nbZY &lt;- (-0.5)\ng &lt;- sample(1:N_groups,size=N_id,replace=TRUE) # sample into groups\nUg &lt;- rnorm(N_groups,1.5) # group confounds\nX &lt;- rnorm(N_id, Ug[g] ) # individual varying trait\nZ &lt;- rnorm(N_groups) # group varying trait (observed)\nY &lt;- rbern(N_id, p=inv_logit( a0 + X + Ug[g] + bZY*Z[g] ) )\n\ntable(g)\n\n\ncan use a fixed effects model (estimate a different average rate for each group, without pooling) which soaks up the confounding - but its inefficient and it cannot identify any group-level effects\n\n\n# fixed effects\n# X deconfounded, but Z unidentified now!\nprecis(glm(Y~X+Z[g]+as.factor(g),family=binomial),pars=c(\"X\",\"Z\"),2)\n\ndat &lt;- list(Y=Y,X=X,g=g,Ng=N_groups,Z=Z)\n\n# fixed effects\nmf &lt;- ulam(\n    alist(\n        Y ~ bernoulli(p),\n        logit(p) &lt;- a[g] + bxy*X + bzy*Z[g],\n        a[g] ~ dnorm(0,10),\n        c(bxy,bzy) ~ dnorm(0,1)\n    ) , data=dat , chains=4 , cores=4 )\n\n# random effects\nmr &lt;- ulam(\n    alist(\n        Y ~ bernoulli(p),\n        logit(p) &lt;- a[g] + bxy*X + bzy*Z[g],\n        transpars&gt; vector[Ng]:a &lt;&lt;- abar + z*tau,\n        z[g] ~ dnorm(0,1),\n        c(bxy,bzy) ~ dnorm(0,1),\n        abar ~ dnorm(0,1),\n        tau ~ dexp(1)\n    ) , data=dat , chains=4 , cores=4 , sample=TRUE )\n\n\nshould expect your model to have posterior high density regions over the true value\nfixed effects model cannot estimate the group level effects\nmultilevel model pulls intercepts towards each other and thus compromises on identifying the confound so that it can get better estimates for each group\n\nbetter estimates for G, worse estimate for X but you can include Z\n\nMundlak Machine\n\ncalculate group average X which is a descendant of the confound (G) SO if we condition on \\(\\overline{X}_G\\) and treat it like a group level variable, it will partly deconfound our model\nestimate a different average rate for each group via partial pooling via including group average X\nbetter X but improper respect for uncertainty in X-bar (ignoring quality of Xbar across groups)\n\n\n\n# The Mundlak Machine\nxbar &lt;- sapply( 1:N_groups , function(j) mean(X[g==j]) )\ndat$Xbar &lt;- xbar\nmrx &lt;- ulam(\n    alist(\n        Y ~ bernoulli(p),\n        logit(p) &lt;- a[g] + bxy*X + bzy*Z[g] + buy*Xbar[g],\n        transpars&gt; vector[Ng]:a &lt;&lt;- abar + z*tau,\n        z[g] ~ dnorm(0,1),\n        c(bxy,buy,bzy) ~ dnorm(0,1),\n        abar ~ dnorm(0,1),\n        tau ~ dexp(1)\n    ) , data=dat , chains=4 , cores=4 , sample=TRUE )\n\n\ncan fix the problem of not respecting the uncertainty in X-bar\n\ntreat G as unknown and use Xi to estimate\nrespects uncertainty in G\nrun two simultaneous regressions\n\n\n\n# The Latent Mundlak Machine\nmru &lt;- ulam(\n    alist(\n        # Y model\n        Y ~ bernoulli(p),\n        logit(p) &lt;- a[g] + bxy*X + bzy*Z[g] + buy*u[g],\n        transpars&gt; vector[Ng]:a &lt;&lt;- abar + z*tau,\n        # X model\n        X ~ normal(mu,sigma),\n        mu &lt;- aX + bux*u[g],\n        vector[Ng]:u ~ normal(0,1),\n        # priors\n        z[g] ~ dnorm(0,1),\n        c(aX,bxy,buy,bzy) ~ dnorm(0,1),\n        bux ~ dexp(1),\n        abar ~ dnorm(0,1),\n        tau ~ dexp(1),\n        sigma ~ dexp(1)\n    ) , data=dat , chains=4 , cores=4 , sample=TRUE )\n\n\ncan use fixed effects if you are not interested in group-level predictors or prediction\ncan include average X but it is better to use the latent model\nconfounds vary a lot - there is no one answer"
  },
  {
    "objectID": "notes/notes-14.html",
    "href": "notes/notes-14.html",
    "title": "Lecture 14 - Correlated Features",
    "section": "",
    "text": "Rose:\nThorn:"
  },
  {
    "objectID": "notes/notes-14.html#rose-thorn",
    "href": "notes/notes-14.html#rose-thorn",
    "title": "Lecture 14 - Correlated Features",
    "section": "",
    "text": "Rose:\nThorn:"
  },
  {
    "objectID": "notes/notes-14.html#add-correlated-features",
    "href": "notes/notes-14.html#add-correlated-features",
    "title": "Lecture 14 - Correlated Features",
    "section": "Add Correlated Features",
    "text": "Add Correlated Features\n\nwhen you build models that can identify correlation between features you can use partial pooling between those correlated features\none prior distribution for each cluster\n\none feature: one dimensional distribution (varying intercepts)\ntwo features: two-D distribution (often use multivariate normal distribution - have mean and variation but also correlation between the parameters)\nN features: N-dimensional distribution\n\nhard part: learning associations\n\n\n###########\n# non-centered varying slopes with and without covariance\n\ndat &lt;- list(\n    C = d$use.contraception,\n    D = as.integer(d$district),\n    U = d$urban,\n    A = standardize(d$age.centered),\n    K = d$living.children )\n\n# no covariance\nmCDUnc &lt;- ulam(\n    alist(\n        C ~ bernoulli(p),\n        logit(p) &lt;- a[D] + b[D]*U,\n        # define effects using other parameters\n        save&gt; vector[61]:a &lt;&lt;- abar + za*sigma,\n        save&gt; vector[61]:b &lt;&lt;- bbar + zb*tau,\n        # z-scored effects\n        vector[61]:za ~ normal(0,1),\n        vector[61]:zb ~ normal(0,1),\n        # ye olde hyper-priors\n        c(abar,bbar) ~ normal(0,1),\n        c(sigma,tau) ~ exponential(1)\n    ) , data=dat , chains=4 , cores=4 )\n\n\nit is hard to learn the correlation from any finite sample\nLKJ correlation matrix priors - prior for correlation matrices\n\ntends to have shapes\ncan be skeptical of extreme correlations\n\n\n\n# covariance - centered\nmCDUcov &lt;- ulam(\n    alist(\n        C ~ bernoulli(p),\n        logit(p) &lt;- a[D] + b[D]*U,\n        # define effects using other parameters\n        transpars&gt; vector[61]:a &lt;&lt;- v[,1],\n        transpars&gt; vector[61]:b &lt;&lt;- v[,2],\n        # priors - centered correlated varying effects\n        matrix[61,2]:v ~ multi_normal(abar,Rho,sigma),\n        vector[2]:abar ~ normal(0,1),\n        corr_matrix[2]:Rho ~ lkj_corr(4),\n        vector[2]:sigma ~ exponential(1)\n    ) , data=dat , chains=4 , cores=4 )\n\n\ncentering vs non-centering to increase efficiency of models\n\ncentered = priors of priors (parameters inside the priors)\nnon-centered = changing model to not have hyper-priors (but be mathematically equivalent) to have increased efficiencies\n\n\n\n# covariance - non-centered\nmCDUcov_nc &lt;- ulam(\n    alist(\n        C ~ bernoulli(p),\n        logit(p) &lt;- a[D] + b[D]*U,\n        # define effects using other parameters\n        # this is the non-centered Cholesky machine\n        transpars&gt; vector[61]:a &lt;&lt;- abar[1] + v[,1],\n        transpars&gt; vector[61]:b &lt;&lt;- abar[2] + v[,2],\n        transpars&gt; matrix[61,2]:v &lt;-\n            compose_noncentered( sigma , L_Rho , Z ),\n        # priors - note that none have parameters inside them\n        # that is what makes them non-centered\n        matrix[2,61]:Z ~ normal( 0 , 1 ),\n        vector[2]:abar ~ normal(0,1),\n        cholesky_factor_corr[2]:L_Rho ~ lkj_corr_cholesky( 4 ),\n        vector[2]:sigma ~ exponential(1),\n        # convert Cholesky to Corr matrix\n        gq&gt; matrix[2,2]:Rho &lt;&lt;- Chol_to_Corr(L_Rho)\n    ) , data=dat , chains=4 , cores=4 )\n\n\nnice to compare prior to posterior distribution to make sure the model learned something\ncorrelated varying effects models are easier to fit in Bayesian framework\npriors learn correlation structure\nvarying effects can be correlated even if the prior doesn’t learn the correlations"
  },
  {
    "objectID": "notes/notes-14.html#inconvenient-posteriors",
    "href": "notes/notes-14.html#inconvenient-posteriors",
    "title": "Lecture 14 - Correlated Features",
    "section": "Inconvenient Posteriors",
    "text": "Inconvenient Posteriors\n\ntransforming priors can help with divergent transitions because it changes the shape of the model\nbrms uses non-centered priors as default in multilevel models\n\nnot always better\n\ncentered and non-centered priors are better in different contexts\n\ncentered: lots of data in each cluster (data probability dominant)\nnon-centered: many clusters, sparse evidence (prior dominant)"
  },
  {
    "objectID": "notes/notes-11.html",
    "href": "notes/notes-11.html",
    "title": "Lecture 11 - Ordered Categories",
    "section": "",
    "text": "Rose: extremely relevant\nThorn: ahhhh how to actually do it"
  },
  {
    "objectID": "notes/notes-11.html#rose-thorn",
    "href": "notes/notes-11.html#rose-thorn",
    "title": "Lecture 11 - Ordered Categories",
    "section": "",
    "text": "Rose: extremely relevant\nThorn: ahhhh how to actually do it"
  },
  {
    "objectID": "notes/notes-11.html#trolley-problems",
    "href": "notes/notes-11.html#trolley-problems",
    "title": "Lecture 11 - Ordered Categories",
    "section": "Trolley Problems",
    "text": "Trolley Problems\n\nthere is a runaway trolley, you are next to a switch\nif you do not pull the switch, it will kill 5 people. If you pull the switch, one person dies\nwhat is the ethics of pulling the switch\ncan assess people’s reactions to trolley problems to assess ethics (can’t actually do trolley problems)\nthree variables that people try to analyze: action, intention, contact\n\ntaking an action is less morally permissible than not\nintention seems more monstrous\nintended access are even worse if they involve contact\n\ntrolley data: answering 30 trolley problems asking how appropriate is the action from 1-7?\noutcome data is ordered categorical data\nestimand: how do action, intention, and contact influence response to a trolley story?\n\nhow are influences of A/I/C associated with other variables?"
  },
  {
    "objectID": "notes/notes-11.html#ordered-categories",
    "href": "notes/notes-11.html#ordered-categories",
    "title": "Lecture 11 - Ordered Categories",
    "section": "Ordered Categories",
    "text": "Ordered Categories\n\ncategories of discrete types with ordered relationships\ndistances between the categories doesn’t have to be the same (e.g., going from 4-5 is probably easier than going from 6-7 because reaching the max means more than shifting in the middle )\nanchor points are common (defaults when we are not sure/feeling meh)\n\nnot everyone shares the same anchor points\n\nneed to think of outcomes as cumulative distribution -&gt; i.e., 5 is everything 5 and under added together\n\nbuild log-odds parameters that correspond to this (log(probability of thing / probability of thing not happening))\nlogit link models\ncumulative proportion -&gt; cumulative log-odds to model these data\nparameters are on cumulative log-odds scale = cutpoints\nnumber of cutpoints you need is n-1 of outcomes (because last one is Infinity because we added the cumulative proportion of everything together to get to 1)\nto predict the data, we have to recalculate cumulative proportion\n\nHow to make it a function of variables (GLM)?\n\nstratify cutpoints\noffset each cutpoint by value of linear model\nRi ~ OrderedLogit(\\(\\phi_i , \\alpha\\))\n\\(\\phi_i\\) = \\(\\beta x_i\\)\n\\(\\alpha\\) = cutpoint\nthere is no intercept in phi because the intercept is already accounted for with cutpoints\nBigger phis give you smaller average responses and smaller phis give you larger average responses (if phi is subtracted - double check software)\n\nExample:\n\n\\(R_i \\sim OrderedLogit(\\phi_i , \\alpha)\\)\n\\(\\phi_i = \\beta_A A_i + \\beta_C C_i + \\beta_I I_i\\)\n\\(\\beta \\sim Normal(0, 0.5)\\)\n\\(\\alpha_j \\sim Normal(0, 1)\\)\n\n\n\ndata(Trolley)\nd &lt;- Trolley\ndat &lt;- list(\n    R = d$response,\n    A = d$action,\n    I = d$intention,\n    C = d$contact\n)\n\nmRX &lt;- ulam(\n    alist(\n        R ~ dordlogit(phi,alpha),\n        phi &lt;- bA*A + bI*I + bC*C,\n        c(bA,bI,bC) ~ normal(0,0.5),\n        alpha ~ normal(0,1)\n    ) , data=dat , chains=4 , cores=4 )\n\nprecis(mRX,2)\n\n\nafter modelling, simulate different outcomes\n\n\n# plot predictive distributions for each treatment\n\nvals &lt;- c(0,1,1) # A,I,C\nRsim &lt;- mcreplicate( 100 , sim(mRX,data=list(A=vals[1],I=vals[2],C=vals[3])) , mc.cores=6 )\nsimplehist(as.vector(Rsim),lwd=8,col=2,xlab=\"Response\")\nmtext(concat(\"A=\",vals[1],\", I=\",vals[2],\", C=\",vals[3]))"
  },
  {
    "objectID": "notes/notes-11.html#competing-causes",
    "href": "notes/notes-11.html#competing-causes",
    "title": "Lecture 11 - Ordered Categories",
    "section": "Competing Causes",
    "text": "Competing Causes\n\nwe can stratify by competing causes - just stratify the model by the variable of interest\n\n\n# total effect of gender\ndat$G &lt;- ifelse(d$male==1,2,1)\nmRXG &lt;- ulam(\n    alist(\n        R ~ dordlogit(phi,alpha),\n        phi &lt;- bA[G]*A + bI[G]*I + bC[G]*C,\n        bA[G] ~ normal(0,0.5),\n        bI[G] ~ normal(0,0.5),\n        bC[G] ~ normal(0,0.5),\n        alpha ~ normal(0,1)\n    ) , data=dat , chains=4 , cores=4 )\n\nprecis(mRXG,2)\n\nvals &lt;- c(0,1,1,2) # A,I,C,G\nRsim &lt;- mcreplicate( 100 , sim(mRXG,data=list(A=vals[1],I=vals[2],C=vals[3],G=vals[4])) , mc.cores=6 )\nsimplehist(as.vector(Rsim),lwd=8,col=2,xlab=\"Response\")\nmtext(concat(\"A=\",vals[1],\", I=\",vals[2],\", C=\",vals[3],\", G=\",vals[4]))\n\n\nis this the causal effect of gender?\n\nconfounded because this is a voluntary sample\neverything is causally associated with participation\nparticipation is implicitly conditioned on - it is a collider\nbecause all our covarying effects of interest are already stratified by the collider participation, it is impossible to get the causal effect of gender BUT we can get direct effect of gender (if we stratify appropriately)\n\n\n\n\n\n\nhow do we put these metric predictors into the model?"
  },
  {
    "objectID": "notes/notes-11.html#ordered-monotonic-predictors",
    "href": "notes/notes-11.html#ordered-monotonic-predictors",
    "title": "Lecture 11 - Ordered Categories",
    "section": "Ordered Monotonic Predictors",
    "text": "Ordered Monotonic Predictors\n\neducation is an ordered category that is a predictor\n\nunlikely that each level has the same effect\nwant a parameter for each level\ntake top level as “maximum effect” and each level gets their own beta and multiply each education level by maximum effect\nindividual delta parameters form a simplex (vector that sums to 1)\nprobability distribution that sums to 1 = Dirichlet\n\nDirichlet\n\ndistribution for distributions\nvector that sums to 1\nneed the same number of input numbers as levels\nbigger the numbers get, the less variation there is in the distributions\nhaving the same number doesn’t mean they are all the same, it means there is no prior expectation of which ones are bigger than the others\n\n\n\n# distributions of education and age\n\nedu_levels &lt;- c( 6 , 1 , 8 , 4 , 7 , 2 , 5 , 3 )\nedu_new &lt;- edu_levels[ d$edu ]\n\ndat$E &lt;- edu_new\ndat$a &lt;- rep(2,7) # dirichlet prior\n\nmRXE &lt;- ulam(\n    alist(\n        R ~ ordered_logistic( phi , alpha ),\n        phi &lt;- bE*sum( delta_j[1:E] ) + bA*A + bI*I + bC*C,\n        alpha ~ normal( 0 , 1 ),\n        c(bA,bI,bC,bE) ~ normal( 0 , 0.5 ),\n        vector[8]: delta_j &lt;&lt;- append_row( 0 , delta ),\n        simplex[7]: delta ~ dirichlet( a )\n    ), data=dat , chains=4 , cores=4 )\n\nprecis(mRXE,2)\n\n# version with transpars\nmRXE2 &lt;- ulam(\n    alist(\n        R ~ ordered_logistic( phi , alpha ),\n        phi &lt;- bE*sum( delta_j[1:E] ) + bA*A + bI*I + bC*C,\n        alpha ~ normal( 0 , 1 ),\n        c(bA,bI,bC,bE) ~ normal( 0 , 0.5 ),\n        transpars&gt; vector[8]: delta_j &lt;&lt;- append_row( 0 , delta ),\n        simplex[7]: delta ~ dirichlet( a )\n    ), data=dat , chains=4 , cores=4 )\n\nl &lt;- link(mRXE2)\n\n\ndeltas from output show the proportion of the effect that is attributed to each education level\n\n\n# BIG MODEL\n\ndat$Y &lt;- standardize(d$age)\n\n# single-threaded version\nmRXEYG &lt;- ulam(\n    alist(\n        R ~ ordered_logistic( phi , alpha ),\n        phi &lt;- bE[G]*sum( delta_j[1:E] ) + \n               bA[G]*A + bI[G]*I + bC[G]*C +\n               bY[G]*Y,\n        alpha ~ normal( 0 , 1 ),\n        bA[G] ~ normal( 0 , 0.5 ),\n        bI[G] ~ normal( 0 , 0.5 ),\n        bC[G] ~ normal( 0 , 0.5 ),\n        bE[G] ~ normal( 0 , 0.5 ),\n        bY[G] ~ normal( 0 , 0.5 ),\n        vector[8]: delta_j &lt;&lt;- append_row( 0 , delta ),\n        simplex[7]: delta ~ dirichlet( a )\n    ), data=dat , chains=4 , cores=4 )\n\n# multi-threaded version\nmRXEYGt &lt;- ulam(\n    alist(\n        R ~ ordered_logistic( phi , alpha ),\n        phi &lt;- bE[G]*sum( delta_j[1:E] ) + \n               bA[G]*A + bI[G]*I + bC[G]*C +\n               bY[G]*Y,\n        alpha ~ normal( 0 , 1 ),\n        bA[G] ~ normal( 0 , 0.5 ),\n        bI[G] ~ normal( 0 , 0.5 ),\n        bC[G] ~ normal( 0 , 0.5 ),\n        bE[G] ~ normal( 0 , 0.5 ),\n        bY[G] ~ normal( 0 , 0.5 ),\n        vector[8]: delta_j &lt;&lt;- append_row( 0 , delta ),\n        simplex[7]: delta ~ dirichlet( a )\n    ), data=dat , chains=4 , cores=4 , threads=2 )\n\nprecis(mRXEYGt,2)"
  },
  {
    "objectID": "notes/notes-11.html#complex-causal-effects",
    "href": "notes/notes-11.html#complex-causal-effects",
    "title": "Lecture 11 - Ordered Categories",
    "section": "Complex Causal Effects",
    "text": "Complex Causal Effects\n\ncausal effects (predicted consequences of intervention) require marginalization\ncausal effect of education requires distribution of age and gender to average over\nsimulate causal effects after thinking carefully over the range of the population you’d like to estimate over\nproblem 1: should not marginalize over this sample because of selection bias (participation)! Post-stratify to new target\nproblem 2: should not set all ages to the same education\n\nwhat does a real population look like?\n\ncausal effect of age requires effect of age on education, which we cannot estimate (because of participation!)\nno matter how complex, its still just a generative simulation using posterior samples\n\nneed generative model to plan estimation\nneed generative model to compute causal estimates"
  },
  {
    "objectID": "notes/notes-11.html#repeat-observations",
    "href": "notes/notes-11.html#repeat-observations",
    "title": "Lecture 11 - Ordered Categories",
    "section": "Repeat Observations",
    "text": "Repeat Observations\n\nrepeating stories and individuals\n\nnot confounds because the treatment is randomized"
  },
  {
    "objectID": "notes/notes-11.html#bonus-post-stratification",
    "href": "notes/notes-11.html#bonus-post-stratification",
    "title": "Lecture 11 - Ordered Categories",
    "section": "Bonus: Post-Stratification",
    "text": "Bonus: Post-Stratification\n\nquality of data is more important than quantity\nbigger samples amplify biases\na non-representative sample can be better than a representative one\n\ndifferent aspects of data matter than representation\ncan correct for non-representativeness\n\nbasic problem: sample is not the target\n\npost-stratification is principled methods for extrapolating from sample to population\npost-stratification requires causal model of reasons sample differs from population\n\nselection nodes\n\n[S] indicates what the sample is being selected by (e.g., age -&gt; different ages are less likely to respond to a survey)\nmany sources of data are already filtered by selection effects\nthe right thing to do depends upon causes of selection\n\nmany questions are really post-stratification questions\njustified descriptions require causal information and post stratification\ntime trends should account for changes in measurement/population\ncomparison is post-stratification from one population to another\nPAPER: a causal framework for cross-cultural generalizability\n4 step plan for honest digital scholarship\n\n\n\nwhat are we trying to describe?\nwhat is the ideal data for doing so?\nwhat data do we actually have?\nwhat causes the differences between (2) and (3)?\n[optional] is there a way to use (3) + (4) to do (1)?"
  }
]