[
  {
    "objectID": "CODE_OF_CONDUCT.html",
    "href": "CODE_OF_CONDUCT.html",
    "title": "Statistical Rethinking colearning 2023 - Bella",
    "section": "",
    "text": "We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.\nWe pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.\n\n\n\nExamples of behavior that contributes to a positive environment for our community include:\n\nDemonstrating empathy and kindness toward other people\nBeing respectful of differing opinions, viewpoints, and experiences\nGiving and gracefully accepting constructive feedback\nAccepting responsibility and apologizing to those affected by our mistakes, and learning from the experience\nFocusing on what is best not just for us as individuals, but for the overall community\n\nExamples of unacceptable behavior include:\n\nThe use of sexualized language or imagery, and sexual attention or advances of any kind\nTrolling, insulting or derogatory comments, and personal or political attacks\nPublic or private harassment\nPublishing others’ private information, such as a physical or email address, without their explicit permission\nOther conduct which could reasonably be considered inappropriate in a professional setting\n\n\n\n\nCommunity leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.\nCommunity leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.\n\n\n\nThis Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.\n\n\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be reported to Alec (robit.alec@gmail.com), or Isabella. All complaints will be reviewed and investigated promptly and fairly.\nAll community leaders are obligated to respect the privacy and security of the reporter of any incident.\n\n\n\nCommunity leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:\n\n\nCommunity Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.\nConsequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.\n\n\n\nCommunity Impact: A violation through a single incident or series of actions.\nConsequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.\n\n\n\nCommunity Impact: A serious violation of community standards, including sustained inappropriate behavior.\nConsequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.\n\n\n\nCommunity Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals.\nConsequence: A permanent ban from any sort of public interaction within the community.\n\n\n\n\nThis Code of Conduct is adapted from the Contributor Covenant, version 2.0, available at https://www.contributor-covenant.org/version/2/0/code_of_conduct.html.\nCommunity Impact Guidelines were inspired by Mozilla’s code of conduct enforcement ladder.\nFor answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistical Rethinking colearning 2023 - Bella",
    "section": "",
    "text": "NOTE: website format taken from Alec Robitaille\n\nSchedule\n\nLectures\nHomework\n\nParticipant notes and homework solutions\nResources\nInstallation\nCode of Conduct\n\n\n\nSecond round of Statistical Rethinking colearning, this time with 2023 lectures and homework.\nThe first round of Statistical Rethinking colearning (2022) is available here.\n\n\n\n\n\n\n\nMeeting date\nReading\nLectures\n\n\n\n\n26 January\nChapters 1, 2 and 3\n[1] <Science Before Statistics> <Slides>  [2] <Garden of Forking Data> <Slides>\n\n\n\nChapter 4\n[3] <Geocentric Models> <Slides>  [4] <Categories and Curves> <Slides>\n\n\n\nChapters 5 and 6\n[5] <Elemental Confounds> <Slides>  [6] <Good and Bad Controls> <Slides>\n\n\n\nChapters 7 and 8\n[7] Overfitting  [8] Interactions\n\n\n\nChapters 9, 10 and 11\n[9] Markov chain Monte Carlo  [10] Binomial GLMs\n\n\n\nChapters 11 and 12\n[11] Poisson GLMs  [12] Ordered Categories\n\n\n\nChapter 13\n[13] Multilevel Models  [14] Multi-Multilevel Models\n\n\n\nChapter 14\n[15] Varying Slopes  [16] Gaussian Processes\n\n\n\nChapter 15\n[17] Measurement Error  [18] Missing Data\n\n\n\nChapters 16 and 17\n[19] Beyond GLMs: State-space Models, ODEs  [20] Horoscopes\n\n\n\n\n\n\n\n\n\nMeeting date\nHomework\nSolutions\n\n\n\n\n2 February\nHomework 1\nSolutions\n\n\n\nHomework 2\nSolutions\n\n\n\nHomework 3\nSolutions\n\n\n\nHomework 4\nSolutions\n\n\n\nHomework 5\nSolutions\n\n\n\nHomework 6\nSolutions\n\n\n\nHomework 7\nSolutions\n\n\n\nHomework 8\nSolutions\n\n\n\nHomework 9\nSolutions\n\n\n\nHomework 10\nSolutions\n\n\n\n\n\n\n\n\nAlec\nBella (this repo)\n\n\n\n\n\nAdditional material using other packages or languages\n\nOriginal R: https://github.com/rmcelreath/rethinking/\nR + Tidyverse + ggplot2 + brms: https://bookdown.org/content/4857/\nPython and PyMC3: Python/PyMC3\nJulia and Turing: https://github.com/StatisticalRethinkingJulia and https://github.com/StatisticalRethinkingJulia/TuringModels.jl\n\nSee Richard’s comments about these here: https://github.com/rmcelreath/stat_rethinking_2023#coding\n2022 colearning:\n\nLectures: https://github.com/rmcelreath/stat_rethinking_2022#calendar--topical-outline\nHomework: https://github.com/rmcelreath/stat_rethinking_2022/tree/main/homework\n\nAlso, Alec’s notes and solutions of the 2019 material: https://github.com/robitalec/statistical-rethinking and https://www.statistical-rethinking.robitalec.ca/\n\n\n\nPackage specific install directions. We’ll update these as we go!\nRethinking\n\nrethinking\n\nStan\n\ncmdstanr\nRStan\nbrms\n\nTargets\n\ntargets\nstantargets\n\nV8, needed for the dagitty package\n\nV8\n\n\n\n\nPlease note that this project is released with a Code of Conduct. By participating in this project you agree to abide by its terms."
  },
  {
    "objectID": "homework/homework-02.html",
    "href": "homework/homework-02.html",
    "title": "Homework - Week 02",
    "section": "",
    "text": "Question 1: From the Howell1 dataset, consider only the people younger than 13 years old. Estimate the causal association between age and weight. Assume that age influences weight through two paths. First, age influences height, and height influences weight. Second, age directly influences weight through age-related changes in muscle growth and body proportions.\nDraw the DAG that represents these causal relationships. And then write a generative simulation that takes age as an input and simulates height and weight, obeying the relationships in the DAG.\nDAG:\n\n# DAG\ndag <- dagify(\n    weight ~ age + height,\n    height ~ age,\n    exposure = 'age',\n    outcome = 'weight'\n)\n\nggdag(dag) +\n    theme_dag()\n\n\n\n\nGenerative Simulation:\n\n# A = age, H = height, W = weight\nbAH <- 5 # as age increases, height increases by a lot\nbHW <- 0.5 # as height increases, weight proportionally increases cm/kg\nbAW <- 0.1 # as age increases, there is a small direct increase in weight\n\nsim_AW <- function(A, bAH, bHW, bAW){\n  N <- length(A)\n  H <- rnorm(N, bAH*A, 2)\n  W <- rnorm(N, bHW*H + bAW*A, 2)\n  data.frame(A, H, W)\n}\n\n# randomly generated individuals \nA <- runif(n=20, min=1, max=13)\n\ntest_sim <- sim_AW(A, bAH, bHW, bAW)\n\n# view relationship\nggplot(data = test_sim, aes(x=A, y=W)) + \n  geom_point() + \n  theme_classic()\n\n\n\n\nHave some weird things happening here (decreases in weight with age) but overall a positive-ish relationship between age-weight that is partially dependent on height.\n\n\nQuestion 2: Use a linear regression to estimate the total causal effect of each year of growth on weight.\nTotal causal effect means that we do not account for the effect of height. It is a linear regression of weight ~ age (where age is a proxy for year of growth).\n\ndata(Howell1)\nd <- Howell1 %>% \n  filter(age <= 13) %>%\n  select(c(weight, age))\n\n# get a list of priors present in the model\ndefault_prior <- get_prior(weight ~ age, data = d, family = gaussian())\n\n# get model\ntar_load(h02_mAW)\nmAW <- h02_mAW\n\n# model formula\nmAW$formula\n\nweight ~ age \n\n# priors\nmAW$prior\n\n          prior     class coef group resp dpar nlpar lb ub       source\n uniform(0, 10)         b                                          user\n uniform(0, 10)         b  age                             (vectorized)\n   normal(5, 1) Intercept                                          user\n exponential(1)     sigma                             0            user\n\n# check diagnostics\nplot(mAW)\n\n\n\n# get a summary of posterior distribution\nposterior_summary(mAW)\n\n               Estimate  Est.Error        Q2.5       Q97.5\nb_Intercept    7.036794 0.36425544    6.327332    7.753923\nb_age          1.335759 0.05138795    1.235176    1.442238\nsigma          2.596536 0.15143607    2.320393    2.903958\nlprior       -55.469858 2.07865655  -59.474311  -51.354272\nlp__        -427.576650 1.31101667 -431.020299 -426.121602\n\n\n\n\nQuestion 3: Now suppose the causal association between age and weight might be different for boys and girls. Use a single linear regression, with a categorical variable for sex, to estimate the total causal effect of age on weight separately for boys and girls. How do girls and boys differ? Provide one or more posterior contrasts as a summary.\nUpdated DAG:\n\n# DAG\ndag <- dagify(\n    weight ~ height + age + sex,\n    height ~ age + sex,\n    exposure = 'age',\n    outcome = 'weight'\n)\n\nggdag(dag) +\n    theme_dag()\n\n\n\n\nModel, stratified by sex:\n\ndata(Howell1)\nd2 <- Howell1 %>% \n  filter(age <= 13) %>%\n  mutate(sex = as.factor(case_when(male == 0 ~ 1, \n                         male == 1 ~ 2))) %>%\n  select(c(weight, age, sex))\n\n# get a list of priors present in the model\ndefault_prior2 <- get_prior(weight ~ age + sex, data = d2, family = gaussian())\n\n# load model\ntar_load(h02_mAWS)\nmAWS <- h02_mAWS\n\n# model formula\nmAWS$formula\n\nweight ~ age + sex \n\n# priors\nmAWS$prior\n\n          prior     class coef group resp dpar nlpar lb ub       source\n uniform(0, 10)         b                                          user\n uniform(0, 10)         b  age                             (vectorized)\n uniform(0, 10)         b sex2                             (vectorized)\n   normal(5, 1) Intercept                                          user\n exponential(1)     sigma                             0            user\n\n# check diagnostics\nplot(mAWS)\n\n\n\n# get a summary of posterior distribution\nposterior_summary(mAWS)\n\n               Estimate  Est.Error         Q2.5       Q97.5\nb_Intercept    6.421874 0.39163836    5.6340126    7.180696\nb_age          1.328347 0.04822519    1.2345120    1.425671\nb_sex2         1.406962 0.39901129    0.6507742    2.223799\nsigma          2.499072 0.13997303    2.2404017    2.788606\nlprior       -57.983164 1.96677528  -61.9331028  -54.196221\nlp__        -424.165388 1.42733997 -427.9576483 -422.453753\n\n# get prediction options \npred_vals <- data.frame(age = seq(0, 13, 0.5))\n\n# simulate from model where sex = 1 and sex = 2 \npost_pred_female <- predicted_draws(mAWS, pred_vals %>% mutate(sex = 1)) \npost_pred_male <- predicted_draws(mAWS, pred_vals %>% mutate(sex = 2))\n\n# prediction table \npost_differences <- data.frame(\n  predict_sex_female = post_pred_female$.prediction,\n  predict_sex_male = post_pred_male$.prediction,\n  age = post_pred_female$age\n) %>%\n  mutate(diff = predict_sex_male - predict_sex_female)\n\n\n\nggplot(post_differences, aes(age, diff)) +\n    stat_lineribbon() +\n  scale_fill_manual(values = met.brewer(\"VanGogh3\", 3)) +\n    labs(\n      x = 'age',\n        y = 'predicted difference in weight (males-females)') +\n  theme_classic()\n\n\n\n\n\n\nQuestion 4: The data in data(Oxboys) (rethinking package) are growth records for 26 boys measured over 9 periods. I want you to model their growth. Specifically, model the increments in growth from one period (Occasion in the data table) to the next. Each increment is simply the difference between height in one occasion and height in the previous occasion. Since none of these boys shrunk during the study, all of the growth increments are greater than zero. Estimate the posterior distribution of these increments. Constrain the distribution so it is always positive-it should not be possible for the model to think that boys can shrink from year to year. Finally compute the posterior distribution of the total growth over all 9 occasions."
  },
  {
    "objectID": "homework/homework-01.html",
    "href": "homework/homework-01.html",
    "title": "Homework - Week 01",
    "section": "",
    "text": "Question 1: Suppose the globe tossing data (Lecture 2, Chapter 2) had turned out to be 4 water and 11 land. Construct the posterior distribution.\n\n# define how many tosses were W & L\nw <- 4\nl <- 11\n\n# define p, every 0.01 between 0,1 \np <- seq(0, 1, by = 0.01)\n\n# function to compute posterior by every p\ncompute_posterior <- function(W, L, poss){\n  ways <- sapply(poss, function(q) (q*4)^W * ((1-q)*4)^L)\n  post <- ways/sum(ways)\n  data.frame(poss, ways, post = round(post,3))\n}\n\n# compute posterior for our data\nposterior <- compute_posterior(w, l, p)\n\n# visualize posterior \nggplot(data = posterior) + \n  geom_line(aes(poss, post)) + \n  theme_classic() + \n  xlab(\"proportion water\") + \n  ylab(\"probability\")\n\n\n\n\nNOTE: Bonus answer in solutions is somewhat key to understanding next answer. This specific problem, where you have a continuous value bounded by 0,1 with an infinite number of p, follows a beta distribution. Because the beta distribution is defined, you can instead just use that mathematical equation to come to the same conclusion as above.\n\nggplot() +\n  stat_function(fun = function(x) dbeta(x, 4+1, 11+1), color = \"black\",\n                size = 1) +\n  theme_classic() + \n  xlab(\"p\") + \n  ylab(\"\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\nQuestion 2: Using the posterior distribution from 1, compute the posterior predictive distribution for the next 5 tosses of the same globe. I recommend you use the sampling method.\n\n# sample our posterior (beta distribution with 4 W, 11 L)\nn <- 1e4\npost_samples <- rbeta(n, 4+1, 11+1)\n\n# sim globe function\nsim_globe <- function(p, N){\n  \n  sample(c(\"W\", \"L\"), size = N, prob=c(p, 1-p), replace = T)\n\n}\n\n# simulate posterior predictive distribution for next 5 tosses of the globe\n# how many of the 5 tosses will = W? Tested 1e4 times\npred_post <- sapply(post_samples, function(p) sum(sim_globe(p, 5)==\"W\"))\n\n# create table that counts number of incidences of each option (i.e., how many times will 0 W appear, 1 W, 2 W, 3 W, 4 W, 5 W?)\ntab_post <- table(pred_post)\npred_post_df <- as.data.frame(tab_post)\n\nggplot(pred_post_df, aes(as.factor(pred_post), Freq)) + \n  geom_col() + \n  theme_classic() + \n  xlab(\"number of W\") + \n  ylab(\"count\")\n\n\n\n\nNOTE: Can also use rbinom() function instead of sim_globe() since there are only two possible outcomes.\n\n\nQuestion 3: Use the posterior predictive distribution from 2 to calculate the probability of 3 or more water samples in the next 5 tosses.\nWe know that \\[ p = \\frac{ways}{sum(ways)} \\] Therefore, count the incidences of 3, 4, 5 in posterior predictive distribution from 2 and divide by total number of ways (1e4)\n\np_threeplus <- (sum(pred_post_df[which(pred_post_df[,1]==3 | pred_post_df[,1]==4 | \n                                         pred_post_df[,1]==5),2]))/n\n\nprint(paste0(\"p = \", p_threeplus))\n\n[1] \"p = 0.1766\"\n\n\n\n\nQuestion 4: Suppose you observe W = 5 water points, but you forgot to write down how many times the globe was tossed, so you don’t know the number of land points L. Assume that p = 0.7 and compute the posterior distribution of the number of tosses N. Hint: Use the binomial distribution.\nBefore, we were solving for p using \\[ p^W(1-p)^L \\] Which was a beta distribution because p was a continuous variable bound between 0,1. Now we want to solve for N, with W and p. We have a success/fail dataset, with a probability of success, therefore we use the binomial distribution. We need to calculate the probability of 0-5 N’s resulting in 5 successes (W).\n\n# N & L = unknown\nW <- 5\np <- 0.7\nN_max <- 25\nlst <- seq(W, N_max, by = 1)\n\n# calculate binomial distribution for each N between lower bound (W) and upper bound (N_max) with p = 0.7\nx <- sapply(lst, function(x) dbinom(W, x, p))\n\nggplot(data.frame(data = x, N = W:N_max), aes(x = N, y = x)) +\n  geom_col() +\n  labs(y = \"Probability\", x = \"N\") + \n  theme_classic()"
  },
  {
    "objectID": "homework/homework-listing.html",
    "href": "homework/homework-listing.html",
    "title": "Homework",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\nDate\n\n\nAuthor\n\n\n\n\n\n\nHomework - Week 01\n\n\n\n\nIsabella C. Richmond\n\n\n\n\nHomework - Week 02\n\n\n\n\nIsabella C. Richmond\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "notes/notes-04.html",
    "href": "notes/notes-04.html",
    "title": "Lecture 04 - Categories & Curves",
    "section": "",
    "text": "Rose: learning what GAMs are , thinking of age/time as a cause\nThorn: I haven’t taken time to fix the plot code from the lectures"
  },
  {
    "objectID": "notes/notes-04.html#drawing-inferences",
    "href": "notes/notes-04.html#drawing-inferences",
    "title": "Lecture 04 - Categories & Curves",
    "section": "Drawing Inferences",
    "text": "Drawing Inferences\n\nlinear model can accomodate anything, thus we need to think carefully about our scientific model\ngenerative model + multiple estimands = multiple estimators\nquite often the estimate we want is not in a summary table because it depends on multiple unknowns, so we often need to do post-processing"
  },
  {
    "objectID": "notes/notes-04.html#categories",
    "href": "notes/notes-04.html#categories",
    "title": "Lecture 04 - Categories & Curves",
    "section": "Categories",
    "text": "Categories\n\ncategories are discrete and non-linear\ndiscrete, unordered types\nwe want to stratify by category, to fit a separate line for each"
  },
  {
    "objectID": "notes/notes-04.html#howell-data",
    "href": "notes/notes-04.html#howell-data",
    "title": "Lecture 04 - Categories & Curves",
    "section": "Howell Data",
    "text": "Howell Data\n\n\nLoading required package: rstan\n\n\nLoading required package: StanHeaders\n\n\nLoading required package: ggplot2\n\n\nrstan (Version 2.21.8, GitRev: 2e1f913d3ca3)\n\n\nFor execution on a local, multicore CPU with excess RAM we recommend calling\noptions(mc.cores = parallel::detectCores()).\nTo avoid recompilation of unchanged Stan programs, we recommend calling\nrstan_options(auto_write = TRUE)\n\n\nLoading required package: cmdstanr\n\n\nThis is cmdstanr version 0.5.3\n\n\n- CmdStanR documentation and vignettes: mc-stan.org/cmdstanr\n\n\n- CmdStan path: /home/icrichmond/.cmdstan/cmdstan-2.31.0\n\n\n- CmdStan version: 2.31.0\n\n\nLoading required package: parallel\n\n\nrethinking (Version 2.21)\n\n\n\nAttaching package: 'rethinking'\n\n\nThe following object is masked from 'package:rstan':\n\n    stan\n\n\nThe following object is masked from 'package:stats':\n\n    rstudent\n\n\n\nhow are height, weight, and sex causally related?\n\n\nd <- dagitty(\"dag {\n                H -> W\n                S -> W\n                S -> H\n             }\")\n\ndrawdag(d)\n\n\n\n\n\nheight influences weight\nsex influences weight and height\nweight is influenced by height and sex\ninfluence of sex is both direct and indirect on weight\n\\(H = f_{H}(S)\\)\n\\(W = f_{W}(H,S)\\)\nUnobserved causes are ignorable unless they are shared between variables (common cause) = confound\n\n\nsim_HW <- function(S, b, a){\n  N <- length(S)\n  H <- ifelse(S==1, 150, 160) + rnorm(N, 0, 5)\n  W <- a[S] + b[S]*H + rnorm(N, 0, 5)\n  data.frame(S, H, W)\n}\n\nS <- rbern(100)+1\ndat <- sim_HW(S, b=c(0.5, 0.6), a=c(0,0))\nhead(dat)\n\n  S        H         W\n1 1 146.4099  73.51127\n2 1 149.9027  68.05120\n3 1 147.7109  68.68594\n4 2 163.7692  88.42429\n5 2 167.1060 106.98612\n6 2 155.6135 102.51284\n\n\n\nscientific questions:\n\ncausal effect of H on W?\ncausal effect of S on W?\ndirect causal effect of S on W?\n\nwe need to stratify by S to answer qs 2 and 3\ncoding categorical variables\n\nindicator variables (0/1)\nindex variables (1,2,3,4)\nindex variables are generally preferable\n\nindex variables\n\noften we want to give each index the same prior\n\ntotal causal effect of sex\n\n\nS <- rep(1, 100)\nsimF <- sim_HW(S, b=c(0.5, 0.6), a=c(0,0))\n\nS <- rep(2, 100)\nsimM <- sim_HW(S, b=c(0.5, 0.6), a=c(0,0))\n\n# effect of sex (male-female)\nmean(simM$W - simF$W)\n\n[1] 20.24868\n\n\n\nestimating model and synthetic example\n\n\nS <- rbern(100)+1\ndat <- sim_HW(S, b = c(0.5,0.6), a = c(0,0))\n\nm_SW <- quap(alist(\n  W ~ dnorm(mu, sigma),\n  mu <- a[S],\n  a[S] ~ dnorm(60, 10),\n  sigma ~ dunif(0, 10)\n), data = dat)\n\nprecis(m_SW, depth = 2)\n\n           mean        sd     5.5%     94.5%\na[1]  74.670123 0.8155807 73.36667 75.973578\na[2]  96.430134 0.7843093 95.17666 97.683611\nsigma  5.668577 0.4012195  5.02735  6.309803\n\n\n\nanalyze the real sample\n\n\nd <- Howell1\nd <- d[ d$age >= 18,]\n\ndat <- list(\n  W = d$weight,\n  S = d$male + 1\n)\n\nm_SW <- quap(alist(\n  W ~ dnorm(mu, sigma), \n  mu <- a[S],\n  a[S] ~ dnorm(60, 10),\n  sigma ~ dunif(0, 10)\n), data = dat)\n\n\nposterior means and predictions\n\n\n# posterior mean W\npost <- extract.samples(m_SW)\ndens(post$a[,1])\n\n\n\n#dens(post$a[,2], add = T)\n\n# posterior W predictions\nW1 <- rnorm(1000, post$a[,1], post$sigma)\nW2 <- rnorm(1000, post$a[,2], post$sigma)\ndens(W1)\n\n\n\n#dens(W2, add = T)\n\n# contrast\nW_contrast <- W2 - W1\ndens(W_contrast)\n\n\n\n# proportion above zero\nsum(W_contrast > 0)/1000\n\n[1] 0.814\n\nsum(W_contrast < 0)/1000\n\n[1] 0.186\n\n\n\ncontrasting\n\nneed to compute the difference between the categories\nit is not legitimate to compare overlap in distributions\nwe must compute contrast distribution\n\n\n\nmu_contrast <- post$a[,2] - post$a[,1]\ndens(mu_contrast)\n\n\n\n\n\n“controlling” for the indirect effect of sex through height\n\n\nS <- rbern(100)+1\n# slopes are the same so there is no effect of height on weight through slope but men are on average 10 kg heavier (intercept 10)\nset.seed(12)\ndat <- sim_HW(S, b = c(0.5, 0.5), a = c(0, 10))\n\n\n\\(W_{i} \\sim Normal(\\mu _{i}, \\sigma)\\)\n\\(\\mu _{i} = \\alpha _{S[i]} + \\beta _{S[i]}(H_{i} - \\bar H)\\)\n\nThis equation centers the height \\((H_{i} - \\bar H)\\)\nCentering H means that alpha represents the average weight of a person with average height\n\\(\\alpha = [\\alpha _{1}, \\alpha _{2}]\\) , \\(\\beta = [\\beta _{1}, \\beta _{2}]\\)\n\nanalyze the sample\n\n\nd <- Howell1\nd <- d[d$age >= 18, ]\ndat <- list(W = d$weight, H = d$height, Hbar = mean(d$height), S = d$male + 1)\n\nm_SHW <- quap(alist(\n  W ~ dnorm(mu, sigma),\n  mu <- a[S] + b[S]*(H-Hbar),\n  a[S] ~ dnorm(60, 10),\n  b[S] ~ dunif(0, 1),\n  sigma ~ dunif(0, 10)\n), data = dat)\n\n\nwe need to compute the difference of expected weight at each height to get the actual estimate that we are looking for (for the direct effect of sex on weight)\n\n\nxseq <- seq(from=130, to=190, len=50)\n\nmuF <- link(m_SHW, data = list(S=rep(1,50), H=xseq, Hbar=mean(d$height)))\n#lines(xseq, apply(muF, 2, mean))\n\nmuM <- link(m_SHW, data = list(S=rep(2,50), H=xseq, Hbar = mean(d$height)))\n#lines(xseq, apply(muM, 2, mean))\n\nmu_contrast <- muF - muM\n\n#plot(NULL, xlim=range(xseq))\n#for (p in c(0.5, 0.6, 0.7, 0.8, 0.9, 0.99))\n#  shade(apply(mu_contrast, 2, PI, prob = p), xseq)\n#abline(h=0)\n\n\nnearly all of the causal effects of S acts through H\n\nwhen we block H, we see very little effect of sex on weight"
  },
  {
    "objectID": "notes/notes-04.html#categorical-variables",
    "href": "notes/notes-04.html#categorical-variables",
    "title": "Lecture 04 - Categories & Curves",
    "section": "Categorical Variables",
    "text": "Categorical Variables\n\ncommon, easy to use with index coding\nuse samples to compute relevant contrasts\nalways summarize (mean, interval) as the last step\nwe want mean difference and not difference of means"
  },
  {
    "objectID": "notes/notes-04.html#curves-from-lines",
    "href": "notes/notes-04.html#curves-from-lines",
    "title": "Lecture 04 - Categories & Curves",
    "section": "Curves from Lines",
    "text": "Curves from Lines\n\nmany non linear relationships\nlinear models can easily fit curves: 2 strategies\n\npolynomials (bad)\nsplines and GAMs (less bad)\n\npolynomial models\n\nstill linear because its an additive function of the parameters\ncreate strange symmetries and explosive uncertainty\nno local smoothing, only global smoothing\ndo not use\n\nsplines\n\ngreat for locally inferred function\nadd together a bunch of locally trained terms\ncan add as many locally trained terms as you want\neach term has a weight and slope and only affects its own region"
  },
  {
    "objectID": "notes/notes-04.html#full-luxury-bayes",
    "href": "notes/notes-04.html#full-luxury-bayes",
    "title": "Lecture 04 - Categories & Curves",
    "section": "Full Luxury Bayes",
    "text": "Full Luxury Bayes\n\ninstead of two models for two estimands, use one model for full causal model\ncan simulate interventions with this approach\n\n\nm_SHW_full <- quap(alist(\n  \n  # weight\n  W ~ dnorm(mu, sigma),\n  mu <- a[S] + b[S]*(H-Hbar),\n  a[S] ~ dnorm(60, 10),\n  b[S] ~ dunif(0, 1),\n  sigma ~ dunif(0, 10),\n  \n  # height\n  H ~ dnorm(nu, tau),\n  nu <- h[S],\n  h[S] ~ dnorm(160, 10),\n  tau ~ dunif(0, 10)\n  \n), data = dat)\n\n\npost <- extract.samples(m_SHW_full)\nHbar <- dat$Hbar\nn <- 1e4\n\nwith(post, {\n  \n  H_S1 <- rnorm(n, h[,1], tau)\n  W_S1 <- rnorm(n, a[,2] + b[,1]*(H_S2-Hbar), sigma)\n  \n  W_do_S <<- W_S2 - W_S1\n  \n})\n\n# automate\nHWsim <- sim(m_SHW_full, data = list(S=c(1,2)), vars = c(\"H\", \"W\"))\nW_do_S_auto <- HWsim$W[,2] - HWsim$W[,1]\n\n\nyou can either do one statistical model for each estimand OR one simulation for each estimand (full luxury Bayes)"
  },
  {
    "objectID": "notes/notes-04.html#todo",
    "href": "notes/notes-04.html#todo",
    "title": "Lecture 04 - Categories & Curves",
    "section": "TODO:",
    "text": "TODO:\n\nfix weird plot erroring"
  },
  {
    "objectID": "notes/notes-02.html",
    "href": "notes/notes-02.html",
    "title": "Lecture 02 - The Garden of Forking Data",
    "section": "",
    "text": "Rose: remembering that posterior distributions are relative probabilities is a great foundation to move forward on\nThorn: equations are harddd + baseR + installing rstan + misclassification ahh"
  },
  {
    "objectID": "notes/notes-02.html#globe-example",
    "href": "notes/notes-02.html#globe-example",
    "title": "Lecture 02 - The Garden of Forking Data",
    "section": "Globe Example",
    "text": "Globe Example\nestimand: proportion of the globe covered in water - do people know what an estimand is?"
  },
  {
    "objectID": "notes/notes-02.html#generative-model-globe",
    "href": "notes/notes-02.html#generative-model-globe",
    "title": "Lecture 02 - The Garden of Forking Data",
    "section": "Generative Model (globe)",
    "text": "Generative Model (globe)\n\nstart with how the variables influence each other, i.e., causal model\n\n\nlibrary(dagitty)\nlibrary(rethinking)\n\nLoading required package: rstan\n\n\nLoading required package: StanHeaders\n\n\nLoading required package: ggplot2\n\n\nrstan (Version 2.21.8, GitRev: 2e1f913d3ca3)\n\n\nFor execution on a local, multicore CPU with excess RAM we recommend calling\noptions(mc.cores = parallel::detectCores()).\nTo avoid recompilation of unchanged Stan programs, we recommend calling\nrstan_options(auto_write = TRUE)\n\n\nLoading required package: cmdstanr\n\n\nThis is cmdstanr version 0.5.3\n\n\n- CmdStanR documentation and vignettes: mc-stan.org/cmdstanr\n\n\n- CmdStan path: /home/icrichmond/.cmdstan/cmdstan-2.31.0\n\n\n- CmdStan version: 2.31.0\n\n\nLoading required package: parallel\n\n\nrethinking (Version 2.21)\n\n\n\nAttaching package: 'rethinking'\n\n\nThe following object is masked from 'package:rstan':\n\n    stan\n\n\nThe following object is masked from 'package:stats':\n\n    rstudent\n\nd <- dagitty(\"dag {\n                p -> W\n                p -> L\n                N -> W \n                N -> L }\")\n\ndrawdag(d)\n\n\n\n\n\\[\nW,L = f(p,N)\n\\]\n\nW and L are functions of p and N"
  },
  {
    "objectID": "notes/notes-02.html#bayesian-data-analysis",
    "href": "notes/notes-02.html#bayesian-data-analysis",
    "title": "Lecture 02 - The Garden of Forking Data",
    "section": "Bayesian Data Analysis",
    "text": "Bayesian Data Analysis\nFor each possible explanation of the sample,\n\ncount all the ways the sample could happen\nexplanations with more ways to produce the sample are more plausible"
  },
  {
    "objectID": "notes/notes-02.html#garden-of-forking-data",
    "href": "notes/notes-02.html#garden-of-forking-data",
    "title": "Lecture 02 - The Garden of Forking Data",
    "section": "Garden of Forking Data",
    "text": "Garden of Forking Data\n\nrelies on samples being independent\nrelative differences between probabilities are dependent on sample size (differences will be smaller with smaller sample sizes because there is less evidence)\nnormalizing to probability allows for interpretability and easier math\ncollection of probabilities is a posterior distribution\n\nADD CODE (minute 35):\n\nsample <- c(\"W\", \"L\", \"W\", \"W\", \"W\", \"L\", \"W\", \"L\", \"W\")\n\nW <- sum(sample==\"W\")\nL <- sum(sample==\"L\")\np <- c(0, 0.25, 0.5, 0.75, 1)\nways <- sapply(p, function(q) (q*4)^W * ((1-q)*4)^L)\nprob <- ways/sum(ways)\ncbind(p, ways, prob)\n\n        p ways       prob\n[1,] 0.00    0 0.00000000\n[2,] 0.25   27 0.02129338\n[3,] 0.50  512 0.40378549\n[4,] 0.75  729 0.57492114\n[5,] 1.00    0 0.00000000"
  },
  {
    "objectID": "notes/notes-02.html#testing",
    "href": "notes/notes-02.html#testing",
    "title": "Lecture 02 - The Garden of Forking Data",
    "section": "Testing",
    "text": "Testing\n\nhave to test\ntest using extreme values where you intuitively know the right answer\n\n\nsim_globe <- function(p = 0.7, N = 9){\n  \n  sample(c(\"W\", \"L\"), size = N, prob=c(p, 1-p), replace = T)\n}\n\nsim_globe()\n\n[1] \"W\" \"W\" \"L\" \"W\" \"W\" \"W\" \"W\" \"W\" \"W\"\n\nsim_globe(p=1, N=11) \n\n [1] \"W\" \"W\" \"W\" \"W\" \"W\" \"W\" \"W\" \"W\" \"W\" \"W\" \"W\"\n\nsum(sim_globe(p=0.5, N=1e4) == \"W\")/1e4\n\n[1] 0.502\n\n\nFunction:\n\nlibrary(crayon)\n\n\nAttaching package: 'crayon'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    %+%\n\nmake_bar <- function(q,size=20) {\n    n <- round(q*size)\n    s1 <- concat( rep(\"#\",n) )\n    s2 <- concat( rep(\" \",size-n) )\n    concat(s1,s2)\n}\n\ncompute_posterior <- function(the_sample, poss = c(0,0.25,0.5,0.75,1)){\n  W <- sum(the_sample==\"W\")\n  L <- sum(the_sample==\"L\")\n  ways <- sapply(poss, function(q) (q*4)^W * ((1-q)*4)^L)\n  post <- ways/sum(ways)\n  bars <- sapply(post, function(q) make_bar(q))\n  data.frame(poss, ways, post = round(post,3), bars)\n}\n\ncompute_posterior(sim_globe())\n\n  poss ways  post                 bars\n1 0.00    0 0.000                     \n2 0.25   81 0.097 ##                  \n3 0.50  512 0.612 ############        \n4 0.75  243 0.291 ######              \n5 1.00    0 0.000"
  },
  {
    "objectID": "notes/notes-02.html#real-number-sampling",
    "href": "notes/notes-02.html#real-number-sampling",
    "title": "Lecture 02 - The Garden of Forking Data",
    "section": "Real Number Sampling",
    "text": "Real Number Sampling\n\nmore possibilities = less probability in each option/outcome\n\nprobability is spread out across many options\n\nnormalizing to probability allows our equation to calculate infinite number of “sides”/outcomes\n\n\\[\np^W(1-p)^L\n\\]\np = probability\ndensity = probability when we are assessing infinite number of possibilities\n\nshape of the posterior embodies sample size\n\nno min sample size -> just more uncertain posterior\nposterior distribution embodies sample size\n\nno point estimates! estimate is entire posterior distribution\n\ncan use summary points from post dist for communication purposes\n\nintervals are merely indicators of the shape of the posterior distribution\n\nno “true interval” i.e., 95% CI doesn’t exist\ninterval is just distribution lower/upper bounds"
  },
  {
    "objectID": "notes/notes-02.html#analyze-sample-summarize",
    "href": "notes/notes-02.html#analyze-sample-summarize",
    "title": "Lecture 02 - The Garden of Forking Data",
    "section": "Analyze Sample + Summarize",
    "text": "Analyze Sample + Summarize\n\npost_samples <- rbeta(1e3, 6+1, 3+1)\n\ndens(post_samples, lwd = 4, col = 2, xlab = \"prop water\", adj = 0.1)\n\ncurve(dbeta(x, 6+1, 3+1), add = T, lty = 2, lwd = 3)\n\n\n\n\n\nposterior prediction = “what would we bet?”\n\nhow many W’s do we expect to see in the next 10 tosses\n\nfor each sample of post dist, we can create a predictive distribution, then posterior predictive\n\nincorporates uncertainty from posterior distribution\n\n\n\npost_samples <- rbeta(1e4, 6+1, 3+1)\n\npred_post <- sapply(post_samples, function(p) \nsum(sim_globe(p, 10)==\"W\"))\n\ntab_post <- table(pred_post)\n\n#for (i in 0:10) lines(c(i,i),c(0,tab_post[i+1]), lwd = 4, col = 4)"
  },
  {
    "objectID": "notes/notes-02.html#misclassification-bonus-round",
    "href": "notes/notes-02.html#misclassification-bonus-round",
    "title": "Lecture 02 - The Garden of Forking Data",
    "section": "Misclassification (Bonus Round)",
    "text": "Misclassification (Bonus Round)\n\nW* is misclassified due to sampling error and measurement process\n\ntrue W is unknown\n\n\n\nlibrary(dagitty)\nlibrary(rethinking)\n\nd <- dagitty(\"dag {\n                p -> W\n                N -> W\n                W -> Wm\n                M -> Wm\n             }\")\n\ndrawdag(d)\n\n\n\n\n\nincorporate measurement error with x (error rate of 10%)\n\n\nsim_globe2 <- function(p = 0.7, N = 9, x = 0.1){\n  \n  true_sample <- sample(c(\"W\", \"L\"), size = N, prob = c(p, 1-p), replace = T)\n  \n  obs_sample <- ifelse(runif(N) < x,\n                       ifelse(true_sample == \"W\", \"L\", \"W\"),\n                       true_sample)\n  \n  return(obs_sample)\n  \n}\n\n\nhow do you know the error rate?\ndon’t understand mechanism behind incorporating x but understand why x needs to be incorporated + consequences of not"
  },
  {
    "objectID": "notes/notes-02.html#todo",
    "href": "notes/notes-02.html#todo",
    "title": "Lecture 02 - The Garden of Forking Data",
    "section": "TODO",
    "text": "TODO\n\nDAG coords\nRead misclassification in 3rd ed."
  },
  {
    "objectID": "notes/notes-01.html",
    "href": "notes/notes-01.html",
    "title": "Lecture 01 - The Golem of Prague",
    "section": "",
    "text": "Rose: excited for fewer examples + sensitivity analyses\nBONUS: I LOVE DAGS\nThorn: i am having a hard time imagining/extending my thinking re: causal imputation + unique null hypotheses (focus on causation)"
  },
  {
    "objectID": "notes/notes-01.html#third-edition",
    "href": "notes/notes-01.html#third-edition",
    "title": "Lecture 01 - The Golem of Prague",
    "section": "Third Edition",
    "text": "Third Edition\n\npeach boxes instead of blue boxes"
  },
  {
    "objectID": "notes/notes-01.html#causal-inference",
    "href": "notes/notes-01.html#causal-inference",
    "title": "Lecture 01 - The Golem of Prague",
    "section": "Causal Inference",
    "text": "Causal Inference\n\nstatistical models require scientific (causal) models\ncorrelation is a very limited measure of association\n\nassociation can occur without correlation\n\ncausal prediction = prediction of the consequences of an intervention (implications of changing one variable on another variable)\n\nknowing the cause of an action allows you to create predictions\n\ncausal imputation = knowing the cause of an action allows you to reconstruct possible outcomes (i.e., what if I had done something else?)"
  },
  {
    "objectID": "notes/notes-01.html#dags",
    "href": "notes/notes-01.html#dags",
    "title": "Lecture 01 - The Golem of Prague",
    "section": "DAGs",
    "text": "DAGs\n\nabstract causal models: includes names of variables and their causal relationships\ntells you the consequences of an intervention\nfacilitates you asking scientific questions"
  },
  {
    "objectID": "notes/notes-01.html#golems",
    "href": "notes/notes-01.html#golems",
    "title": "Lecture 01 - The Golem of Prague",
    "section": "Golems",
    "text": "Golems\n\nstatistical models = golems\noften not possible to design and outline a null hypothesis that is meaningful to reject in observational science\n\nwhat is a null ecological community?\n\nthink of good example/explanation for no null ecology/previous two slides\n\nread textbook section\nfocus on third example?\ntakeaway is that null hypothesis does not give you cause/process behind outcome\n\nwhat is your null? is it unique?"
  },
  {
    "objectID": "notes/notes-01.html#todo",
    "href": "notes/notes-01.html#todo",
    "title": "Lecture 01 - The Golem of Prague",
    "section": "TODO",
    "text": "TODO\n\nread textbook section on null ecology\nagenda\ndiscuss code options"
  },
  {
    "objectID": "notes/notes-listing.html",
    "href": "notes/notes-listing.html",
    "title": "Notes",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\nDate\n\n\nAuthor\n\n\n\n\n\n\nLecture 01 - The Golem of Prague\n\n\nJan 23, 2023\n\n\nIsabella C. Richmond\n\n\n\n\nLecture 02 - The Garden of Forking Data\n\n\nJan 23, 2023\n\n\nIsabella C. Richmond\n\n\n\n\nLecture 03 - Geocentric Models\n\n\nFeb 8, 2023\n\n\nIsabella C. Richmond\n\n\n\n\nLecture 04 - Categories & Curves\n\n\nFeb 8, 2023\n\n\nIsabella C. Richmond\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "notes/notes-03.html",
    "href": "notes/notes-03.html",
    "title": "Lecture 03 - Geocentric Models",
    "section": "",
    "text": "Rose: flow reminder! Retrograde explanation\nThorn: installing rethinking on windows"
  },
  {
    "objectID": "notes/notes-03.html#linear-regressions",
    "href": "notes/notes-03.html#linear-regressions",
    "title": "Lecture 03 - Geocentric Models",
    "section": "Linear Regressions",
    "text": "Linear Regressions\n\nessentially a geocentric model - overly simplified\nseparate from a causal model\nassociations are from the causal model, not the statistical model"
  },
  {
    "objectID": "notes/notes-03.html#gaussian-distributions",
    "href": "notes/notes-03.html#gaussian-distributions",
    "title": "Lecture 03 - Geocentric Models",
    "section": "Gaussian Distributions",
    "text": "Gaussian Distributions\n\nthere are many more ways to end up in the center than to end up on the periphery\nwhy the Gaussian distribution spontaneously occurs in natural systems\ngenerative: if we add fluctuations, we tend towards normal distribution (lots of summed fluctuations in nature)\ninferential: estimating mean and variance, normal distribution is best one to use because it is least informative (no other information present)\nnormal distribution is just a tool for estimating mean/variance because it has widest distribution\n\ndata doesn’t have to be normal to be able to leverage the tool to estimate mean/variance"
  },
  {
    "objectID": "notes/notes-03.html#workflow",
    "href": "notes/notes-03.html#workflow",
    "title": "Lecture 03 - Geocentric Models",
    "section": "Workflow",
    "text": "Workflow\n\nState a clear question\nSketch causal assumptions\nDefine generative model from sketch\nUse generative model to build estimator\nProfit\n\n\nlibrary(rethinking)\nlibrary(dagitty)\ndata(Howell1)"
  },
  {
    "objectID": "notes/notes-03.html#describing-models",
    "href": "notes/notes-03.html#describing-models",
    "title": "Lecture 03 - Geocentric Models",
    "section": "Describing Models",
    "text": "Describing Models\n\\[\nW_{i}= \\beta H_{i} + U_{i}\n\\]\n\\[\nU_{i} \\sim Normal(0, \\sigma)\n\\]\n\\[\nH_{i} \\sim Uniform(130, 170)\n\\]\n\n= is deterministic\n~ is distributional"
  },
  {
    "objectID": "notes/notes-03.html#howell-example",
    "href": "notes/notes-03.html#howell-example",
    "title": "Lecture 03 - Geocentric Models",
    "section": "Howell Example",
    "text": "Howell Example\n\nQuestion: Describe association between adult weight and height\n\n\nd2 <- Howell1[Howell1$age>=18,]\n\n\nScientific model: weight is some function of height and unobserved influences\n\n\nd <- dagitty(\"dag {\n                H -> W\n                U -> W\n             }\")\n\ndrawdag(d)\n\n\n\n\n\\[\nW = f(H, U)\n\\]\n\nGenerative/Statistical model\n\n\nTwo options: dynamic (complex and ongoing) and static\n\nStatic model allows us to imagine changes at specific times and still use a Gaussian distribution\n\nFor adults, weight is a proportion of height plus the influence of unobserved causes\n\n\\[\nW = \\beta H+U\n\\]\n\nsim_weight <- function(H, b, sd){\n  U <- rnorm(length(H), 0, sd)\n  W <- b*H + U\n  return(W)\n}\n\nH <- runif(200, min = 130, max = 170)\nW <- sim_weight(H, b = 0.5, sd = 5)\nplot(W ~ H, col = 2, lwd = 3)\n\n\n\n\n\nwe want to estimate how the average weight changes with height\n\\[\nE(W_{i}|H_{i}) = \\alpha + \\beta H_{i}\n\\]\n\\(E(W_{i}|H_{i})\\) = average weight conditional on height\n\\(\\alpha\\) = intercept (when height is 0, what is weight? Scientifically should be zero but putting it in model to make sure our model is scientifically sound)\n\\(\\beta\\) = slope\nPosterior distribution:\n\n\\[\nPr(\\alpha,\\beta,\\sigma| H_{i}, W_{i}) = \\frac{Pr(W_{i}|H_{i}, \\alpha,\\beta,\\sigma)Pr(\\alpha,\\beta,\\sigma)}{Z}\n\\]\n\\(W_{i} \\sim Normal(\\mu _{i}), \\sigma)\\)\n\\(\\mu _{i} = \\alpha + \\beta H_{i}\\)\nalpha, sigma, beta are unknown so we need a posterior distribution for them (and they are dependent on the data)\n\\(Pr(\\alpha,\\beta,\\sigma| H_{i}, W_{i})\\) = posterior probability of specific line\n\\(Pr(W_{i}|H_{i}, \\alpha,\\beta,\\sigma)\\) = probability of each weight dependent on height value, alpha, beta, sigma\n\\(Pr(\\alpha,\\beta,\\sigma)\\) = prior\n\n\nStatistical Model\n\nQuadratic approximation\n\n\\(W_{i} \\sim Normal(\\mu _{i}), \\sigma)\\)\n\\(\\mu _{i} = \\alpha + \\beta H_{i}\\)\n\\(\\alpha \\sim Normal(0,10)\\)\n\\(\\beta \\sim Uniform(0,1)\\)\n\\(\\sigma \\sim Uniform(0, 10)\\)\n\n\n\nm3.1 <- quap(alist(\n  W ~ dnorm(mu, sigma),\n  mu <- a + b*H,\n  a ~ dnorm(0, 10),\n  b ~ dunif(0, 1),\n  sigma ~ dunif(0, 10)\n), data = list(W=W, H=H))\n\n\nPriors\n\nwe want to constrain to scientifically plausible values\njustify with information outside the data - like the rest of model\n\n\n\nn <- 1e3\na <- rnorm(n, 0, 10)\nb <- runif(n, 0, 1)\nplot(NULL, xlim = c(130, 170), ylim = c(50, 90))\n\n\n\n#for (j in 1:50) abline(a=a[j], b=b[j])\n\n\nValidate model\n\n\ntest statistical model with simulated observations from scientific model - need to test if your model is working\nuse many values and make sure your model responds appropriately\n\n\n# model summary\nprecis(m3.1)\n\n            mean         sd        5.5%     94.5%\na     -5.8858341 4.43189057 -12.9688512 1.1971830\nb      0.5333368 0.02965231   0.4859467 0.5807269\nsigma  5.0861692 0.25438887   4.6796067 5.4927318\n\n\n\nAnalyze data\n\n\ndat <- list(W = d2$weight, H = d2$height)\nm3.2 <- quap(alist(\n  W ~ dnorm(mu, sigma),\n  mu <- a + b*H,\n  a ~ dnorm(0, 10), \n  b ~ dunif(0, 1),\n  sigma ~ dunif(0, 10)\n), data = dat)\n\nprecis(m3.2)\n\n             mean         sd        5.5%      94.5%\na     -43.3811650 4.17121574 -50.0475734 -36.714757\nb       0.5717697 0.02695371   0.5286925   0.614847\nsigma   4.2532082 0.16178562   3.9946436   4.511773\n\n\n\nparameters are not independent of one another, they cannot be independently interpreted\n\nuse posterior predictions and describe/interpret those by sampling the posterior distribution\n\n\n\npost <- extract.samples(m3.2)\nplot(d2$height, d2$weight)\n#for (j in 1:20)\n#  abline(a=post$a[j], b=post$b[j])\n\nheight_seq <- seq(130, 190, len = 20)\nW_postpred <- sim(m3.2, data = list(H=height_seq))\nW_PI <- apply(W_postpred, 2, PI)\nlines(height_seq, W_PI[1,])\nlines(height_seq, W_PI[2,])"
  },
  {
    "objectID": "notes/notes-03.html#todo",
    "href": "notes/notes-03.html#todo",
    "title": "Lecture 03 - Geocentric Models",
    "section": "TODO:",
    "text": "TODO:\n\nfigure out plot erroring"
  }
]