[
  {
    "objectID": "CODE_OF_CONDUCT.html",
    "href": "CODE_OF_CONDUCT.html",
    "title": "Statistical Rethinking colearning 2023 - Bella",
    "section": "",
    "text": "We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.\nWe pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.\n\n\n\nExamples of behavior that contributes to a positive environment for our community include:\n\nDemonstrating empathy and kindness toward other people\nBeing respectful of differing opinions, viewpoints, and experiences\nGiving and gracefully accepting constructive feedback\nAccepting responsibility and apologizing to those affected by our mistakes, and learning from the experience\nFocusing on what is best not just for us as individuals, but for the overall community\n\nExamples of unacceptable behavior include:\n\nThe use of sexualized language or imagery, and sexual attention or advances of any kind\nTrolling, insulting or derogatory comments, and personal or political attacks\nPublic or private harassment\nPublishing others’ private information, such as a physical or email address, without their explicit permission\nOther conduct which could reasonably be considered inappropriate in a professional setting\n\n\n\n\nCommunity leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.\nCommunity leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.\n\n\n\nThis Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.\n\n\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be reported to Alec (robit.alec@gmail.com), or Isabella. All complaints will be reviewed and investigated promptly and fairly.\nAll community leaders are obligated to respect the privacy and security of the reporter of any incident.\n\n\n\nCommunity leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:\n\n\nCommunity Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.\nConsequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.\n\n\n\nCommunity Impact: A violation through a single incident or series of actions.\nConsequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.\n\n\n\nCommunity Impact: A serious violation of community standards, including sustained inappropriate behavior.\nConsequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.\n\n\n\nCommunity Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals.\nConsequence: A permanent ban from any sort of public interaction within the community.\n\n\n\n\nThis Code of Conduct is adapted from the Contributor Covenant, version 2.0, available at https://www.contributor-covenant.org/version/2/0/code_of_conduct.html.\nCommunity Impact Guidelines were inspired by Mozilla’s code of conduct enforcement ladder.\nFor answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistical Rethinking colearning 2023 - Bella",
    "section": "",
    "text": "NOTE: website format taken from Alec Robitaille\n\nSchedule\n\nLectures\nHomework\n\nParticipant notes and homework solutions\nResources\nInstallation\nCode of Conduct\n\n\n\nSecond round of Statistical Rethinking colearning, this time with 2023 lectures and homework.\nThe first round of Statistical Rethinking colearning (2022) is available here.\n\n\n\n\n\n\n\nMeeting date\nReading\nLectures\n\n\n\n\n26 January\nChapters 1, 2 and 3\n[1] <Science Before Statistics> <Slides>  [2] <Garden of Forking Data> <Slides>\n\n\n09 February\nChapter 4\n[3] <Geocentric Models> <Slides>  [4] <Categories and Curves> <Slides>\n\n\n23 February\nChapters 5 and 6\n[5] <Elemental Confounds> <Slides>  [6] <Good and Bad Controls> <Slides>\n\n\n09 March\nChapters 7 and 8\n[7] <Overfitting> <Slides>  [8] <MCMC> <Slides>\n\n\n23 March\nChapters 9, 10 and 11\n[9] <Modeling Events> <Slides>  [10] <Counts and Confounds> <Slides>\n\n\n06 April\nChapters 11 and 12\n[11] <Ordered Categories> <Slides>  [12] <Multilevel Models> <Slides>\n\n\n20 April\nChapter 13\n[13] <Multilevel Adventures> <Slides>  [14] <Correlated Features> <Slides>\n\n\n04 May\nChapter 14\n[15] <Social Networks> <Slides>  [16] <Gaussian Processes> <Slides>\n\n\n18 May\nChapter 15\n[17] Measurement Error  [18] Missing Data\n\n\n01 June\nChapters 16 and 17\n[19] Beyond GLMs: State-space Models, ODEs  [20] Horoscopes\n\n\n\n\n\n\n\n\n\nMeeting date\nHomework\nSolutions\n\n\n\n\n2 February\nHomework 1\nSolutions\n\n\n16 February\nHomework 2\nSolutions\n\n\n2 March\nHomework 3\nSolutions\n\n\n16 March\nHomework 4\nSolutions\n\n\n30 March\nHomework 5\nSolutions\n\n\n13 April\nHomework 6\nSolutions\n\n\n27 April\nHomework 7\nSolutions\n\n\n11 May\nHomework 8\nSolutions\n\n\n25 May\nHomework 9\nSolutions\n\n\n01 June\nHomework 10\nSolutions\n\n\n\n\n\n\n\n\nAlec\nBella (this repo)\n\n\n\n\n\nAdditional material using other packages or languages\n\nOriginal R: https://github.com/rmcelreath/rethinking/\nR + Tidyverse + ggplot2 + brms: https://bookdown.org/content/4857/\nPython and PyMC3: Python/PyMC3\nJulia and Turing: https://github.com/StatisticalRethinkingJulia and https://github.com/StatisticalRethinkingJulia/TuringModels.jl\n\nSee Richard’s comments about these here: https://github.com/rmcelreath/stat_rethinking_2023#coding\n2022 colearning:\n\nLectures: https://github.com/rmcelreath/stat_rethinking_2022#calendar--topical-outline\nHomework: https://github.com/rmcelreath/stat_rethinking_2022/tree/main/homework\n\nAlso, Alec’s notes and solutions of the 2019 material: https://github.com/robitalec/statistical-rethinking and https://www.statistical-rethinking.robitalec.ca/\n\n\n\nPackage specific install directions. We’ll update these as we go!\nRethinking\n\nrethinking\n\nStan\n\ncmdstanr\nRStan\nbrms\n\nTargets\n\ntargets\nstantargets\n\nV8, needed for the dagitty package\n\nV8\n\n\n\n\nPlease note that this project is released with a Code of Conduct. By participating in this project you agree to abide by its terms."
  },
  {
    "objectID": "homework/homework-04.html",
    "href": "homework/homework-04.html",
    "title": "Homework - Week 04",
    "section": "",
    "text": "Question 1: Revisit the marriage, age, and happiness collider bias example from Chapter 6. Run models m6.9 and m6.10 again (pages 178–179). Compare these two models using both PSIS and WAIC. Which model is expected to make better predictions, according to these criteria, and which model yields the correct causal inference?\n\nd <- sim_happiness(seed = 1977, N_years = 1000)\nd2 <- d[d$age > 17, ]\nd2$A <- (d2$age - 18) / (65 - 18)\nd2$mid <- d2$married + 1\n\n# model with marital status and age\ntar_load(h04_q1a)\nmMA <- h04_q1a\n\nmMA %>% \n  gather_draws(b_Intercept, b_mid, b_A, sigma) %>% \n  median_qi() %>%\n  ggplot(aes(y = .variable, x = .value, xmin = .lower, xmax = .upper)) + \n  geom_pointinterval() + \n  theme_classic() +\n  labs(y = \"\", x = \"mean value +/- 95% CIs\")\n\n\n\n# model with age\ntar_load(h04_q1b)\nmA <- h04_q1b\n\nmA %>% \n  gather_draws(b_Intercept, b_A, sigma) %>% \n  median_qi() %>%\n  ggplot(aes(y = .variable, x = .value, xmin = .lower, xmax = .upper)) + \n  geom_pointinterval() + \n  theme_classic() +\n  labs(y = \"\", x = \"mean value +/- 95% CIs\")\n\n\n\n# compare with PSIS & WAIC\ncMA <- add_criterion(mMA, c(\"loo\", \"waic\"))\ncA <- add_criterion(mA, c(\"loo\", \"waic\"))\nloo_compare(cMA, cA, criterion = \"loo\")\n\n    elpd_diff se_diff\ncMA    0.0       0.0 \ncA  -147.7      15.4 \n\nloo_compare(cMA, cA, criterion = \"waic\")\n\n    elpd_diff se_diff\ncMA    0.0       0.0 \ncA  -147.7      15.4 \n\n\nThe correct predictive model includes both marriage status and age. However, marriage status is a collider and the correct causal model includes only age. Thus, the correct causal model and the best predictive model are different in this case (because predictive models favour colliders).\n\n\nQuestion 2: Reconsider the urban fox analysis from last week’s homework. On the basis of PSIS and WAIC scores, which combination of variables best predicts body weight (W, weight)? What causal interpretation can you assign each coefficient (parameter) from the best scoring model?\n\n\nQuestion 3: Build a predictive model of the relationship show on the cover of the book, the relationship between the timing of cherry blossoms and March temperature in the same year. The data are found in data(cherry_blossoms). Consider at least two different models (functional relationships) to predict doy with temp. You could for example compare a linear model with a quadratic model. Compare them with PSIS or WAIC.\n\n\nSuppose March temperatures reach 9 degrees by the year 2050. What does your best model predict for the predictive distribution of the day-in-year that the cherry trees will blossom?\n\n\nQuestion 4: The data in data(Dinosaurs) are body mass estimates at different estimated ages for six different dinosaur species. See ?Dinosaurs for more details. Choose one or more of these species (at least one, but as many as you like) and model its growth. To be precise: Make a predictive model of body mass using age as a predictor. Consider two or more model types for the function relating age to body mass and score each using PSIS and WAIC.\n\n\nWhich model do you think is best, on predictive grounds? On scientific grounds? If your answers to these questions differ, why?\n\n\nThis is a challenging exercise, because the data are so scarce. But it is also a realistic example, because people publish Nature papers with even less data. So do your best, and I look forward to seeing your growth curves."
  },
  {
    "objectID": "homework/homework-03.html",
    "href": "homework/homework-03.html",
    "title": "Homework - Week 03",
    "section": "",
    "text": "Question 1: The first two problems are based on the same data. The data in data(foxes) are 116 foxes from 30 different urban groups in England. These fox groups are like street gangs. Group size (groupsize) varies from 2 to 8 individuals. Each group maintains its own (almost exclusive) urban territory. Some territories are larger than others. The area variable encodes this information. Some territories also have more avgfood than others. And food influences the weight of each fox. Assume this DAG:\n\n\n\n\n\n\n\nwhere F is avgfood, G is groupsize, A is area, and W is weight. Use the backdoor criterion and estimate the total causa influence of A on F. What effect would increasing the territory have on the amount of food inside of it?\nTotal causal effect of A on F is just F ~ A.\n\n# get data\ndata(foxes)\nd <- foxes %>% \n  select(c(avgfood, area)) %>%\n  mutate(avgfood = standardize(avgfood), \n         area = standardize(area))\n\n# get a list of priors present in the model\ndefault_prior <- get_prior(avgfood ~ area, data = d, family = gaussian())\n\n# load model\ntar_load(h03_q1)\nm1<- h03_q1\n\n# model formula\nm1$formula\n\navgfood ~ area \n\n# priors\nm1$prior\n\n          prior     class coef group resp dpar nlpar lb ub       source\n normal(0, 0.5)         b                                          user\n normal(0, 0.5)         b area                             (vectorized)\n normal(0, 0.5) Intercept                                          user\n exponential(1)     sigma                             0            user\n\n# check diagnostics\nplot(m1)\n\n\n\n# get a summary of posterior distribution\nposterior_summary(m1)\n\n                Estimate  Est.Error         Q2.5        Q97.5\nb_Intercept   0.00180027 0.04424605  -0.08699153   0.08779513\nb_area        0.87508681 0.04532430   0.78381207   0.96385458\nsigma         0.47642964 0.03193351   0.41960619   0.54381478\nlprior       -2.46759468 0.15979059  -2.79675921  -2.15820677\nlp__        -81.07429865 1.29904575 -84.42465769 -79.65751830\n\n# plot the effect\nm1 %>% \n  gather_draws(b_Intercept, b_area, sigma) %>% \n  median_qi() %>%\n  ggplot(aes(y = .variable, x = .value, xmin = .lower, xmax = .upper)) + \n  geom_pointinterval() + \n  theme_classic() +\n  labs(y = \"\", x = \"mean value +/- 95% CIs\")\n\n\n\n\nIt looks like the effect of territory size on food availability is a strong positive relationship (remember to interpret this as standardized variables - so effect is stronger than it seems).\n\n\nQuestion 2: Infer the total causal effect of adding food, F, to a territory on the weight W of foxes. Can you calculate the causal effect by simulating an intervention on food?\n\n# get data\ndata(foxes)\nd <- foxes %>% \n  select(c(avgfood, weight)) %>%\n  mutate(avgfood = standardize(avgfood), \n         weight = standardize(weight))\n\n# get a list of priors present in the model\ndefault_prior <- get_prior(weight ~ avgfood, data = d, family = gaussian())\n\n# load model\ntar_load(h03_q2)\nm2<- h03_q2\n\n# model formula\nm2$formula\n\nweight ~ avgfood \n\n# priors\nm2$prior\n\n          prior     class    coef group resp dpar nlpar lb ub       source\n normal(0, 0.5)         b                                             user\n normal(0, 0.5)         b avgfood                             (vectorized)\n normal(0, 0.5) Intercept                                             user\n exponential(1)     sigma                                0            user\n\n# check diagnostics\nplot(m2)\n\n\n\n# get a summary of posterior distribution\nposterior_summary(m2)\n\n                 Estimate  Est.Error         Q2.5        Q97.5\nb_Intercept  6.834984e-04 0.09026490   -0.1761782    0.1706892\nb_avgfood   -2.153602e-02 0.09414535   -0.2049801    0.1656456\nsigma        1.010654e+00 0.06928777    0.8865267    1.1533827\nlprior      -1.497178e+00 0.08130099   -1.6705898   -1.3623127\nlp__        -1.670499e+02 1.25890208 -170.3416937 -165.6393421\n\n# simulate an intervention on food\n# make empty df \nsimdf <- tibble(weight = double(), \n                    avgfood = double(), \n                    condition = character())\n# sample W from data\nn <- 1e3\nsampW <- sample(d$weight, size = n, replace = T)\n# make df where avgfood = 0\nsimdf_nofood <- simdf %>%\n  add_row(weight = sampW, avgfood = rep(0, n), condition = rep('no food', n))\n\n# make df where avgfood = 1\nsimdf_food <- simdf %>%\n  add_row(weight = sampW, avgfood = rep(1, n), condition = rep('food', n))\n\n# simulate model output when F = 0\nepred_nofood <- simdf_nofood %>% \n  add_epred_draws(m2)\n\n# simulate model output when F = 1\nepred_food <- simdf_food %>%\n  add_epred_draws(m2)\n\n# visualize contrast of intervention on food\nepred <- as.data.frame(rbind(epred_nofood, epred_food))\n\nggplot(data = epred, aes(x = .epred, y = condition)) +\n  stat_pointinterval(.width = c(.66, .95)) + \n  theme_classic() + \n  labs(y = \"\", x = \"mean value +/- 66% and 95% CIs\")\n\n\n\n\nWe see that the total effect of food on weight is very slightly negative. Adding food would result in a small decrease in average weight. However, there is almost equal plausibility that it could be a positive effect as well and the effect is very small.\n\n\nQuestion 3: Infer the direct causal effect of adding food F to a territory on the weight W of foxes. In light of your estimates from this problem and the previous one, what do you think is going on with these foxes?\nTo assess the direct causal effect of adding food on fox weight, we need to stratify by group size. This means simply adding group size to the model.\n\n# get data\ndata(foxes)\nd <- foxes %>% \n  select(c(avgfood, weight, groupsize)) %>%\n  mutate(avgfood = standardize(avgfood), \n         weight = standardize(weight),\n         groupsize = standardize(groupsize))\n\n# get a list of priors present in the model\ndefault_prior <- get_prior(weight ~ avgfood + groupsize, data = d, family = gaussian())\n\n# load model\ntar_load(h03_q3)\nm3<- h03_q3\n\n# model formula\nm3$formula\n\nweight ~ avgfood + groupsize \n\n# priors\nm3$prior\n\n          prior     class      coef group resp dpar nlpar lb ub       source\n normal(0, 0.5)         b                                               user\n normal(0, 0.5)         b   avgfood                             (vectorized)\n normal(0, 0.5)         b groupsize                             (vectorized)\n normal(0, 0.5) Intercept                                               user\n exponential(1)     sigma                                  0            user\n\n# check diagnostics\nplot(m3)\n\n\n\n# get a summary of posterior distribution\nposterior_summary(m3)\n\n                 Estimate  Est.Error          Q2.5        Q97.5\nb_Intercept -1.290813e-03 0.08931639   -0.17236064    0.1769677\nb_avgfood    4.729671e-01 0.18599490    0.09371001    0.8265943\nb_groupsize -5.707044e-01 0.18681170   -0.93313280   -0.1968301\nsigma        9.646206e-01 0.06488804    0.84658064    1.1065583\nlprior      -2.895702e+00 0.75889644   -4.70678173   -1.7899017\nlp__        -1.629948e+02 1.44232128 -166.61982834 -161.1684968\n\n# plot\nm3 %>% \n  gather_draws(b_Intercept, b_avgfood, b_groupsize, sigma) %>% \n  median_qi() %>%\n  ggplot(aes(y = .variable, x = .value, xmin = .lower, xmax = .upper)) + \n  geom_pointinterval() + \n  theme_classic() +\n  labs(y = \"\", x = \"mean value +/- 95% CIs\")\n\n\n\n\nOnce we stratify for group size, we see that the effect of food availability on weight is stronger, and positive. We also see that group size has a moderate negative effect on weight. The effect sizes are almost the same size, but in opposite directions. When calculating the total effect, the effect of group size may be masking the effect of food on fox weight.\nBiological explanation: increases in territory size increase food availability (Q1) - but increases in food don’t affect fox weight. Is this effect being cancelled out by subsequent increases in group size? Can test causal model\n\n# load model\ntar_load(h03_q3b)\nm3b <- h03_q3b\n\n# model formula\nm3b$formula\n\ngroupsize ~ avgfood \n\n# check diagnostics\nplot(m3b)\n\n\n\n# get a summary of posterior distribution\nposterior_summary(m3b)\n\n                 Estimate  Est.Error         Q2.5        Q97.5\nb_Intercept -1.491733e-04 0.04127697  -0.08049194   0.07950636\nb_avgfood    8.959363e-01 0.04117644   0.81502122   0.97602255\nsigma        4.386855e-01 0.02912356   0.38752297   0.50095616\nlprior      -2.502469e+00 0.14990467  -2.82347812  -2.22383426\nlp__        -7.181795e+01 1.24592339 -75.01707671 -70.41421943\n\n# plot\nm3b %>% \n  gather_draws(b_Intercept, b_avgfood, sigma) %>% \n  median_qi() %>%\n  ggplot(aes(y = .variable, x = .value, xmin = .lower, xmax = .upper)) + \n  geom_pointinterval() + \n  theme_classic() +\n  labs(y = \"\", x = \"mean value +/- 95% CIs\")\n\n\n\n\nAverage food has a strong positive effect on group size. More food = larger groups = negative/minimal effect on weight.\n\n\nQuestion 4: Suppose there is an unovserved confound that influences F and G, like this:\n\n\n\n\n\n\n\nAssuming the DAG above is correct, again estimate both the total and direct causal effects of F on W. What impact does the unobserved confound have?\nTotal effect: Impossible because to close backdoor path you need to stratify on G - then you are estimating the direct effect.\nDirect effect: Stratify by G."
  },
  {
    "objectID": "homework/homework-02.html",
    "href": "homework/homework-02.html",
    "title": "Homework - Week 02",
    "section": "",
    "text": "Question 1: From the Howell1 dataset, consider only the people younger than 13 years old. Estimate the causal association between age and weight. Assume that age influences weight through two paths. First, age influences height, and height influences weight. Second, age directly influences weight through age-related changes in muscle growth and body proportions.\nDraw the DAG that represents these causal relationships. And then write a generative simulation that takes age as an input and simulates height and weight, obeying the relationships in the DAG.\nDAG:\n\n# DAG\ndag <- dagify(\n    weight ~ age + height,\n    height ~ age,\n    exposure = 'age',\n    outcome = 'weight'\n)\n\nggdag(dag) +\n    theme_dag()\n\n\n\n\nGenerative Simulation:\n\n# A = age, H = height, W = weight\nbAH <- 5 # as age increases, height increases by a lot\nbHW <- 0.5 # as height increases, weight proportionally increases cm/kg\nbAW <- 0.1 # as age increases, there is a small direct increase in weight\n\nsim_AW <- function(A, bAH, bHW, bAW){\n  N <- length(A)\n  H <- rnorm(N, bAH*A, 2)\n  W <- rnorm(N, bHW*H + bAW*A, 2)\n  data.frame(A, H, W)\n}\n\n# randomly generated individuals \nA <- runif(n=20, min=1, max=13)\n\ntest_sim <- sim_AW(A, bAH, bHW, bAW)\n\n# view relationship\nggplot(data = test_sim, aes(x=A, y=W)) + \n  geom_point() + \n  theme_classic()\n\n\n\n\nHave some weird things happening here (decreases in weight with age) but overall a positive-ish relationship between age-weight that is partially dependent on height.\n\n\nQuestion 2: Use a linear regression to estimate the total causal effect of each year of growth on weight.\nTotal causal effect means that we do not account for the effect of height. It is a linear regression of weight ~ age (where age is a proxy for year of growth).\n\ndata(Howell1)\nd <- Howell1 %>% \n  filter(age <= 13) %>%\n  select(c(weight, age))\n\n# get a list of priors present in the model\ndefault_prior <- get_prior(weight ~ age, data = d, family = gaussian())\n\n# get model\ntar_load(h02_mAW)\nmAW <- h02_mAW\n\n# model formula\nmAW$formula\n\nweight ~ age \n\n# priors\nmAW$prior\n\n          prior     class coef group resp dpar nlpar lb ub       source\n uniform(0, 10)         b                                          user\n uniform(0, 10)         b  age                             (vectorized)\n   normal(5, 1) Intercept                                          user\n exponential(1)     sigma                             0            user\n\n# check diagnostics\nplot(mAW)\n\n\n\n# get a summary of posterior distribution\nposterior_summary(mAW)\n\n               Estimate  Est.Error        Q2.5       Q97.5\nb_Intercept    7.036794 0.36425544    6.327332    7.753923\nb_age          1.335759 0.05138795    1.235176    1.442238\nsigma          2.596536 0.15143607    2.320393    2.903958\nlprior       -55.469858 2.07865655  -59.474311  -51.354272\nlp__        -427.576650 1.31101667 -431.020299 -426.121602\n\n\n\n\nQuestion 3: Now suppose the causal association between age and weight might be different for boys and girls. Use a single linear regression, with a categorical variable for sex, to estimate the total causal effect of age on weight separately for boys and girls. How do girls and boys differ? Provide one or more posterior contrasts as a summary.\nUpdated DAG:\n\n# DAG\ndag <- dagify(\n    weight ~ height + age + sex,\n    height ~ age + sex,\n    exposure = 'age',\n    outcome = 'weight'\n)\n\nggdag(dag) +\n    theme_dag()\n\n\n\n\nModel, stratified by sex:\n\ndata(Howell1)\nd2 <- Howell1 %>% \n  filter(age <= 13) %>%\n  mutate(sex = as.factor(case_when(male == 0 ~ 1, \n                         male == 1 ~ 2))) %>%\n  select(c(weight, age, sex))\n\n# get a list of priors present in the model\ndefault_prior2 <- get_prior(weight ~ age + sex, data = d2, family = gaussian())\n\n# load model\ntar_load(h02_mAWS)\nmAWS <- h02_mAWS\n\n# model formula\nmAWS$formula\n\nweight ~ age + sex \n\n# priors\nmAWS$prior\n\n          prior     class coef group resp dpar nlpar lb ub       source\n uniform(0, 10)         b                                          user\n uniform(0, 10)         b  age                             (vectorized)\n uniform(0, 10)         b sex2                             (vectorized)\n   normal(5, 1) Intercept                                          user\n exponential(1)     sigma                             0            user\n\n# check diagnostics\nplot(mAWS)\n\n\n\n# get a summary of posterior distribution\nposterior_summary(mAWS)\n\n               Estimate  Est.Error         Q2.5       Q97.5\nb_Intercept    6.421874 0.39163836    5.6340126    7.180696\nb_age          1.328347 0.04822519    1.2345120    1.425671\nb_sex2         1.406962 0.39901129    0.6507742    2.223799\nsigma          2.499072 0.13997303    2.2404017    2.788606\nlprior       -57.983164 1.96677528  -61.9331028  -54.196221\nlp__        -424.165388 1.42733997 -427.9576483 -422.453753\n\n# get prediction options \npred_vals <- data.frame(age = seq(0, 13, 0.5))\n\n# simulate from model where sex = 1 and sex = 2 \npost_pred_female <- predicted_draws(mAWS, pred_vals %>% mutate(sex = 1)) \npost_pred_male <- predicted_draws(mAWS, pred_vals %>% mutate(sex = 2))\n\n# prediction table \npost_differences <- data.frame(\n  predict_sex_female = post_pred_female$.prediction,\n  predict_sex_male = post_pred_male$.prediction,\n  age = post_pred_female$age\n) %>%\n  mutate(diff = predict_sex_male - predict_sex_female)\n\n\n\nggplot(post_differences, aes(age, diff)) +\n    stat_lineribbon() +\n  scale_fill_manual(values = met.brewer(\"VanGogh3\", 3)) +\n    labs(\n      x = 'age',\n        y = 'predicted difference in weight (males-females)') +\n  theme_classic()\n\n\n\n\n\n\nQuestion 4: The data in data(Oxboys) (rethinking package) are growth records for 26 boys measured over 9 periods. I want you to model their growth. Specifically, model the increments in growth from one period (Occasion in the data table) to the next. Each increment is simply the difference between height in one occasion and height in the previous occasion. Since none of these boys shrunk during the study, all of the growth increments are greater than zero. Estimate the posterior distribution of these increments. Constrain the distribution so it is always positive-it should not be possible for the model to think that boys can shrink from year to year. Finally compute the posterior distribution of the total growth over all 9 occasions."
  },
  {
    "objectID": "homework/homework-01.html",
    "href": "homework/homework-01.html",
    "title": "Homework - Week 01",
    "section": "",
    "text": "Question 1: Suppose the globe tossing data (Lecture 2, Chapter 2) had turned out to be 4 water and 11 land. Construct the posterior distribution.\n\n# define how many tosses were W & L\nw <- 4\nl <- 11\n\n# define p, every 0.01 between 0,1 \np <- seq(0, 1, by = 0.01)\n\n# function to compute posterior by every p\ncompute_posterior <- function(W, L, poss){\n  ways <- sapply(poss, function(q) (q*4)^W * ((1-q)*4)^L)\n  post <- ways/sum(ways)\n  data.frame(poss, ways, post = round(post,3))\n}\n\n# compute posterior for our data\nposterior <- compute_posterior(w, l, p)\n\n# visualize posterior \nggplot(data = posterior) + \n  geom_line(aes(poss, post)) + \n  theme_classic() + \n  xlab(\"proportion water\") + \n  ylab(\"probability\")\n\n\n\n\nNOTE: Bonus answer in solutions is somewhat key to understanding next answer. This specific problem, where you have a continuous value bounded by 0,1 with an infinite number of p, follows a beta distribution. Because the beta distribution is defined, you can instead just use that mathematical equation to come to the same conclusion as above.\n\nggplot() +\n  stat_function(fun = function(x) dbeta(x, 4+1, 11+1), color = \"black\",\n                size = 1) +\n  theme_classic() + \n  xlab(\"p\") + \n  ylab(\"\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\nQuestion 2: Using the posterior distribution from 1, compute the posterior predictive distribution for the next 5 tosses of the same globe. I recommend you use the sampling method.\n\n# sample our posterior (beta distribution with 4 W, 11 L)\nn <- 1e4\npost_samples <- rbeta(n, 4+1, 11+1)\n\n# sim globe function\nsim_globe <- function(p, N){\n  \n  sample(c(\"W\", \"L\"), size = N, prob=c(p, 1-p), replace = T)\n\n}\n\n# simulate posterior predictive distribution for next 5 tosses of the globe\n# how many of the 5 tosses will = W? Tested 1e4 times\npred_post <- sapply(post_samples, function(p) sum(sim_globe(p, 5)==\"W\"))\n\n# create table that counts number of incidences of each option (i.e., how many times will 0 W appear, 1 W, 2 W, 3 W, 4 W, 5 W?)\ntab_post <- table(pred_post)\npred_post_df <- as.data.frame(tab_post)\n\nggplot(pred_post_df, aes(as.factor(pred_post), Freq)) + \n  geom_col() + \n  theme_classic() + \n  xlab(\"number of W\") + \n  ylab(\"count\")\n\n\n\n\nNOTE: Can also use rbinom() function instead of sim_globe() since there are only two possible outcomes.\n\n\nQuestion 3: Use the posterior predictive distribution from 2 to calculate the probability of 3 or more water samples in the next 5 tosses.\nWe know that \\[ p = \\frac{ways}{sum(ways)} \\] Therefore, count the incidences of 3, 4, 5 in posterior predictive distribution from 2 and divide by total number of ways (1e4)\n\np_threeplus <- (sum(pred_post_df[which(pred_post_df[,1]==3 | pred_post_df[,1]==4 | \n                                         pred_post_df[,1]==5),2]))/n\n\nprint(paste0(\"p = \", p_threeplus))\n\n[1] \"p = 0.1843\"\n\n\n\n\nQuestion 4: Suppose you observe W = 5 water points, but you forgot to write down how many times the globe was tossed, so you don’t know the number of land points L. Assume that p = 0.7 and compute the posterior distribution of the number of tosses N. Hint: Use the binomial distribution.\nBefore, we were solving for p using \\[ p^W(1-p)^L \\] Which was a beta distribution because p was a continuous variable bound between 0,1. Now we want to solve for N, with W and p. We have a success/fail dataset, with a probability of success, therefore we use the binomial distribution. We need to calculate the probability of 0-5 N’s resulting in 5 successes (W).\n\n# N & L = unknown\nW <- 5\np <- 0.7\nN_max <- 25\nlst <- seq(W, N_max, by = 1)\n\n# calculate binomial distribution for each N between lower bound (W) and upper bound (N_max) with p = 0.7\nx <- sapply(lst, function(x) dbinom(W, x, p))\n\nggplot(data.frame(data = x, N = W:N_max), aes(x = N, y = x)) +\n  geom_col() +\n  labs(y = \"Probability\", x = \"N\") + \n  theme_classic()"
  },
  {
    "objectID": "homework/homework-listing.html",
    "href": "homework/homework-listing.html",
    "title": "Homework",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\nDate\n\n\nAuthor\n\n\n\n\n\n\nHomework - Week 01\n\n\n\n\nIsabella C. Richmond\n\n\n\n\nHomework - Week 02\n\n\n\n\nIsabella C. Richmond\n\n\n\n\nHomework - Week 03\n\n\n\n\nIsabella C. Richmond\n\n\n\n\nHomework - Week 04\n\n\n\n\nIsabella C. Richmond\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "notes/notes-04.html",
    "href": "notes/notes-04.html",
    "title": "Lecture 04 - Categories & Curves",
    "section": "",
    "text": "Rose: learning what GAMs are , thinking of age/time as a cause\nThorn: I haven’t taken time to fix the plot code from the lectures"
  },
  {
    "objectID": "notes/notes-04.html#drawing-inferences",
    "href": "notes/notes-04.html#drawing-inferences",
    "title": "Lecture 04 - Categories & Curves",
    "section": "Drawing Inferences",
    "text": "Drawing Inferences\n\nlinear model can accomodate anything, thus we need to think carefully about our scientific model\ngenerative model + multiple estimands = multiple estimators\nquite often the estimate we want is not in a summary table because it depends on multiple unknowns, so we often need to do post-processing"
  },
  {
    "objectID": "notes/notes-04.html#categories",
    "href": "notes/notes-04.html#categories",
    "title": "Lecture 04 - Categories & Curves",
    "section": "Categories",
    "text": "Categories\n\ncategories are discrete and non-linear\ndiscrete, unordered types\nwe want to stratify by category, to fit a separate line for each"
  },
  {
    "objectID": "notes/notes-04.html#howell-data",
    "href": "notes/notes-04.html#howell-data",
    "title": "Lecture 04 - Categories & Curves",
    "section": "Howell Data",
    "text": "Howell Data\n\n\nLoading required package: rstan\n\n\nLoading required package: StanHeaders\n\n\nLoading required package: ggplot2\n\n\nrstan (Version 2.21.8, GitRev: 2e1f913d3ca3)\n\n\nFor execution on a local, multicore CPU with excess RAM we recommend calling\noptions(mc.cores = parallel::detectCores()).\nTo avoid recompilation of unchanged Stan programs, we recommend calling\nrstan_options(auto_write = TRUE)\n\n\nLoading required package: cmdstanr\n\n\nThis is cmdstanr version 0.5.3\n\n\n- CmdStanR documentation and vignettes: mc-stan.org/cmdstanr\n\n\n- CmdStan path: /home/icrichmond/.cmdstan/cmdstan-2.31.0\n\n\n- CmdStan version: 2.31.0\n\n\nLoading required package: parallel\n\n\nrethinking (Version 2.21)\n\n\n\nAttaching package: 'rethinking'\n\n\nThe following object is masked from 'package:rstan':\n\n    stan\n\n\nThe following object is masked from 'package:stats':\n\n    rstudent\n\n\n\nhow are height, weight, and sex causally related?\n\n\nd <- dagitty(\"dag {\n                H -> W\n                S -> W\n                S -> H\n             }\")\n\ndrawdag(d)\n\n\n\n\n\nheight influences weight\nsex influences weight and height\nweight is influenced by height and sex\ninfluence of sex is both direct and indirect on weight\n\\(H = f_{H}(S)\\)\n\\(W = f_{W}(H,S)\\)\nUnobserved causes are ignorable unless they are shared between variables (common cause) = confound\n\n\nsim_HW <- function(S, b, a){\n  N <- length(S)\n  H <- ifelse(S==1, 150, 160) + rnorm(N, 0, 5)\n  W <- a[S] + b[S]*H + rnorm(N, 0, 5)\n  data.frame(S, H, W)\n}\n\nS <- rbern(100)+1\ndat <- sim_HW(S, b=c(0.5, 0.6), a=c(0,0))\nhead(dat)\n\n  S        H         W\n1 2 159.5952  96.97320\n2 1 145.4824  70.05005\n3 1 144.3575  82.79749\n4 2 162.6258 100.43484\n5 1 146.7997  76.89398\n6 2 164.3618 105.25174\n\n\n\nscientific questions:\n\ncausal effect of H on W?\ncausal effect of S on W?\ndirect causal effect of S on W?\n\nwe need to stratify by S to answer qs 2 and 3\ncoding categorical variables\n\nindicator variables (0/1)\nindex variables (1,2,3,4)\nindex variables are generally preferable\n\nindex variables\n\noften we want to give each index the same prior\n\ntotal causal effect of sex\n\n\nS <- rep(1, 100)\nsimF <- sim_HW(S, b=c(0.5, 0.6), a=c(0,0))\n\nS <- rep(2, 100)\nsimM <- sim_HW(S, b=c(0.5, 0.6), a=c(0,0))\n\n# effect of sex (male-female)\nmean(simM$W - simF$W)\n\n[1] 21.52404\n\n\n\nestimating model and synthetic example\n\n\nS <- rbern(100)+1\ndat <- sim_HW(S, b = c(0.5,0.6), a = c(0,0))\n\nm_SW <- quap(alist(\n  W ~ dnorm(mu, sigma),\n  mu <- a[S],\n  a[S] ~ dnorm(60, 10),\n  sigma ~ dunif(0, 10)\n), data = dat)\n\nprecis(m_SW, depth = 2)\n\n           mean        sd      5.5%     94.5%\na[1]  75.815504 0.8057976 74.527684 77.103324\na[2]  96.224152 0.7748631 94.985771 97.462533\nsigma  5.600023 0.3963665  4.966553  6.233493\n\n\n\nanalyze the real sample\n\n\nd <- Howell1\nd <- d[ d$age >= 18,]\n\ndat <- list(\n  W = d$weight,\n  S = d$male + 1\n)\n\nm_SW <- quap(alist(\n  W ~ dnorm(mu, sigma), \n  mu <- a[S],\n  a[S] ~ dnorm(60, 10),\n  sigma ~ dunif(0, 10)\n), data = dat)\n\n\nposterior means and predictions\n\n\n# posterior mean W\npost <- extract.samples(m_SW)\ndens(post$a[,1])\n\n\n\n#dens(post$a[,2], add = T)\n\n# posterior W predictions\nW1 <- rnorm(1000, post$a[,1], post$sigma)\nW2 <- rnorm(1000, post$a[,2], post$sigma)\ndens(W1)\n\n\n\n#dens(W2, add = T)\n\n# contrast\nW_contrast <- W2 - W1\ndens(W_contrast)\n\n\n\n# proportion above zero\nsum(W_contrast > 0)/1000\n\n[1] 0.806\n\nsum(W_contrast < 0)/1000\n\n[1] 0.194\n\n\n\ncontrasting\n\nneed to compute the difference between the categories\nit is not legitimate to compare overlap in distributions\nwe must compute contrast distribution\n\n\n\nmu_contrast <- post$a[,2] - post$a[,1]\ndens(mu_contrast)\n\n\n\n\n\n“controlling” for the indirect effect of sex through height\n\n\nS <- rbern(100)+1\n# slopes are the same so there is no effect of height on weight through slope but men are on average 10 kg heavier (intercept 10)\nset.seed(12)\ndat <- sim_HW(S, b = c(0.5, 0.5), a = c(0, 10))\n\n\n\\(W_{i} \\sim Normal(\\mu _{i}, \\sigma)\\)\n\\(\\mu _{i} = \\alpha _{S[i]} + \\beta _{S[i]}(H_{i} - \\bar H)\\)\n\nThis equation centers the height \\((H_{i} - \\bar H)\\)\nCentering H means that alpha represents the average weight of a person with average height\n\\(\\alpha = [\\alpha _{1}, \\alpha _{2}]\\) , \\(\\beta = [\\beta _{1}, \\beta _{2}]\\)\n\nanalyze the sample\n\n\nd <- Howell1\nd <- d[d$age >= 18, ]\ndat <- list(W = d$weight, H = d$height, Hbar = mean(d$height), S = d$male + 1)\n\nm_SHW <- quap(alist(\n  W ~ dnorm(mu, sigma),\n  mu <- a[S] + b[S]*(H-Hbar),\n  a[S] ~ dnorm(60, 10),\n  b[S] ~ dunif(0, 1),\n  sigma ~ dunif(0, 10)\n), data = dat)\n\n\nwe need to compute the difference of expected weight at each height to get the actual estimate that we are looking for (for the direct effect of sex on weight)\n\n\nxseq <- seq(from=130, to=190, len=50)\n\nmuF <- link(m_SHW, data = list(S=rep(1,50), H=xseq, Hbar=mean(d$height)))\n#lines(xseq, apply(muF, 2, mean))\n\nmuM <- link(m_SHW, data = list(S=rep(2,50), H=xseq, Hbar = mean(d$height)))\n#lines(xseq, apply(muM, 2, mean))\n\nmu_contrast <- muF - muM\n\n#plot(NULL, xlim=range(xseq))\n#for (p in c(0.5, 0.6, 0.7, 0.8, 0.9, 0.99))\n#  shade(apply(mu_contrast, 2, PI, prob = p), xseq)\n#abline(h=0)\n\n\nnearly all of the causal effects of S acts through H\n\nwhen we block H, we see very little effect of sex on weight"
  },
  {
    "objectID": "notes/notes-04.html#categorical-variables",
    "href": "notes/notes-04.html#categorical-variables",
    "title": "Lecture 04 - Categories & Curves",
    "section": "Categorical Variables",
    "text": "Categorical Variables\n\ncommon, easy to use with index coding\nuse samples to compute relevant contrasts\nalways summarize (mean, interval) as the last step\nwe want mean difference and not difference of means"
  },
  {
    "objectID": "notes/notes-04.html#curves-from-lines",
    "href": "notes/notes-04.html#curves-from-lines",
    "title": "Lecture 04 - Categories & Curves",
    "section": "Curves from Lines",
    "text": "Curves from Lines\n\nmany non linear relationships\nlinear models can easily fit curves: 2 strategies\n\npolynomials (bad)\nsplines and GAMs (less bad)\n\npolynomial models\n\nstill linear because its an additive function of the parameters\ncreate strange symmetries and explosive uncertainty\nno local smoothing, only global smoothing\ndo not use\n\nsplines\n\ngreat for locally inferred function\nadd together a bunch of locally trained terms\ncan add as many locally trained terms as you want\neach term has a weight and slope and only affects its own region"
  },
  {
    "objectID": "notes/notes-04.html#full-luxury-bayes",
    "href": "notes/notes-04.html#full-luxury-bayes",
    "title": "Lecture 04 - Categories & Curves",
    "section": "Full Luxury Bayes",
    "text": "Full Luxury Bayes\n\ninstead of two models for two estimands, use one model for full causal model\ncan simulate interventions with this approach\n\n\nm_SHW_full <- quap(alist(\n  \n  # weight\n  W ~ dnorm(mu, sigma),\n  mu <- a[S] + b[S]*(H-Hbar),\n  a[S] ~ dnorm(60, 10),\n  b[S] ~ dunif(0, 1),\n  sigma ~ dunif(0, 10),\n  \n  # height\n  H ~ dnorm(nu, tau),\n  nu <- h[S],\n  h[S] ~ dnorm(160, 10),\n  tau ~ dunif(0, 10)\n  \n), data = dat)\n\n\npost <- extract.samples(m_SHW_full)\nHbar <- dat$Hbar\nn <- 1e4\n\nwith(post, {\n  \n  H_S1 <- rnorm(n, h[,1], tau)\n  W_S1 <- rnorm(n, a[,2] + b[,1]*(H_S2-Hbar), sigma)\n  \n  W_do_S <<- W_S2 - W_S1\n  \n})\n\n# automate\nHWsim <- sim(m_SHW_full, data = list(S=c(1,2)), vars = c(\"H\", \"W\"))\nW_do_S_auto <- HWsim$W[,2] - HWsim$W[,1]\n\n\nyou can either do one statistical model for each estimand OR one simulation for each estimand (full luxury Bayes)"
  },
  {
    "objectID": "notes/notes-04.html#todo",
    "href": "notes/notes-04.html#todo",
    "title": "Lecture 04 - Categories & Curves",
    "section": "TODO:",
    "text": "TODO:\n\nfix weird plot erroring"
  },
  {
    "objectID": "notes/notes-02.html",
    "href": "notes/notes-02.html",
    "title": "Lecture 02 - The Garden of Forking Data",
    "section": "",
    "text": "Rose: remembering that posterior distributions are relative probabilities is a great foundation to move forward on\nThorn: equations are harddd + baseR + installing rstan + misclassification ahh"
  },
  {
    "objectID": "notes/notes-02.html#globe-example",
    "href": "notes/notes-02.html#globe-example",
    "title": "Lecture 02 - The Garden of Forking Data",
    "section": "Globe Example",
    "text": "Globe Example\nestimand: proportion of the globe covered in water - do people know what an estimand is?"
  },
  {
    "objectID": "notes/notes-02.html#generative-model-globe",
    "href": "notes/notes-02.html#generative-model-globe",
    "title": "Lecture 02 - The Garden of Forking Data",
    "section": "Generative Model (globe)",
    "text": "Generative Model (globe)\n\nstart with how the variables influence each other, i.e., causal model\n\n\nlibrary(dagitty)\nlibrary(rethinking)\n\nLoading required package: rstan\n\n\nLoading required package: StanHeaders\n\n\nLoading required package: ggplot2\n\n\nrstan (Version 2.21.8, GitRev: 2e1f913d3ca3)\n\n\nFor execution on a local, multicore CPU with excess RAM we recommend calling\noptions(mc.cores = parallel::detectCores()).\nTo avoid recompilation of unchanged Stan programs, we recommend calling\nrstan_options(auto_write = TRUE)\n\n\nLoading required package: cmdstanr\n\n\nThis is cmdstanr version 0.5.3\n\n\n- CmdStanR documentation and vignettes: mc-stan.org/cmdstanr\n\n\n- CmdStan path: /home/icrichmond/.cmdstan/cmdstan-2.31.0\n\n\n- CmdStan version: 2.31.0\n\n\nLoading required package: parallel\n\n\nrethinking (Version 2.21)\n\n\n\nAttaching package: 'rethinking'\n\n\nThe following object is masked from 'package:rstan':\n\n    stan\n\n\nThe following object is masked from 'package:stats':\n\n    rstudent\n\nd <- dagitty(\"dag {\n                p -> W\n                p -> L\n                N -> W \n                N -> L }\")\n\ndrawdag(d)\n\n\n\n\n\\[\nW,L = f(p,N)\n\\]\n\nW and L are functions of p and N"
  },
  {
    "objectID": "notes/notes-02.html#bayesian-data-analysis",
    "href": "notes/notes-02.html#bayesian-data-analysis",
    "title": "Lecture 02 - The Garden of Forking Data",
    "section": "Bayesian Data Analysis",
    "text": "Bayesian Data Analysis\nFor each possible explanation of the sample,\n\ncount all the ways the sample could happen\nexplanations with more ways to produce the sample are more plausible"
  },
  {
    "objectID": "notes/notes-02.html#garden-of-forking-data",
    "href": "notes/notes-02.html#garden-of-forking-data",
    "title": "Lecture 02 - The Garden of Forking Data",
    "section": "Garden of Forking Data",
    "text": "Garden of Forking Data\n\nrelies on samples being independent\nrelative differences between probabilities are dependent on sample size (differences will be smaller with smaller sample sizes because there is less evidence)\nnormalizing to probability allows for interpretability and easier math\ncollection of probabilities is a posterior distribution\n\nADD CODE (minute 35):\n\nsample <- c(\"W\", \"L\", \"W\", \"W\", \"W\", \"L\", \"W\", \"L\", \"W\")\n\nW <- sum(sample==\"W\")\nL <- sum(sample==\"L\")\np <- c(0, 0.25, 0.5, 0.75, 1)\nways <- sapply(p, function(q) (q*4)^W * ((1-q)*4)^L)\nprob <- ways/sum(ways)\ncbind(p, ways, prob)\n\n        p ways       prob\n[1,] 0.00    0 0.00000000\n[2,] 0.25   27 0.02129338\n[3,] 0.50  512 0.40378549\n[4,] 0.75  729 0.57492114\n[5,] 1.00    0 0.00000000"
  },
  {
    "objectID": "notes/notes-02.html#testing",
    "href": "notes/notes-02.html#testing",
    "title": "Lecture 02 - The Garden of Forking Data",
    "section": "Testing",
    "text": "Testing\n\nhave to test\ntest using extreme values where you intuitively know the right answer\n\n\nsim_globe <- function(p = 0.7, N = 9){\n  \n  sample(c(\"W\", \"L\"), size = N, prob=c(p, 1-p), replace = T)\n}\n\nsim_globe()\n\n[1] \"W\" \"W\" \"W\" \"W\" \"W\" \"W\" \"W\" \"W\" \"W\"\n\nsim_globe(p=1, N=11) \n\n [1] \"W\" \"W\" \"W\" \"W\" \"W\" \"W\" \"W\" \"W\" \"W\" \"W\" \"W\"\n\nsum(sim_globe(p=0.5, N=1e4) == \"W\")/1e4\n\n[1] 0.4963\n\n\nFunction:\n\nlibrary(crayon)\n\n\nAttaching package: 'crayon'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    %+%\n\nmake_bar <- function(q,size=20) {\n    n <- round(q*size)\n    s1 <- concat( rep(\"#\",n) )\n    s2 <- concat( rep(\" \",size-n) )\n    concat(s1,s2)\n}\n\ncompute_posterior <- function(the_sample, poss = c(0,0.25,0.5,0.75,1)){\n  W <- sum(the_sample==\"W\")\n  L <- sum(the_sample==\"L\")\n  ways <- sapply(poss, function(q) (q*4)^W * ((1-q)*4)^L)\n  post <- ways/sum(ways)\n  bars <- sapply(post, function(q) make_bar(q))\n  data.frame(poss, ways, post = round(post,3), bars)\n}\n\ncompute_posterior(sim_globe())\n\n  poss ways  post                 bars\n1 0.00    0 0.000                     \n2 0.25   81 0.097 ##                  \n3 0.50  512 0.612 ############        \n4 0.75  243 0.291 ######              \n5 1.00    0 0.000"
  },
  {
    "objectID": "notes/notes-02.html#real-number-sampling",
    "href": "notes/notes-02.html#real-number-sampling",
    "title": "Lecture 02 - The Garden of Forking Data",
    "section": "Real Number Sampling",
    "text": "Real Number Sampling\n\nmore possibilities = less probability in each option/outcome\n\nprobability is spread out across many options\n\nnormalizing to probability allows our equation to calculate infinite number of “sides”/outcomes\n\n\\[\np^W(1-p)^L\n\\]\np = probability\ndensity = probability when we are assessing infinite number of possibilities\n\nshape of the posterior embodies sample size\n\nno min sample size -> just more uncertain posterior\nposterior distribution embodies sample size\n\nno point estimates! estimate is entire posterior distribution\n\ncan use summary points from post dist for communication purposes\n\nintervals are merely indicators of the shape of the posterior distribution\n\nno “true interval” i.e., 95% CI doesn’t exist\ninterval is just distribution lower/upper bounds"
  },
  {
    "objectID": "notes/notes-02.html#analyze-sample-summarize",
    "href": "notes/notes-02.html#analyze-sample-summarize",
    "title": "Lecture 02 - The Garden of Forking Data",
    "section": "Analyze Sample + Summarize",
    "text": "Analyze Sample + Summarize\n\npost_samples <- rbeta(1e3, 6+1, 3+1)\n\ndens(post_samples, lwd = 4, col = 2, xlab = \"prop water\", adj = 0.1)\n\ncurve(dbeta(x, 6+1, 3+1), add = T, lty = 2, lwd = 3)\n\n\n\n\n\nposterior prediction = “what would we bet?”\n\nhow many W’s do we expect to see in the next 10 tosses\n\nfor each sample of post dist, we can create a predictive distribution, then posterior predictive\n\nincorporates uncertainty from posterior distribution\n\n\n\npost_samples <- rbeta(1e4, 6+1, 3+1)\n\npred_post <- sapply(post_samples, function(p) \nsum(sim_globe(p, 10)==\"W\"))\n\ntab_post <- table(pred_post)\n\n#for (i in 0:10) lines(c(i,i),c(0,tab_post[i+1]), lwd = 4, col = 4)"
  },
  {
    "objectID": "notes/notes-02.html#misclassification-bonus-round",
    "href": "notes/notes-02.html#misclassification-bonus-round",
    "title": "Lecture 02 - The Garden of Forking Data",
    "section": "Misclassification (Bonus Round)",
    "text": "Misclassification (Bonus Round)\n\nW* is misclassified due to sampling error and measurement process\n\ntrue W is unknown\n\n\n\nlibrary(dagitty)\nlibrary(rethinking)\n\nd <- dagitty(\"dag {\n                p -> W\n                N -> W\n                W -> Wm\n                M -> Wm\n             }\")\n\ndrawdag(d)\n\n\n\n\n\nincorporate measurement error with x (error rate of 10%)\n\n\nsim_globe2 <- function(p = 0.7, N = 9, x = 0.1){\n  \n  true_sample <- sample(c(\"W\", \"L\"), size = N, prob = c(p, 1-p), replace = T)\n  \n  obs_sample <- ifelse(runif(N) < x,\n                       ifelse(true_sample == \"W\", \"L\", \"W\"),\n                       true_sample)\n  \n  return(obs_sample)\n  \n}\n\n\nhow do you know the error rate?\ndon’t understand mechanism behind incorporating x but understand why x needs to be incorporated + consequences of not"
  },
  {
    "objectID": "notes/notes-02.html#todo",
    "href": "notes/notes-02.html#todo",
    "title": "Lecture 02 - The Garden of Forking Data",
    "section": "TODO",
    "text": "TODO\n\nDAG coords\nRead misclassification in 3rd ed."
  },
  {
    "objectID": "notes/notes-01.html",
    "href": "notes/notes-01.html",
    "title": "Lecture 01 - The Golem of Prague",
    "section": "",
    "text": "Rose: excited for fewer examples + sensitivity analyses\nBONUS: I LOVE DAGS\nThorn: i am having a hard time imagining/extending my thinking re: causal imputation + unique null hypotheses (focus on causation)"
  },
  {
    "objectID": "notes/notes-01.html#third-edition",
    "href": "notes/notes-01.html#third-edition",
    "title": "Lecture 01 - The Golem of Prague",
    "section": "Third Edition",
    "text": "Third Edition\n\npeach boxes instead of blue boxes"
  },
  {
    "objectID": "notes/notes-01.html#causal-inference",
    "href": "notes/notes-01.html#causal-inference",
    "title": "Lecture 01 - The Golem of Prague",
    "section": "Causal Inference",
    "text": "Causal Inference\n\nstatistical models require scientific (causal) models\ncorrelation is a very limited measure of association\n\nassociation can occur without correlation\n\ncausal prediction = prediction of the consequences of an intervention (implications of changing one variable on another variable)\n\nknowing the cause of an action allows you to create predictions\n\ncausal imputation = knowing the cause of an action allows you to reconstruct possible outcomes (i.e., what if I had done something else?)"
  },
  {
    "objectID": "notes/notes-01.html#dags",
    "href": "notes/notes-01.html#dags",
    "title": "Lecture 01 - The Golem of Prague",
    "section": "DAGs",
    "text": "DAGs\n\nabstract causal models: includes names of variables and their causal relationships\ntells you the consequences of an intervention\nfacilitates you asking scientific questions"
  },
  {
    "objectID": "notes/notes-01.html#golems",
    "href": "notes/notes-01.html#golems",
    "title": "Lecture 01 - The Golem of Prague",
    "section": "Golems",
    "text": "Golems\n\nstatistical models = golems\noften not possible to design and outline a null hypothesis that is meaningful to reject in observational science\n\nwhat is a null ecological community?\n\nthink of good example/explanation for no null ecology/previous two slides\n\nread textbook section\nfocus on third example?\ntakeaway is that null hypothesis does not give you cause/process behind outcome\n\nwhat is your null? is it unique?"
  },
  {
    "objectID": "notes/notes-01.html#todo",
    "href": "notes/notes-01.html#todo",
    "title": "Lecture 01 - The Golem of Prague",
    "section": "TODO",
    "text": "TODO\n\nread textbook section on null ecology\nagenda\ndiscuss code options"
  },
  {
    "objectID": "notes/notes-08.html#rose-thorn",
    "href": "notes/notes-08.html#rose-thorn",
    "title": "Lecture 08 - Markov chain Monte Carlo",
    "section": "Rose / Thorn",
    "text": "Rose / Thorn\nRose: skateboarding metaphor\nThorn: not understanding parameters in model specification"
  },
  {
    "objectID": "notes/notes-08.html#modelling-approaches",
    "href": "notes/notes-08.html#modelling-approaches",
    "title": "Lecture 08 - Markov chain Monte Carlo",
    "section": "Modelling Approaches",
    "text": "Modelling Approaches\n\nquadratic approximation makes strong assumptions about what the model looks like - approximately Gaussian\nMCMC is intensive but with less assumptions and more flexible"
  },
  {
    "objectID": "notes/notes-08.html#markov-chain-monte-carlo",
    "href": "notes/notes-08.html#markov-chain-monte-carlo",
    "title": "Lecture 08 - Markov chain Monte Carlo",
    "section": "Markov chain Monte Carlo",
    "text": "Markov chain Monte Carlo\n\nchain: sequence of draws from distribution\nMarkov chain: history doesn’t matter, just where you are now\nMonte Carlo: random simulation"
  },
  {
    "objectID": "notes/notes-08.html#hamiltonian-monte-carlo",
    "href": "notes/notes-08.html#hamiltonian-monte-carlo",
    "title": "Lecture 08 - Markov chain Monte Carlo",
    "section": "Hamiltonian Monte Carlo",
    "text": "Hamiltonian Monte Carlo\n\nuses random pathways that follow the distribution of the variables\n\n\ndag <- dagify(\n  S ~ Q + J + X,\n  Q ~ X,\n  J ~ Z,\n  outcome = 'S',\n  latent = 'Q',\n    coords = list(x = c(Q = 0, X = 0, S = 1, J = 1, Z = 2),\n                y = c(X = 0, J = 0, Z = 0, Q = 1, S = 1))\n)\n\nggdag(dag) + theme_dag()\n\n\n\n\n\nEstimand: association between wine quality and wine origin. Stratify by judge for efficiency.\n\n\ndata(Wines2012)\nd <- Wines2012\n\ndat <- list(\n  S = standardize(d$score),\n  J = as.numeric(d$judge), \n  W = as.numeric(d$wine),\n  X = ifelse(d$wine.amer == 1,1,2),\n  Z = ifelse(d$judge.amer == 1,1,2)\n)\n\nmQ <- ulam(alist(\n  S ~ dnorm(mu, sigma), \n  mu <- Q[W],\n  Q[W] ~ dnorm(0, 1), \n  sigma ~ dexp(1)),\n  data = dat, chains = 4, cores = 4)\n\nprecis(mQ, 2)\n\n\ntrace plots: visualization of the Markov chain\nwant more than one chain in order to check convergence\n\nconvergence: each chain explores the right distribution and every chain explores the same distribution\n\nR-hat is chain convergence diagnostic\n\nvariance ratio\nif chains do converge, beginning of the chain and end of chain should be exploring the same place and therefore the chain is stationary\nas total variance (among chains) approaches average variance within chains, R-hat approaches 1\ndoes not guarantee convergence but gives an idea that the chains are working when ~ 1\n\nn_eff = effective samples\n\napproximation of how long the chain would be if each sample was completely independent of the one before it\nwhen samples are autocorrelated, you have fewer effective samples\ntypically n_eff is smaller than number of samples you actually took\n\n\n\nmQO <- ulam(alist(\n  S ~ dnorm(mu, sigma), \n  mu <- Q[W] + O[X],\n  Q[W] ~ dnorm(0, 1),\n  O[X] ~ dnorm(0, 1),\n  sigma ~ dexp(1)),\n  data = dat, chains = 4, cores = 4)\n\nplot(precis(mQO, 2))\n\nmQOJ <- ulam(alist(\n  S ~ dnorm(mu, sigma), \n  mu <- (Q[W] + O[X] - H[J])*D[J],\n  Q[W] ~ dnorm(0, 1),\n  O[X] ~ dnorm(0, 1),\n  H[J] ~ dnorm(0, 1),\n  D[J] ~ dexp(1),\n  sigma ~ dexp(1)),\n  data = dat, chains = 4, cores = 4)\n\nplot(precis(mQOJ, 2))\n\n\noften if there is an issue, it is with your model\n\nloop back to basic scientific questions and assumptions\ndivergent transitions are one of the signs of this - “rejected proposal”"
  },
  {
    "objectID": "notes/notes-listing.html",
    "href": "notes/notes-listing.html",
    "title": "Notes",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\nDate\n\n\nAuthor\n\n\n\n\n\n\nLecture 01 - The Golem of Prague\n\n\nJan 23, 2023\n\n\nIsabella C. Richmond\n\n\n\n\nLecture 02 - The Garden of Forking Data\n\n\nJan 23, 2023\n\n\nIsabella C. Richmond\n\n\n\n\nLecture 03 - Geocentric Models\n\n\nFeb 8, 2023\n\n\nIsabella C. Richmond\n\n\n\n\nLecture 04 - Categories & Curves\n\n\nFeb 8, 2023\n\n\nIsabella C. Richmond\n\n\n\n\nLecture 05 - Elemental Confounds\n\n\nFeb 22, 2023\n\n\nIsabella C. Richmond\n\n\n\n\nLecture 06 - Good & Bad Controls\n\n\nFeb 22, 2023\n\n\nIsabella C. Richmond\n\n\n\n\nLecture 07 - Fitting Over & Under\n\n\nMar 8, 2023\n\n\nIsabella C. Richmond\n\n\n\n\nLecture 08 - Markov chain Monte Carlo\n\n\nMar 8, 2023\n\n\nIsabella C. Richmond\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "notes/notes-03.html",
    "href": "notes/notes-03.html",
    "title": "Lecture 03 - Geocentric Models",
    "section": "",
    "text": "Rose: flow reminder! Retrograde explanation\nThorn: installing rethinking on windows"
  },
  {
    "objectID": "notes/notes-03.html#linear-regressions",
    "href": "notes/notes-03.html#linear-regressions",
    "title": "Lecture 03 - Geocentric Models",
    "section": "Linear Regressions",
    "text": "Linear Regressions\n\nessentially a geocentric model - overly simplified\nseparate from a causal model\nassociations are from the causal model, not the statistical model"
  },
  {
    "objectID": "notes/notes-03.html#gaussian-distributions",
    "href": "notes/notes-03.html#gaussian-distributions",
    "title": "Lecture 03 - Geocentric Models",
    "section": "Gaussian Distributions",
    "text": "Gaussian Distributions\n\nthere are many more ways to end up in the center than to end up on the periphery\nwhy the Gaussian distribution spontaneously occurs in natural systems\ngenerative: if we add fluctuations, we tend towards normal distribution (lots of summed fluctuations in nature)\ninferential: estimating mean and variance, normal distribution is best one to use because it is least informative (no other information present)\nnormal distribution is just a tool for estimating mean/variance because it has widest distribution\n\ndata doesn’t have to be normal to be able to leverage the tool to estimate mean/variance"
  },
  {
    "objectID": "notes/notes-03.html#workflow",
    "href": "notes/notes-03.html#workflow",
    "title": "Lecture 03 - Geocentric Models",
    "section": "Workflow",
    "text": "Workflow\n\nState a clear question\nSketch causal assumptions\nDefine generative model from sketch\nUse generative model to build estimator\nProfit\n\n\nlibrary(rethinking)\nlibrary(dagitty)\ndata(Howell1)"
  },
  {
    "objectID": "notes/notes-03.html#describing-models",
    "href": "notes/notes-03.html#describing-models",
    "title": "Lecture 03 - Geocentric Models",
    "section": "Describing Models",
    "text": "Describing Models\n\\[\nW_{i}= \\beta H_{i} + U_{i}\n\\]\n\\[\nU_{i} \\sim Normal(0, \\sigma)\n\\]\n\\[\nH_{i} \\sim Uniform(130, 170)\n\\]\n\n= is deterministic\n~ is distributional"
  },
  {
    "objectID": "notes/notes-03.html#howell-example",
    "href": "notes/notes-03.html#howell-example",
    "title": "Lecture 03 - Geocentric Models",
    "section": "Howell Example",
    "text": "Howell Example\n\nQuestion: Describe association between adult weight and height\n\n\nd2 <- Howell1[Howell1$age>=18,]\n\n\nScientific model: weight is some function of height and unobserved influences\n\n\nd <- dagitty(\"dag {\n                H -> W\n                U -> W\n             }\")\n\ndrawdag(d)\n\n\n\n\n\\[\nW = f(H, U)\n\\]\n\nGenerative/Statistical model\n\n\nTwo options: dynamic (complex and ongoing) and static\n\nStatic model allows us to imagine changes at specific times and still use a Gaussian distribution\n\nFor adults, weight is a proportion of height plus the influence of unobserved causes\n\n\\[\nW = \\beta H+U\n\\]\n\nsim_weight <- function(H, b, sd){\n  U <- rnorm(length(H), 0, sd)\n  W <- b*H + U\n  return(W)\n}\n\nH <- runif(200, min = 130, max = 170)\nW <- sim_weight(H, b = 0.5, sd = 5)\nplot(W ~ H, col = 2, lwd = 3)\n\n\n\n\n\nwe want to estimate how the average weight changes with height\n\\[\nE(W_{i}|H_{i}) = \\alpha + \\beta H_{i}\n\\]\n\\(E(W_{i}|H_{i})\\) = average weight conditional on height\n\\(\\alpha\\) = intercept (when height is 0, what is weight? Scientifically should be zero but putting it in model to make sure our model is scientifically sound)\n\\(\\beta\\) = slope\nPosterior distribution:\n\n\\[\nPr(\\alpha,\\beta,\\sigma| H_{i}, W_{i}) = \\frac{Pr(W_{i}|H_{i}, \\alpha,\\beta,\\sigma)Pr(\\alpha,\\beta,\\sigma)}{Z}\n\\]\n\\(W_{i} \\sim Normal(\\mu _{i}), \\sigma)\\)\n\\(\\mu _{i} = \\alpha + \\beta H_{i}\\)\nalpha, sigma, beta are unknown so we need a posterior distribution for them (and they are dependent on the data)\n\\(Pr(\\alpha,\\beta,\\sigma| H_{i}, W_{i})\\) = posterior probability of specific line\n\\(Pr(W_{i}|H_{i}, \\alpha,\\beta,\\sigma)\\) = probability of each weight dependent on height value, alpha, beta, sigma\n\\(Pr(\\alpha,\\beta,\\sigma)\\) = prior\n\n\nStatistical Model\n\nQuadratic approximation\n\n\\(W_{i} \\sim Normal(\\mu _{i}), \\sigma)\\)\n\\(\\mu _{i} = \\alpha + \\beta H_{i}\\)\n\\(\\alpha \\sim Normal(0,10)\\)\n\\(\\beta \\sim Uniform(0,1)\\)\n\\(\\sigma \\sim Uniform(0, 10)\\)\n\n\n\nm3.1 <- quap(alist(\n  W ~ dnorm(mu, sigma),\n  mu <- a + b*H,\n  a ~ dnorm(0, 10),\n  b ~ dunif(0, 1),\n  sigma ~ dunif(0, 10)\n), data = list(W=W, H=H))\n\n\nPriors\n\nwe want to constrain to scientifically plausible values\njustify with information outside the data - like the rest of model\n\n\n\nn <- 1e3\na <- rnorm(n, 0, 10)\nb <- runif(n, 0, 1)\nplot(NULL, xlim = c(130, 170), ylim = c(50, 90))\n\n\n\n#for (j in 1:50) abline(a=a[j], b=b[j])\n\n\nValidate model\n\n\ntest statistical model with simulated observations from scientific model - need to test if your model is working\nuse many values and make sure your model responds appropriately\n\n\n# model summary\nprecis(m3.1)\n\n           mean         sd       5.5%      94.5%\na     6.9643897 4.60083091 -0.3886267 14.3174061\nb     0.4572968 0.03063682  0.4083333  0.5062604\nsigma 5.3923172 0.27386079  4.9546347  5.8299996\n\n\n\nAnalyze data\n\n\ndat <- list(W = d2$weight, H = d2$height)\nm3.2 <- quap(alist(\n  W ~ dnorm(mu, sigma),\n  mu <- a + b*H,\n  a ~ dnorm(0, 10), \n  b ~ dunif(0, 1),\n  sigma ~ dunif(0, 10)\n), data = dat)\n\nprecis(m3.2)\n\n             mean         sd        5.5%       94.5%\na     -43.3632932 4.17139201 -50.0299833 -36.6966031\nb       0.5716544 0.02695485   0.5285754   0.6147335\nsigma   4.2532387 0.16178910   3.9946684   4.5118089\n\n\n\nparameters are not independent of one another, they cannot be independently interpreted\n\nuse posterior predictions and describe/interpret those by sampling the posterior distribution\n\n\n\npost <- extract.samples(m3.2)\nplot(d2$height, d2$weight)\n#for (j in 1:20)\n#  abline(a=post$a[j], b=post$b[j])\n\nheight_seq <- seq(130, 190, len = 20)\nW_postpred <- sim(m3.2, data = list(H=height_seq))\nW_PI <- apply(W_postpred, 2, PI)\nlines(height_seq, W_PI[1,])\nlines(height_seq, W_PI[2,])"
  },
  {
    "objectID": "notes/notes-03.html#todo",
    "href": "notes/notes-03.html#todo",
    "title": "Lecture 03 - Geocentric Models",
    "section": "TODO:",
    "text": "TODO:\n\nfigure out plot erroring"
  },
  {
    "objectID": "notes/notes-06.html#rose-thorn",
    "href": "notes/notes-06.html#rose-thorn",
    "title": "Lecture 06 - Good & Bad Controls",
    "section": "Rose / Thorn",
    "text": "Rose / Thorn\nRose: identifying and working through bad controls\nThorn:"
  },
  {
    "objectID": "notes/notes-06.html#randomization",
    "href": "notes/notes-06.html#randomization",
    "title": "Lecture 06 - Good & Bad Controls",
    "section": "Randomization",
    "text": "Randomization\n\nrandomizing the treatment can remove the confound (only available for experiments)"
  },
  {
    "objectID": "notes/notes-06.html#causal-thinking",
    "href": "notes/notes-06.html#causal-thinking",
    "title": "Lecture 06 - Good & Bad Controls",
    "section": "Causal Thinking",
    "text": "Causal Thinking\n\nin an experiment, we cut causes of the treatment -> we randomize\nsimulating intervention mimics randomization\ndo(X) means intervene on X\nexample: simple confound\n\n\ndag <- dagify(\n\n    X ~ U,\n\n    Y ~ U + X\n\n)\n\nggdag(dag) +\n\n    theme_dag()\n\n\n\n\n\nstratifying by U removes causal relationship and allows to test effect of X -> Y\nmarginalize or average over control variables\n\nthe coefficient is not usually satisfactory, need to marginalize\n\n\n\ndag <- dagify(\n\n    Baboons ~ Cheetahs,\n\n    Gazelle ~ Baboons + Cheetahs\n\n)\n\nggdag(dag) +\n\n    theme_dag()\n\n\n\n\n\npopulations of each of these species influences the other\n\nwhen cheetahs are present, baboons are scared and do not influence gazelle population\nwhen cheetahs are absent, baboons eat and regulate gazelle population\nto assess causal effect of baboons, need to average over cheetah population"
  },
  {
    "objectID": "notes/notes-06.html#do-calculus",
    "href": "notes/notes-06.html#do-calculus",
    "title": "Lecture 06 - Good & Bad Controls",
    "section": "Do-Calculus",
    "text": "Do-Calculus\n\nallows us to determine if it is possible to answer our question using a DAG\nbackdoor criterion\n\nshortcut to apply do-calculus to use your eyes\nrule to find a set of variables to stratify by to yield estimate of our estimand\n\n\nidentify all paths connecting treatment (X) to outcome (Y)\npaths with arrows entering X are backdoor paths (non-causal paths)\nfind adjustment set that closes/blocks all backdoor paths\n\n\n\n\n\n# simulate confounded Y\nN <- 200\nb_XY <- 0\nb_UY <- -1\nb_UZ <- -1\nb_ZX <- 1\n\nset.seed(10)\nU <- rbern(N)\nZ <- rnorm(N, b_UZ*U)\nX <- rnorm(N, b_ZX*Z)\nY <- rnorm(N, b_XY*X + b_UY*U)\nd <- list(Y=Y, X=X, Z=Z)\n\n# ignore U,Z \nm_YX <- quap(alist(\n  Y ~ dnorm(mu, sigma),\n  mu <- a + b_XY*X,\n  a ~ dnorm(0,1),\n  b_XY ~ dnorm(0, 1),\n  sigma ~ dexp(1)\n), data = d)\n\n# stratify by Z \nm_YXZ <- quap(alist(\n    Y ~ dnorm(mu, sigma),\n  mu <- a + b_XY*X + b_Z*Z,\n  a ~ dnorm(0,1),\n  c(b_XY, b_Z) ~ dnorm(0, 1),\n  sigma ~ dexp(1)\n), data = d)\n\npost <- extract.samples(m_YX)\npost2 <- extract.samples(m_YXZ)\ndens(post$b_XY)\n\n\n\n#dens(post2$b_XY, add = TRUE)\n\n\nany variable you add to a model as part of the adjustment set (ie to control for), its coefficients are usually not interpretable\nminimum adjustment set is not always the best set"
  },
  {
    "objectID": "notes/notes-06.html#good-bad-controls",
    "href": "notes/notes-06.html#good-bad-controls",
    "title": "Lecture 06 - Good & Bad Controls",
    "section": "Good & Bad Controls",
    "text": "Good & Bad Controls\n\ncontrol variable: variable introduced to an analysis so that a causal estimate is possible\n\ngood and bad controls\n\nvariables not being collinear is not a good reason for including/excluding variables\n\ncollinearity can arise from many statistical processes\n\npost-treatment variables are often risky controls\nif there is no backdoor path to variable of interest, you don’t need to control for it\n\n\n# sim confounding by post-treatment variable\n\nf <- function(n=100,bXZ=1,bZY=1) {\n    X <- rnorm(n)\n    u <- rnorm(n)\n    Z <- rnorm(n, bXZ*X + u)\n    Y <- rnorm(n, bZY*Z + u )\n    bX <- coef( lm(Y ~ X) )['X']\n    bXZ <- coef( lm(Y ~ X + Z) )['X']\n    return( c(bX,bXZ) )\n}\n\nsim <- mcreplicate( 1e4 , f(bZY=0), mc.cores = 1)\n\n[ 1000 / 10000 ]\n[ 2000 / 10000 ]\n[ 3000 / 10000 ]\n[ 4000 / 10000 ]\n[ 5000 / 10000 ]\n[ 6000 / 10000 ]\n[ 7000 / 10000 ]\n[ 8000 / 10000 ]\n[ 9000 / 10000 ]\n[ 10000 / 10000 ]\n\ndens( sim[1,] , lwd=3 , xlab=\"posterior mean\" , xlim=c(-1,0.8) , ylim=c(0,2.7)  )\n\n\n\n#dens( sim[2,] , lwd=3 , col=2 , add=TRUE )\n\n\ncase control bias (selection on outcome)\n\nvery bad to add descendents of your outcome to your model\nweakly stratifying by the outcome (e.g., stratifying by Z)\n\n\n\ndag <- dagify(\n    Y ~ X,\n    Z ~ Y\n)\n\nggdag(dag) +\n    theme_dag()\n\n\n\n\n\nf <- function(n=100,bXY=1,bYZ=1) {\n    X <- rnorm(n)\n    Y <- rnorm(n, bXY*X )\n    Z <- rnorm(n, bYZ*Y )\n    bX <- coef( lm(Y ~ X) )['X']\n    bXZ <- coef( lm(Y ~ X + Z) )['X']\n    return( c(bX,bXZ) )\n}\n\nsim <- mcreplicate( 1e4 , f(), mc.cores = 1 )\n\n[ 1000 / 10000 ]\n[ 2000 / 10000 ]\n[ 3000 / 10000 ]\n[ 4000 / 10000 ]\n[ 5000 / 10000 ]\n[ 6000 / 10000 ]\n[ 7000 / 10000 ]\n[ 8000 / 10000 ]\n[ 9000 / 10000 ]\n[ 10000 / 10000 ]\n\ndens( sim[1,] , lwd=3 , xlab=\"posterior mean\" , xlim=c(0,1.5) , ylim=c(0,5)  )\n\n\n\n#dens( sim[2,] , lwd=3 , col=2 , add=TRUE )\n\n\nprecision parasite\n\nno backdoors because Z is not connected to Y except through X\nnot good to stratify Z because you are explaining part of the effect of X with Z\n\n\n\ndag <- dagify(\n    Y ~ X,\n    X ~ Z\n)\n\nggdag(dag) +\n    theme_dag()\n\n\n\n\n\nf <- function(n=100,bZX=1,bXY=1) {\n    Z <- rnorm(n)\n    X <- rnorm(n, bZX*Z )\n    Y <- rnorm(n, bXY*X )\n    bX <- coef( lm(Y ~ X) )['X']\n    bXZ <- coef( lm(Y ~ X + Z) )['X']\n    return( c(bX,bXZ) )\n}\n\nsim <- mcreplicate( 1e4 , f(n=50), mc.cores = 1 )\n\n[ 1000 / 10000 ]\n[ 2000 / 10000 ]\n[ 3000 / 10000 ]\n[ 4000 / 10000 ]\n[ 5000 / 10000 ]\n[ 6000 / 10000 ]\n[ 7000 / 10000 ]\n[ 8000 / 10000 ]\n[ 9000 / 10000 ]\n[ 10000 / 10000 ]\n\ndens( sim[1,] , lwd=3 , xlab=\"posterior mean\" , xlim=c(0.5,1.5) , ylim=c(0,4.5)  )\ndens( sim[2,] , lwd=3 , col=2 , add=TRUE )\n\n\n\n\n\nbias amplification\n\nX and Y confounded by u\nadding Z biases your answer because it “double” activates the confound\n\n\n\ndag <- dagify(\n    Y ~ X + u,\n    X ~ Z + u\n)\n\nggdag(dag) +\n    theme_dag()\n\n\n\n\n\nf <- function(n=100,bZX=1,bXY=1) {\n    Z <- rnorm(n)\n    u <- rnorm(n)\n    X <- rnorm(n, bZX*Z + u )\n    Y <- rnorm(n, bXY*X + u )\n    bX <- coef( lm(Y ~ X) )['X']\n    bXZ <- coef( lm(Y ~ X + Z) )['X']\n    return( c(bX,bXZ) )\n}\n\nsim <- mcreplicate( 1e4 , f(bXY=0), mc.cores = 1)\n\n[ 1000 / 10000 ]\n[ 2000 / 10000 ]\n[ 3000 / 10000 ]\n[ 4000 / 10000 ]\n[ 5000 / 10000 ]\n[ 6000 / 10000 ]\n[ 7000 / 10000 ]\n[ 8000 / 10000 ]\n[ 9000 / 10000 ]\n[ 10000 / 10000 ]\n\ndens( sim[1,] , lwd=3 , xlab=\"posterior mean\" , xlim=c(-0.5,1) , ylim=c(0,5.5)  )\n\n\n\n#dens( sim[2,] , lwd=3 , col=2 , add=TRUE )\n\nabline_w <- function(...,col=1,lwd=1,dlwd=2) {\n    abline(...,col=\"white\",lwd=lwd+dlwd)\n    abline(...,col=col,lwd=lwd)\n}\n\nn <- 1000\nZ <- rbern(n)\nu <- rnorm(n)\nX <- rnorm(n, 7*Z + u )\nY <- rnorm(n, 0*X + u )\n\ncols <- c( col.alpha(2,0.5) , col.alpha(4,0.5) )\nplot( X , Y  , col=cols[Z+1] , lwd=2 )\n\n\n\n#abline_w( lm(Y~X) , lwd=3 )\n#\n#abline_w( lm(Y[Z==1]~X[Z==1]) , lwd=3 , col=4 )\n#\n#abline_w( lm(Y[Z==0]~X[Z==0]) , lwd=3 , col=2 )"
  },
  {
    "objectID": "notes/notes-06.html#summary",
    "href": "notes/notes-06.html#summary",
    "title": "Lecture 06 - Good & Bad Controls",
    "section": "Summary",
    "text": "Summary\n\nadding control variables can be worse than omitting\nthere are good controls - backdoor criterion\nmake assumptions explicit"
  },
  {
    "objectID": "notes/notes-06.html#bonus---table-2-fallacy",
    "href": "notes/notes-06.html#bonus---table-2-fallacy",
    "title": "Lecture 06 - Good & Bad Controls",
    "section": "Bonus - Table 2 Fallacy",
    "text": "Bonus - Table 2 Fallacy\n\nnot all coefficients are causal effects\nstatistical model designed to identify X -> Y will not also identify effects of control variables\n\\(Y_i \\sim Normal(\\mu_i, \\sigma)\\)\n\\(\\mu_i = \\alpha + \\beta_xX_i + \\beta_SS_i + \\beta_AA_i\\)\nthink through DAG for each control variable to see what the coefficient actually means\nno interpretation without causal representation"
  },
  {
    "objectID": "notes/notes-06.html#todo",
    "href": "notes/notes-06.html#todo",
    "title": "Lecture 06 - Good & Bad Controls",
    "section": "TODO",
    "text": "TODO\n\nread Table 2 Fallacy paper"
  },
  {
    "objectID": "notes/notes-07.html#rose-thorn",
    "href": "notes/notes-07.html#rose-thorn",
    "title": "Lecture 07 - Fitting Over & Under",
    "section": "Rose / Thorn",
    "text": "Rose / Thorn\nRose: not trying to be perfect, just better\nThorn: thinking through prediction vs causality - when do I want prediction?"
  },
  {
    "objectID": "notes/notes-07.html#problems-of-prediction",
    "href": "notes/notes-07.html#problems-of-prediction",
    "title": "Lecture 07 - Fitting Over & Under",
    "section": "Problems of Prediction",
    "text": "Problems of Prediction\n\nwhat function describes the data (fitting, compression)\nwhat functions explains these points (causal inference)\nwhat would happen if we changed the data (intervention)\nwhat is the next observation from the same process (prediction)\n\nprediction is the absence of intervention\nprediction does not require causal inference\n\nLeave-one-out cross-validation\n\n\n\ndrop one point\nfit line to remaining\npredict dropped point\nrepeat (1) with next point\nscore is error on dropped\n\n\ntask you use to assess the expected predictive accuracy of a statistical procedure\nscore in: fit to the sample / score out: fit to prediction\nLPPD (log posterior probability of observation) used for cross-validation because it includes the entire posterior\nmore flexible patterns generally perform better in sample and worse out of sample"
  },
  {
    "objectID": "notes/notes-07.html#cross-validation",
    "href": "notes/notes-07.html#cross-validation",
    "title": "Lecture 07 - Fitting Over & Under",
    "section": "Cross-Validation",
    "text": "Cross-Validation\n\nfor simple models, more parameters improves fit to sample BUT may reduce accuracy of predictions out of sample\naccurate models trade off flexibility with overfitting"
  },
  {
    "objectID": "notes/notes-07.html#regularization",
    "href": "notes/notes-07.html#regularization",
    "title": "Lecture 07 - Fitting Over & Under",
    "section": "Regularization",
    "text": "Regularization\n\noverfitting depends upon the priors\ndon’t be too excited about every point in the sample, because not every point in the sample is regular (not all points are representative)\nskeptical priors regularize models - have tighter variance that reduces flexibility\n\ndownweights improbable values\n\nskeptical priors improve model prediction - regularize so that models learn regular features and ignore irregular features\n\nthere is such a thing as too tight priors\n\nRegularizing priors -> for pure prediction uses, you can tune the prior using cross-validation\n\ncausal inference uses science to choose priors"
  },
  {
    "objectID": "notes/notes-07.html#prediction-penalty",
    "href": "notes/notes-07.html#prediction-penalty",
    "title": "Lecture 07 - Fitting Over & Under",
    "section": "Prediction Penalty",
    "text": "Prediction Penalty\n\nFor N points, cross-validation requires fitting N models\n\nfeasible for few data points but for many data points gets unwieldy\n\nImportance sampling (PSIS) and information criteria (WAIC) allow you to assess prediction penalty from one model posterior distribution (for predictive models)\nWAIC, PSIS, cross-validation (CV) measure overfitting\n\nregularization manages overfiting\n\nCausal inference is not addressed by measuring or addressing overfitting\n\nthese tools are addressing the performance of a predictive model, not a causal model\nshould not select causal models based on these values because they are not associated with causality"
  },
  {
    "objectID": "notes/notes-07.html#model-mis-selection",
    "href": "notes/notes-07.html#model-mis-selection",
    "title": "Lecture 07 - Fitting Over & Under",
    "section": "Model Mis-selection",
    "text": "Model Mis-selection\n\nDo not use predictive criteria (WAIC, PSIS, CV) to choose a causal estimate\nPredictive criteria prefer confounds and colliders\n\nimprove predictive accuracy"
  },
  {
    "objectID": "notes/notes-07.html#outliers-robust-regression",
    "href": "notes/notes-07.html#outliers-robust-regression",
    "title": "Lecture 07 - Fitting Over & Under",
    "section": "Outliers & Robust Regression",
    "text": "Outliers & Robust Regression\n\nsome points are more influential than others - ‘outliers’\noutliers are information - don’t necessarily want to remove them\n\nbut they often have high leverage/weight because they are “surprising”\ndropping outliers ignores the problem - predictions will still be bad\nmodel is wrong, not the data\n\ncan quantify the influence of each point on the posterior distribution using cross-validation\ncan also use a mixture model/robust regression to address outliers\ndivorce rate example\n\nMaine and Idaho are outliers in divorce/age relationship\nquantify influence of outliers using PSIS k statistic or WAIC penalty term\nunmodelled sources of variation cause outliers -> error distributions are not constant across the sample\n\nassuming that the dataset has multiple error distributions, with the same mean but different variations indicates that you are using a student t-test\nGaussian distribution has extremely thin tails - very skeptical\nstudent t distribution is much less skeptical, wider tails, much less influenced by outliers + more robust\n\n\n\n\ndata(WaffleDivorce)\nd <- WaffleDivorce\n\n# model\ndat <- list(\n    D = standardize(d$Divorce),\n    M = standardize(d$Marriage),\n    A = standardize(d$MedianAgeMarriage)\n)\n\nm5.3 <- quap(alist(\n  D ~ dnorm(mu, sigma), \n  mu <- a + bM*M + bA*A,\n  a ~ dnorm(0, 0.2),\n  bM ~ dnorm(0, 0.5), \n  bA ~ dnorm(0, 0.5),\n  sigma ~ dexp(1)\n), data = dat)\n\nm5.3t <- quap(alist(\n  D ~ dstudent(2, mu, sigma), \n  mu <- a + bM*M + bA*A,\n  a ~ dnorm(0, 0.2),\n  bM ~ dnorm(0, 0.5), \n  bA ~ dnorm(0, 0.5),\n  sigma ~ dexp(1)\n), data = dat)"
  },
  {
    "objectID": "notes/notes-07.html#robust-regressions",
    "href": "notes/notes-07.html#robust-regressions",
    "title": "Lecture 07 - Fitting Over & Under",
    "section": "Robust Regressions",
    "text": "Robust Regressions\n\nunobserved heterogeneity in sample -> mixture of Gaussian errors\n\nthicker tails means model is less surprised/more robust\n\nstudent-t regression can be a good default for undertheorized domains\n\nbecause Gaussian distribution is so skeptical"
  },
  {
    "objectID": "notes/notes-07.html#prediction",
    "href": "notes/notes-07.html#prediction",
    "title": "Lecture 07 - Fitting Over & Under",
    "section": "Prediction",
    "text": "Prediction\n\npossible to make very good predictions without knowing causes\noptimizing prediction does not reliably reveal causes"
  },
  {
    "objectID": "notes/notes-05.html",
    "href": "notes/notes-05.html",
    "title": "Lecture 05 - Elemental Confounds",
    "section": "",
    "text": "Rose: so helpful and relevant\nThorn: is all my science wrong"
  },
  {
    "objectID": "notes/notes-05.html#correlation",
    "href": "notes/notes-05.html#correlation",
    "title": "Lecture 05 - Elemental Confounds",
    "section": "Correlation",
    "text": "Correlation\n\ncorrelation is common in nature, causation is sparse\nscientific question/estimand -> recipe/estimator -> result/estimate"
  },
  {
    "objectID": "notes/notes-05.html#association-causation",
    "href": "notes/notes-05.html#association-causation",
    "title": "Lecture 05 - Elemental Confounds",
    "section": "Association & Causation",
    "text": "Association & Causation\n\nwe must defend against confounding in our stats\nconfounds mislead us - feature of the sample + how we use it\nfour elemental confounds\n\ncauses of confounds can be extremely diverse but they are all at their core made up of relationships between 3 variables"
  },
  {
    "objectID": "notes/notes-05.html#the-fork",
    "href": "notes/notes-05.html#the-fork",
    "title": "Lecture 05 - Elemental Confounds",
    "section": "The Fork",
    "text": "The Fork\n\nX and Y are associated \\(Y \\not\\!\\perp\\!\\!\\!\\perp X\\)\nShare a common cause Z\nOnce stratified by Z, no association \\(Y \\perp\\!\\!\\!\\perp X | Z\\)\n\n\ndag <- dagify(\n    X ~ Z,\n    Y ~ Z\n)\n\nggdag(dag) +\n    theme_dag()\n\n\n\n\n\nsimulation:\n\n\nn <- 1000\nZ <- rbern(n, 0.5)\nX <- rbern(n, (1-Z)*0.01 + Z*0.9)\nY <- rbern(n, (1-Z)*0.01 + Z*0.9)\n\n\ncols <- c(4,2)\nn <- 300\nZ <- rbern(n, 0.5)\nX <- rnorm(n, (1-Z)*0.01 + Z*0.9)\nY <- rnorm(n, (1-Z)*0.01 + Z*0.9)\n\nplot(X, Y, col=cols[Z+1])\n\n\n\n# abline(lm(Y[Z==1]))\n#\n#\n\n\nexample:\n\nwhy do regions of the USA with higher rates of marriage also have higher rates of divorce?\nestimand: causal effect of marriage rate on divorce rate\n\n\n\ndag <- dagify(\n    D ~ A,\n    M ~ A,\n    D ~ M\n)\n\nggdag(dag) +\n    theme_dag()\n\n\n\n\n\ncauses are not in the data\nis effect of marriage rates on divorce rates just a symptom of common cause age?\nneed to break the fork to test direct effect – stratify by A\ncontinuous variable stratification means that we are adding to that variable essentially to the intercept\nstandardizing variables is almost always helpful in linear regression\n\nmake the mean zero and divide by sd\n\nto stratify by A, include as a term in the linear model:\n\n\\(D _i \\sim Normal(\\mu _i, \\sigma)\\)\n\\(\\mu _i = \\alpha + \\beta_MM_i + \\beta_AA_i\\)\n\\(\\alpha \\sim Normal(0, 0.2)\\)\n\\(\\beta_M \\sim Normal(0,0.5)\\)\n\\(\\beta_A \\sim Normal(0, 0.5)\\)\n\\(\\sigma \\sim Exponential(1)\\)\n\n\n\nlibrary(rethinking)\ndata(WaffleDivorce)\nd <- WaffleDivorce\n\n# model\ndat <- list(\n    D = standardize(d$Divorce),\n    M = standardize(d$Marriage),\n    A = standardize(d$MedianAgeMarriage)\n)\n\nm_DMA <- quap(\n    alist(\n        D ~ dnorm(mu,sigma),\n        mu <- a + bM*M + bA*A,\n        a ~ dnorm(0,0.2),\n        bM ~ dnorm(0,0.5),\n        bA ~ dnorm(0,0.5),\n        sigma ~ dexp(1)\n    ) , data=dat )\n\nplot(precis(m_DMA))\n\n\n\n\n\na causal effect is a manipulation of the generative model, an intervention\ndistribution of D when we intervene (“do”) M \\(p(D|do(M))\\)\n\nimplies deleting all arrows into M and simulating D\n\n\n\npost <- extract.samples(m_DMA)\n\n# sample A from data\nn <- 1e3\nAs <- sample(dat$A, size = n, replace = T)\n\n# simulate D for M=0 (sample mean)\nDM0 <- with(post, rnorm(n, a + bM*0 + bA*As, sigma))\n\n# simulate D for M = 1 (+1 standard deviation)\n# use the same A values \nDM1 <- with(post, rnorm(n, a + bM*1 + bA*As, sigma))\n\n# contrast\nM10_contrast <- DM1 - DM0\ndens(M10_contrast, lwd=4, col=2, xlab=\"effect of increase in M\")"
  },
  {
    "objectID": "notes/notes-05.html#the-pipe",
    "href": "notes/notes-05.html#the-pipe",
    "title": "Lecture 05 - Elemental Confounds",
    "section": "The Pipe",
    "text": "The Pipe\n\nX and Y are associated \\(Y \\not\\!\\perp\\!\\!\\!\\perp X\\)\nInfluence of X on Y trasmitted through Z\nOnce stratified by Z, no association \\(Y \\perp\\!\\!\\!\\perp X | Z\\)\n\n\ndag <- dagify(\n    Y ~ Z,\n    Z ~ X\n)\n\nggdag(dag) +\n    theme_dag()\n\n\n\n\n\nn <- 1000\nX <- rbern(n, 0.5)\nZ <- rbern(n, (1-X)*0.01 + X*0.9)\nY <- rbern(n, (1-Z)*0.01 + Z*0.9)\n\n\neverything that Y knows about X, is already known by Z\n\nonce you learn Z, there is nothing more to learn about the association\n\n\n\ncols <- c(4,2)\nn <- 300\nX <- rbern(n)\nZ <- rbern(n, inv_logit(X))\nY <- rbern(n, (2*Z-1))\n\nWarning in rbinom(n, size = 1, prob = prob): NAs produced\n\n# plot\n\n\nincluding the mediator at the wrong time can lead to incorrect inference\n\n\ndag <- dagify(\n   H1 ~ H0 + Treat,\n   H1 ~ Fungus,\n   Fungus ~ Treat\n   \n)\n\nggdag(dag) +\n    theme_dag()\n\n\n\n\n\nwhat is the total causal effect of treatment?\nTreat -> Fungus -> H1 is a pipe, should not stratify by F\npoost-treatment bias\n\nif you stratify by a consquence of the treatment, it can induce post-treatment bias - gives you a misleading estimate of what you’re after\nconsequences of treatment should not usually be included in the estimator"
  },
  {
    "objectID": "notes/notes-05.html#the-collider",
    "href": "notes/notes-05.html#the-collider",
    "title": "Lecture 05 - Elemental Confounds",
    "section": "The Collider",
    "text": "The Collider\n\nX and Y are not associated (share no causes) \\(Y \\perp\\!\\!\\!\\perp X\\)\nX and Y both influence Z\nOnce stratified by Z, X and Y are associated \\(Y \\not\\!\\perp\\!\\!\\!\\perp X | Z\\)\n\n\ndag <- dagify(\n    Z ~ X + Y\n)\n\nggdag(dag) +\n    theme_dag()\n\n\n\n\n\nn <- 1000\nX <- rbern(n, 0.5)\nY <- rbern(n, 0.5)\nZ <- rbern(n, ifelse(X+Y>0, 0.9, 0.2))\n\n\nstrong correlations caused by the collider could be read as correlation but causes are not in the data\n\n\ncols <- c(4,2)\nN <- 300\nX <- rnorm(N)\nY <- rnorm(N)\nZ <- rbern(N, inv_logit(2*X+2*Y-2))\n\nplot(X,Y, cols = cols[Z+1])\n\nWarning in plot.window(...): \"cols\" is not a graphical parameter\n\n\nWarning in plot.xy(xy, type, ...): \"cols\" is not a graphical parameter\n\n\nWarning in axis(side = side, at = at, labels = labels, ...): \"cols\" is not a\ngraphical parameter\n\nWarning in axis(side = side, at = at, labels = labels, ...): \"cols\" is not a\ngraphical parameter\n\n\nWarning in box(...): \"cols\" is not a graphical parameter\n\n\nWarning in title(...): \"cols\" is not a graphical parameter\n\n\n\n\n# lines\n\n\nsometimes samples come already stratified by collider\nassociations among the things you have measured post-selection is dangerous because the selection is often collider bias\nendogenous colliders\n\nif you include a collider in your estimator you can induce a spurious correlation\nhappens within your analysis\n\nexample: age and happiness\n\nestimand: influence of age on happiness\npossible confound: marital status\nsuppose age has zero influence on happiness, but that both age and happiness influence marital status\n\n\n\ndag <- dagify(\n   M ~ A + H\n)\n\nggdag(dag) +\n    theme_dag()\n\n\n\n\n\nstratified by marital status, negative association between age and happiness even though they are not related -> age/happiness are unrelated"
  },
  {
    "objectID": "notes/notes-05.html#the-descendant",
    "href": "notes/notes-05.html#the-descendant",
    "title": "Lecture 05 - Elemental Confounds",
    "section": "The Descendant",
    "text": "The Descendant\n\nhow it behaves depends on what it is attached to\nA is the descendant, contains information of its “parent”\nX and Y are causally associated through Z \\(Y \\not\\!\\perp\\!\\!\\!\\perp X\\)\nA holds information about Z\nOnce stratified by A, X and Y less associated (if strong enough) \\(Y \\perp\\!\\!\\!\\perp X | A\\)\n\n\ndag <- dagify(\n   Z ~ X,\n   Y ~ Z,\n   A ~ Z\n)\n\nggdag(dag) +\n    theme_dag()\n\n\n\n\n\nn <- 1000\nx <- rbern(n, 0.5)\nz <- rbern(n, (1-x)*0.1 + x*0.9)\ny <- rbern(n, (1-z)*0.1 + z*0.9)\na <- rbern(n, (1-z)*0.1 + z*0.9)\n\n\ndescendants are everywhere - proxies"
  }
]